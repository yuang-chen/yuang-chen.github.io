<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Yac&#39;s Log</title>
    <link>https://yuang-chen.github.io/posts/</link>
    <description>Recent content in Posts on Yac&#39;s Log</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 07 May 2023 23:33:24 +0800</lastBuildDate><atom:link href="https://yuang-chen.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Bitwise Op</title>
      <link>https://yuang-chen.github.io/posts/2023-05-07-bitwise-op/</link>
      <pubDate>Sun, 07 May 2023 23:33:24 +0800</pubDate>
      
      <guid>https://yuang-chen.github.io/posts/2023-05-07-bitwise-op/</guid>
      <description>ðŸ¦¥ An old note.
Bitwise vs Arithmetic running on a vector of size 2^31, bitwise operations are significantly faster than arithmetic counterparts:
seg = 64; volume = (vec_size - 1)/ seg + 1; unsigned bs = log2(seg); unsigned bv= log2(volume); unsigned bbv = volume - 1; Arithmetic: out[i] = i % volume * seg + i / volume
Bitwise: out[i] = ((i &amp;amp; bbv) &amp;lt;&amp;lt; bs) + (i &amp;gt;&amp;gt; bv)</description>
    </item>
    
    <item>
      <title>Omp Parallel Region</title>
      <link>https://yuang-chen.github.io/posts/2023-05-02-omp-parallel-region/</link>
      <pubDate>Tue, 02 May 2023 10:34:19 +0800</pubDate>
      
      <guid>https://yuang-chen.github.io/posts/2023-05-02-omp-parallel-region/</guid>
      <description>The results look suspicious to me&amp;hellip; But I wrote down this note many days ago ðŸ¦¥. Maybe I need to evaluate it again.
Multiple Parallel Regions The cost of constructing parallel region is expensive in OpenMP. Let&amp;rsquo;s use two example for illustration:
Three loops operating on a vector of size 2^31, e.g.,
for(size_t i = 0; i &amp;lt; vec.size(); i++) vec[i] += 1, vec[i] *= 0.9, vec[i] /= 7, Case 1: a large parallel region including the three loops by omp parallel { omp for }</description>
    </item>
    
    <item>
      <title>Omp Collapse</title>
      <link>https://yuang-chen.github.io/posts/2023-05-02-omp-collapse/</link>
      <pubDate>Tue, 02 May 2023 10:28:18 +0800</pubDate>
      
      <guid>https://yuang-chen.github.io/posts/2023-05-02-omp-collapse/</guid>
      <description>One of my old-day notes ðŸ¦¥.
Collapse of Nested Loops The collapse clause converts a prefect nested loop into a single loop then parallelize it. The condition of a perfect nested loop is that, the inner loop is tightly included by the outer loop, and no other codes lying between:
for(int i = 0 ... ) { for(int j = 0 ...) { task[i][j]; } } Such condition is hard to meet.</description>
    </item>
    
    <item>
      <title>Vector vs Array</title>
      <link>https://yuang-chen.github.io/posts/2023-05-01-vector-vs-array/</link>
      <pubDate>Mon, 01 May 2023 12:53:14 +0800</pubDate>
      
      <guid>https://yuang-chen.github.io/posts/2023-05-01-vector-vs-array/</guid>
      <description>Another post recycled from my earlier notes. I really don&amp;rsquo;t have motivation to improve it further ðŸ¦¥.
Vector vs Array Initilization The Vector is the preferred choice for data storage in mordern C++. It is internally implemented based on the Array. However, the performance gap between the two is indeed obvious.
The Vector can be initialized via std::vector&amp;lt;T&amp;gt; vec(size). Meanwhile, an Array is initialized by T* arr = new T[size]</description>
    </item>
    
    <item>
      <title> Gather with SIMD</title>
      <link>https://yuang-chen.github.io/posts/2023-04-27-gather-simd/</link>
      <pubDate>Thu, 27 Apr 2023 13:27:50 +0800</pubDate>
      
      <guid>https://yuang-chen.github.io/posts/2023-04-27-gather-simd/</guid>
      <description>Writing SIMD code that works across different platforms can be a challenging task. The following log illustrates how a seemingly simple operation in C++ can quickly escalate into a significant problem.
Let&amp;rsquo;s look into the code below, where the elements of x is accessed through indices specified by idx.
normal code std::vector&amp;lt;float&amp;gt; x = /*some data*/ std::vector&amp;lt;int&amp;gt; idx = /* index */ for(auto i: idx) { auto data = x[i]; } Gather with Intel In AVX512, Gather is a specific intrinsic function to transfer data from a data array to a target vec, according to an index vec.</description>
    </item>
    
    <item>
      <title>SIMD is Pain</title>
      <link>https://yuang-chen.github.io/posts/2023-04-25-simd-pain-intro/</link>
      <pubDate>Tue, 25 Apr 2023 20:59:39 +0800</pubDate>
      
      <guid>https://yuang-chen.github.io/posts/2023-04-25-simd-pain-intro/</guid>
      <description>Writing code with SIMD for vectorization is painful. It deserves a blog series to record all sorts of pains I have encountered and (partially) overcome.
Indeed, once the pain of coding and debugging is finished, the program is lightning-faster. Nonetheless, I am here to complain instead of praising. Let me state why writing SIMD code is causing me emotional damage:
a single line of normal c++ code could be easily inflated to a dozen lines of code.</description>
    </item>
    
    <item>
      <title>Parallel Algorithms from Libraries</title>
      <link>https://yuang-chen.github.io/posts/2023-04-25-par-algo/</link>
      <pubDate>Tue, 25 Apr 2023 10:16:34 +0800</pubDate>
      
      <guid>https://yuang-chen.github.io/posts/2023-04-25-par-algo/</guid>
      <description>The content of this post is extracted from my previous random notes. I am too lazy to update and organize it ðŸ¦¥.
C++17 new feature &amp;ndash; parallel algorithms The parallel algorithms and execution policies are introduced in C++17. Unfortuantely, according to CppReference, only GCC and Intel support these features. Clang still leaves them unimplemented.
A blog about it.
The parallel library brough by C++17 requires the usage of Intel&amp;rsquo;s oneTBB for multithreading.</description>
    </item>
    
  </channel>
</rss>
