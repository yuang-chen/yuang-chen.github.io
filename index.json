[{"content":"Description std::deque extends the interfaces of std::vector with push_front, pop_front, etc., such that elements can be inserted or removed at the end or beginning at constant time.\nI\u0026rsquo;ve hardly ever incorporated std::deque in my own coding projects, and it\u0026rsquo;s a rarity in other people\u0026rsquo;s work as well.\nCode std::deque is essentially a sequence of individually allocated fixed-size arrays. The real challenge lies in the bookkeeping. Four variables are relied on to keep track of data:\nblock_front block_back index_front inside the block_front index_back inside the block_back #include \u0026lt;iostream\u0026gt; template \u0026lt;typename T\u0026gt; class Deque { private: T** blocks; size_t block_size; size_t num_blocks; size_t blocks_used; size_t block_front; size_t block_back; size_t index_front; size_t index_back; size_t size_; public: Deque(); ~Deque(); void push_back(T elem); void push_front(T elem); void pop_back(); void pop_front(); T front(); T back(); bool empty(); T\u0026amp; at(size_t index); T\u0026amp; operator[](size_t index); size_t size() const; }; template \u0026lt;typename T\u0026gt; Deque\u0026lt;T\u0026gt;::Deque() { num_blocks = 5; block_size = 8; blocks = new T*[num_blocks]; for(int i = 0; i \u0026lt; num_blocks; i++) { blocks[i] = new T[block_size]; // elements are not initialized } // the starting block is the middle block block_front = num_blocks / 2; block_back = num_blocks / 2; // the starting index of element is at the middle of the block index_front = block_size / 2; index_back = block_size / 2 - 1; // ensures start with empty blocks_used = 0; size_ = 0; } template \u0026lt;typename T\u0026gt; Deque\u0026lt;T\u0026gt;::~Deque() { for(int i = 0; i \u0026lt; num_blocks; i++) { delete[] blocks[i]; } delete[] blocks; } /** * case breakdown: * 0. deque is empty * * just add the element * 1. current block is not full * * just add the element * 2. current block is full --\u0026gt; further breakdown: * 2.1. there is space in the next block * * store the element in next block * * update the indexes * 2.2. there is no space in the next block * * create new blocks * * copy the elements to the new blocks * * delete the old blocks * * update the indexes */ template \u0026lt;typename T\u0026gt; void Deque\u0026lt;T\u0026gt;::push_back(T elem) { //if the deque is empty if(size_ == 0) { blocks_used = 1; blocks[block_back][++index_back] = elem; size_++; return; } // if the block is not full if(index_back \u0026lt; block_size - 1) { blocks[block_back][++index_back] = elem; size_++; return; } // ********************* // if current block is full //*********************** // if the next block is not full, move to it if(block_back \u0026lt; num_blocks - 1) { index_back = 0; blocks[++block_back][index_back] = elem; size_++; blocks_used++; return; } // if the next block is full, allocate new blocks // 0 1 1 1 -\u0026gt; 0 0 1 1 1 1 0 0 (8-3)/2 = 2 size_t old_block_front = block_front; num_blocks *= 2; index_back = 0; blocks_used++; size_++; block_front = (num_blocks - blocks_used) / 2; block_back = block_front + blocks_used - 1; T** new_blocks = new T*[num_blocks]; for(size_t i = 0; i \u0026lt; block_front; i++) { new_blocks[i] = new T[block_size]; // allocate new blocks } for(size_t i = block_front; i \u0026lt; block_back; i++) { new_blocks[i] = blocks[old_block_front++]; // reuse old blocks } for(size_t i = block_back; i \u0026lt; num_blocks; i++) { new_blocks[i] = new T[block_size]; // allocate new blocks } new_blocks[block_back][0] = elem; // only delete the ptrptr** T** temp = blocks; blocks = new_blocks; delete[] temp; // std::cout \u0026lt;\u0026lt; \u0026#34;Blocks: \u0026#34; \u0026lt;\u0026lt; num_blocks \u0026lt;\u0026lt; std::endl; // std::cout \u0026lt;\u0026lt; \u0026#34;Blocks used: \u0026#34; \u0026lt;\u0026lt; blocks_used \u0026lt;\u0026lt; std::endl; // std::cout \u0026lt;\u0026lt; \u0026#34;Block front: \u0026#34; \u0026lt;\u0026lt; block_front \u0026lt;\u0026lt; std::endl; // std::cout \u0026lt;\u0026lt; \u0026#34;Block back: \u0026#34; \u0026lt;\u0026lt; block_back \u0026lt;\u0026lt; std::endl; // std::cout \u0026lt;\u0026lt; \u0026#34;Index front: \u0026#34; \u0026lt;\u0026lt; index_front \u0026lt;\u0026lt; std::endl; // std::cout \u0026lt;\u0026lt; \u0026#34;Index back: \u0026#34; \u0026lt;\u0026lt; index_back \u0026lt;\u0026lt; std::endl; // std::cout \u0026lt;\u0026lt; std::endl \u0026lt;\u0026lt; std::endl; } // reverse the push_back template \u0026lt;typename T\u0026gt; void Deque\u0026lt;T\u0026gt;::push_front(T elem) { //if the deque is empty if(size_ == 0) { blocks_used = 1; blocks[block_front][--index_front] = elem; size_++; return; } // if the block is not full if(index_front \u0026gt; 0) { blocks[block_front][--index_front] = elem; size_++; return; } // ********************* // if the block is full //*********************** // if the prior block is not full, move to it if(block_front \u0026gt; 0) { index_front = block_size - 1; blocks[--block_front][index_front] = elem; size_++; blocks_used++; return; } // if the next block is full, allocate new blocks // 1 1 1 0 -\u0026gt; 0 0 1 1 1 1 0 0 (8-3)/2 = 2 size_t old_block_front = block_front; num_blocks *= 2; index_front = block_size - 1; blocks_used++; size_++; block_front = (num_blocks - blocks_used) / 2; block_back = block_front + blocks_used - 1; T** new_blocks = new T*[num_blocks]; for(size_t i = 0; i \u0026lt; block_front; i++) { new_blocks[i] = new T[block_size]; // allocate new blocks } for(size_t i = block_front; i \u0026lt; block_back; i++) { new_blocks[i] = blocks[old_block_front++]; // reuse old blocks } for(size_t i = block_back; i \u0026lt; num_blocks; i++) { new_blocks[i] = new T[block_size]; // allocate new blocks } new_blocks[block_front][index_front] = elem; // only delete the ptrptr** T** temp = blocks; blocks = new_blocks; delete[] temp; // std::cout \u0026lt;\u0026lt; \u0026#34;Blocks: \u0026#34; \u0026lt;\u0026lt; num_blocks \u0026lt;\u0026lt; std::endl; // std::cout \u0026lt;\u0026lt; \u0026#34;Blocks used: \u0026#34; \u0026lt;\u0026lt; blocks_used \u0026lt;\u0026lt; std::endl; // std::cout \u0026lt;\u0026lt; \u0026#34;Block front: \u0026#34; \u0026lt;\u0026lt; block_front \u0026lt;\u0026lt; std::endl; // std::cout \u0026lt;\u0026lt; \u0026#34;Block back: \u0026#34; \u0026lt;\u0026lt; block_back \u0026lt;\u0026lt; std::endl; // std::cout \u0026lt;\u0026lt; \u0026#34;Index front: \u0026#34; \u0026lt;\u0026lt; index_front \u0026lt;\u0026lt; std::endl; // std::cout \u0026lt;\u0026lt; \u0026#34;Index back: \u0026#34; \u0026lt;\u0026lt; index_back \u0026lt;\u0026lt; std::endl; // std::cout \u0026lt;\u0026lt; std::endl \u0026lt;\u0026lt; std::endl; } template \u0026lt;typename T\u0026gt; void Deque\u0026lt;T\u0026gt;::pop_back() { if(size_ == 0) return; // if the back index is not 0 size_--; blocks[block_back][index_back] = std::numeric_limits\u0026lt;T\u0026gt;::max(); // register an error signal index_back--; // if the back index is 0, move to prior block if(index_back == std::numeric_limits\u0026lt;size_t\u0026gt;::max()) // if(int(index_back) == -1) { index_back = block_size - 1; block_back--; } } template \u0026lt;typename T\u0026gt; void Deque\u0026lt;T\u0026gt;::pop_front() { if(size_ == 0) { return; } // if the back index is not 0 size_--; blocks[block_front][index_front] = std::numeric_limits\u0026lt;T\u0026gt;::max(); // register an error signal index_front++; // if the back index is 0 if(index_front == block_size) { block_front++; index_front = 0; } } template \u0026lt;typename T\u0026gt; T Deque\u0026lt;T\u0026gt;::back() { return blocks[block_back][index_back]; } template \u0026lt;typename T\u0026gt; T Deque\u0026lt;T\u0026gt;::front() { return blocks[block_front][index_front]; } template \u0026lt;typename T\u0026gt; bool Deque\u0026lt;T\u0026gt;::empty() { return size_ == 0; } template \u0026lt;typename T\u0026gt; T\u0026amp; Deque\u0026lt;T\u0026gt;::operator[](size_t index) // do not perform boundary checking { const size_t remain = block_size - index_front; if(index \u0026lt; remain) { return blocks[block_front][index_front + index]; } size_t curr_block = block_front + (index + remain) / block_size; size_t curr_index = (index + remain) % block_size; return blocks[curr_block][curr_index]; } template \u0026lt;typename T\u0026gt; T\u0026amp; Deque\u0026lt;T\u0026gt;::at(size_t index) // perform boundary checking { if(index \u0026gt;= size_) { throw std::out_of_range(\u0026#34;Deque::at() index out of range\u0026#34;); } const size_t remain = block_size - index_front; if(index \u0026lt; remain) { return blocks[block_front][index_front + index]; } const size_t curr_block = block_front + (index + remain) / block_size; const size_t curr_index = (index + remain) % block_size; return blocks[curr_block][curr_index]; } template \u0026lt;typename T\u0026gt; size_t Deque\u0026lt;T\u0026gt;::size() const { return size_; } int main() { Deque\u0026lt;int\u0026gt; deq; std::cout \u0026lt;\u0026lt; \u0026#34;-----push_back test-----\u0026#34; \u0026lt;\u0026lt; std::endl; for(int i = 0; i \u0026lt; 21; i++) { deq.push_back(i); std::cout \u0026lt;\u0026lt; deq.back() \u0026lt;\u0026lt; \u0026#39; \u0026#39;; } std::cout \u0026lt;\u0026lt; \u0026#34;\\n-----pop_back test-----\u0026#34; \u0026lt;\u0026lt; std::endl; for(int i = 0; i \u0026lt; 21; i++) { std::cout \u0026lt;\u0026lt; deq.back() \u0026lt;\u0026lt; \u0026#39; \u0026#39;; deq.pop_back(); } std::cout \u0026lt;\u0026lt; \u0026#34;\\n-----push_front test-----\u0026#34; \u0026lt;\u0026lt; std::endl; for(int i = 0; i \u0026lt; 21; i++) { deq.push_front(i); std::cout \u0026lt;\u0026lt; deq.front() \u0026lt;\u0026lt; \u0026#39; \u0026#39;; } std::cout \u0026lt;\u0026lt; \u0026#34;\\n-----pop_front test-----\u0026#34; \u0026lt;\u0026lt; std::endl; for(int i = 0; i \u0026lt; 21; i++) { std::cout \u0026lt;\u0026lt; deq.front() \u0026lt;\u0026lt; \u0026#39; \u0026#39;; deq.pop_front(); } std::cout \u0026lt;\u0026lt; \u0026#34;\\n-----operator[] test-----\u0026#34; \u0026lt;\u0026lt; std::endl; for(int i = 0; i \u0026lt; 11; i++) deq.push_back(i); for(int i = 0; i \u0026lt; 12; i++) { std::cout \u0026lt;\u0026lt; deq[i] \u0026lt;\u0026lt; \u0026#39; \u0026#39;; } std::cout \u0026lt;\u0026lt; \u0026#34;\\n-----at() test-----\u0026#34; \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; deq.size() \u0026lt;\u0026lt; std::endl; for(int i = 0; i \u0026lt; 12; i++) { std::cout \u0026lt;\u0026lt; deq.at(i) \u0026lt;\u0026lt; \u0026#39; \u0026#39;; } } Reference https://stackoverflow.com/questions/6292332/what-really-is-a-deque-in-stl https://github.com/rdmc10/Deque/blob/main/deque.cpp https://en.cppreference.com/w/cpp/container/deque\n","permalink":"https://yuang-chen.github.io/posts/2023-09-04-deque/","summary":"Description std::deque extends the interfaces of std::vector with push_front, pop_front, etc., such that elements can be inserted or removed at the end or beginning at constant time.\nI\u0026rsquo;ve hardly ever incorporated std::deque in my own coding projects, and it\u0026rsquo;s a rarity in other people\u0026rsquo;s work as well.\nCode std::deque is essentially a sequence of individually allocated fixed-size arrays. The real challenge lies in the bookkeeping. Four variables are relied on to keep track of data:","title":"Deque"},{"content":" Array is allocated in stack memory Vector is allocated in heap memory. Its capacity is “pre-allocated”. #include \u0026lt;iostream\u0026gt; template\u0026lt;typename T\u0026gt; class Vector { private: T* data_; size_t size_; size_t capacity_; public: Vector(): data_(nullptr), size_(0), capacity_(0) {} Vector(size_t n_): size_(n_), capacity_(n_) { data_ = new T[n_]; } ~Vector() { delete [] data_; }; T\u0026amp; operator[] (size_t index) { return data_[index]; } const T\u0026amp; operator[] (size_t index) const { return data_[index]; } size_t size() const { return size_; } void push_back(const T\u0026amp; value) { if(size_ == capacity_) { capacity_ = size_ == 0? 1: 2 * capacity_; T* new_data_ = new T[capacity_]; for(size_t i = 0; i \u0026lt; size_; i++) { new_data_[i] = data_[i]; } delete[] data_; data_ = new_data_; } data_[size_] = value; size_++; } }; template\u0026lt;typename T, size_t size_\u0026gt; class Array { private: T data_[size_]; public: T\u0026amp; operator[] (size_t index) { return data_[index]; } const T\u0026amp; operator[] (size_t index) const { return data_[index]; } size_t size() const { return size_; } }; int main() { Vector\u0026lt;int\u0026gt; vec; vec.push_back(10); vec.push_back(2); for(int i =0; i \u0026lt; vec.size(); i++) std::cout \u0026lt;\u0026lt; vec[i] \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; Array\u0026lt;int, 3\u0026gt; arr{}; arr[1] = 9; for(int i =0; i \u0026lt; arr.size(); i++) std::cout \u0026lt;\u0026lt; arr[i] \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; return 0; } ","permalink":"https://yuang-chen.github.io/posts/2023-09-02-vector-array/","summary":"Array is allocated in stack memory Vector is allocated in heap memory. Its capacity is “pre-allocated”. #include \u0026lt;iostream\u0026gt; template\u0026lt;typename T\u0026gt; class Vector { private: T* data_; size_t size_; size_t capacity_; public: Vector(): data_(nullptr), size_(0), capacity_(0) {} Vector(size_t n_): size_(n_), capacity_(n_) { data_ = new T[n_]; } ~Vector() { delete [] data_; }; T\u0026amp; operator[] (size_t index) { return data_[index]; } const T\u0026amp; operator[] (size_t index) const { return data_[index]; } size_t size() const { return size_; } void push_back(const T\u0026amp; value) { if(size_ == capacity_) { capacity_ = size_ == 0?","title":"Vector \u0026 Array"},{"content":"Iterative BFS Despite its apparent simplicity, this approach relies heavily on the utilization of various STL containers. std::unordered_map records the parent of each node std::unordered_set checks if a node has been visited std::queue allows the nodes be accessed in the width-first flow; using std::stack for depth-first flow std::stack reverses the parents, so the path can be printed in root-to-target order. #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;unordered_map\u0026gt; #include \u0026lt;unordered_set\u0026gt; #include \u0026lt;queue\u0026gt; #include \u0026lt;stack\u0026gt; std::stack\u0026lt;int\u0026gt; BFS(const int root, const int target, const std::vector\u0026lt;int\u0026gt;\u0026amp; rowPtr, const std::vector\u0026lt;int\u0026gt;\u0026amp; colIdx) { std::unordered_map\u0026lt;int, int\u0026gt; parent; std::unordered_set\u0026lt;int\u0026gt; visited; std::queue\u0026lt;int\u0026gt; nodeQue; // std::stack\u0026lt;int\u0026gt; nodeStk for DFS std::stack\u0026lt;int\u0026gt; path; bool hasFound = false; nodeQue.push(root); visited.insert(root); while(!nodeQue.empty()) { auto curr = nodeQue.front(); // nodeStk.top() for DFS nodeQue.pop(); if(curr == target) { hasFound = true; break; } for(int i = rowPtr[curr]; i \u0026lt; rowPtr[curr+1]; i++) { auto next = colIdx[i]; if(visited.count(next) == 0) { nodeQue.push(next); visited.insert(next); parent[next] = curr; } } } if(hasFound) { auto curr = target; path.push(curr); if(curr != root) { curr = parent[curr]; path.push(curr); } path.push(root); } return path; } int main() { std::vector\u0026lt;int\u0026gt; rowPointer = {0, 2, 4, 6, 8, 10, 12}; std::vector\u0026lt;int\u0026gt; colIndices = {1, 2, 0, 3, 0, 1, 4, 1, 2, 5, 3, 5}; auto path = BFS(0, 5, rowPointer, colIndices); if(path.empty()) { std::cout \u0026lt;\u0026lt; \u0026#34;Path Not Found\\n\u0026#34;; } else { std::cout \u0026lt;\u0026lt; \u0026#34;Path from Root to Target: \u0026#34;; while(!path.empty()) { std::cout \u0026lt;\u0026lt; path.top() \u0026lt;\u0026lt; \u0026#39; \u0026#39;; path.pop(); } } return 0; } Recursive DFS DFS naturally aligns with recursion. In the earlier example, the iterative DFS employs the std::stack to facilitate the depth-first progression. On the other hand, recursion, which stores function calls in the stack memory, permits the execution of DFS functions in a last-in-first-out manner, eliminating the need for using std::stack. Here, std::vector\u0026lt;bool\u0026gt; is utilized instead of std::unordered_set\u0026lt;int\u0026gt; for memory efficiency. #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; void DFS(const int root, const int target, std::vector\u0026lt;int\u0026gt;\u0026amp; path, std::vector\u0026lt;bool\u0026gt;\u0026amp; visited, const std::vector\u0026lt;int\u0026gt;\u0026amp; rowPtr, const std::vector\u0026lt;int\u0026gt;\u0026amp; colIdx) { visited[root] = true; path.push_back(root); if(root == target) { std::cout \u0026lt;\u0026lt; \u0026#34;DFS path from root to target: \u0026#34;; for(int i = 0; i \u0026lt; path.size(); i++) { std::cout \u0026lt;\u0026lt; path[i] \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } std::cout \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; } for(int i = rowPtr[root]; i \u0026lt; rowPtr[root+1]; i++) { int next = colIdx[i]; if( visited[next] == false ) { DFS(next, target, path, visited, rowPtr, colIdx); } } } int main() { std::vector\u0026lt;int\u0026gt; rowPointer = {0, 2, 4, 6, 8, 10, 12}; std::vector\u0026lt;int\u0026gt; colIndices = {1, 2, 0, 3, 0, 1, 4, 1, 2, 5, 3, 5}; std::vector\u0026lt;int\u0026gt; path; std::vector\u0026lt;bool\u0026gt; visited( rowPointer.size()-1, false ); //used as unordered_set DFS(0, 5, path, visited, rowPointer, colIndices); return 0; } ","permalink":"https://yuang-chen.github.io/posts/2023-09-01-bfs/","summary":"Iterative BFS Despite its apparent simplicity, this approach relies heavily on the utilization of various STL containers. std::unordered_map records the parent of each node std::unordered_set checks if a node has been visited std::queue allows the nodes be accessed in the width-first flow; using std::stack for depth-first flow std::stack reverses the parents, so the path can be printed in root-to-target order. #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;unordered_map\u0026gt; #include \u0026lt;unordered_set\u0026gt; #include \u0026lt;queue\u0026gt; #include \u0026lt;stack\u0026gt; std::stack\u0026lt;int\u0026gt; BFS(const int root, const int target, const std::vector\u0026lt;int\u0026gt;\u0026amp; rowPtr, const std::vector\u0026lt;int\u0026gt;\u0026amp; colIdx) { std::unordered_map\u0026lt;int, int\u0026gt; parent; std::unordered_set\u0026lt;int\u0026gt; visited; std::queue\u0026lt;int\u0026gt; nodeQue; // std::stack\u0026lt;int\u0026gt; nodeStk for DFS std::stack\u0026lt;int\u0026gt; path; bool hasFound = false; nodeQue.","title":"BFS \u0026 DFS"},{"content":"Considering myself a researcher in graph algorithms, I\u0026rsquo;ve come to the surprising realization that my grasp of these algorithms is not as solid as I thought. Hence, this blog series aims to document my exploration of various graph algorithms I\u0026rsquo;ve encountered thus far, regardless of their complexity.\nThe algorithms are selected from the parallel graph frameworks GAP and GBBS, focusing on their single-threaded versions to assess their complexity.\nBreadth-First Search (BFS) Single-Source Shortest Paths (SSSP) Connected Components (CC) Betweenness Centrality (BC) Triangle Counting (TC) * ","permalink":"https://yuang-chen.github.io/posts/2023-08-31-graph-algorithms/","summary":"Considering myself a researcher in graph algorithms, I\u0026rsquo;ve come to the surprising realization that my grasp of these algorithms is not as solid as I thought. Hence, this blog series aims to document my exploration of various graph algorithms I\u0026rsquo;ve encountered thus far, regardless of their complexity.\nThe algorithms are selected from the parallel graph frameworks GAP and GBBS, focusing on their single-threaded versions to assess their complexity.\nBreadth-First Search (BFS) Single-Source Shortest Paths (SSSP) Connected Components (CC) Betweenness Centrality (BC) Triangle Counting (TC) * ","title":"Graph Algorithms"},{"content":"In my HPC-oriented programming, my go-to choices are typically limited to arrays and vectors because of their memory efficiency. Linked lists and hash maps, being non-contiguous in memory space, rarely find their way into my toolkit. These containers draw upon many classic algorithmic designs. Lately, as I\u0026rsquo;ve been revisiting fundamental graph algorithms, I\u0026rsquo;ve also decided to take on the tasks of re-implementing these containers in a simplified illustration.\nThey are:\nC++98: std::map, std::set, std::multimap, and std::multiset C++11: std::unordered_map, std::unordered_set, std::unordered_multimap, and std::unordered_multiset C++23: std::flat_map, std::flat_set, std::flat_multimap, and std::flat_multiset Sequence Containers Data structures which can be accessed sequentially.\nstd::array\u0026lt;T,size\u0026gt; std::vector\u0026lt;T\u0026gt; std::deque\u0026lt;T\u0026gt; std::list\u0026lt;T\u0026gt; std::forward_list\u0026lt;T\u0026gt; Associative Containers Sorted data structures (i.e., balanced binary search tree) that can be quickly searched (O(log n) complexity).\nstd::set\u0026lt;T\u0026gt; std::multiset\u0026lt;T\u0026gt; std::map\u0026lt;Key, Value\u0026gt; std::multimap\u0026lt;Key, Value\u0026gt; Typically, an associate container consists a data type(s), a comparison function, and an allocator\ntemplate\u0026lt;typename Key, typename Value, typename Compare = std::less\u0026lt;key\u0026gt;, typename Allocator = std::allocator\u0026lt;std::pair\u0026lt;const Key, Value\u0026gt;\u0026gt;\u0026gt; Unordered Associative Containers Unsorted data structures (i.e., hashing bucket) that can be quickly searched (O(1) average, O(n) worst-case complexity).\nstd::unordered_set\u0026lt;T\u0026gt; std::unordered_map\u0026lt;Key, Value\u0026gt; std::unordered_multiset\u0026lt;T\u0026gt; std::unordered_multimap\u0026lt;Key, Value\u0026gt; Typically, an unordered associate container consists a data type(s), a hash function, an equal function and an allocator. The equal function indicates this type of containers does not provide support for comparison.\ntemplate\u0026lt; typename Key, typename T, typename Hash = std::hash\u0026lt;Key\u0026gt;, typename KeyEqual = std::equal_to\u0026lt;Key\u0026gt;, typename Allocator = std::allocator\u0026lt;std::pair\u0026lt;const Key, T\u0026gt;\u0026gt; \u0026gt; Container adaptors A different interface for sequential containers.\nstd::stack\u0026lt;T\u0026gt; std::queue\u0026lt;T\u0026gt; std::priority_queue\u0026lt;T\u0026gt; std::flat_set (c++23) std::flat_map (c++23) std::flat_multiset (c++23) std::flat_multimap (c++23) The flat-ordered associative containers in C++23 have the same interface as their C++98 pendants. They adopt from sequence containers, e.g., std::vector by default.\nReference [1] Back to Basics: Standard Library Containers in Cpp - Rainer Grimm - CppCon 2022\n[2] C++23: Four new Associative Containers\n[3] Refresher on Containers, Algorithms and Performance in C++ - Vladimir Vishnevskii - CppCon 2022\n","permalink":"https://yuang-chen.github.io/posts/2023-08-30-stl-containers/","summary":"In my HPC-oriented programming, my go-to choices are typically limited to arrays and vectors because of their memory efficiency. Linked lists and hash maps, being non-contiguous in memory space, rarely find their way into my toolkit. These containers draw upon many classic algorithmic designs. Lately, as I\u0026rsquo;ve been revisiting fundamental graph algorithms, I\u0026rsquo;ve also decided to take on the tasks of re-implementing these containers in a simplified illustration.\nThey are:","title":"STL Containers"},{"content":"Background Scope Guard is a concept reminiscent of the RAII (Resource Acquisition Is Initialization) principle in C++. The idea is to manage resources (like memory, files, network sockets, etc.) using object lifetime. When the object goes out of scope, its destructor ensures that the resource is cleaned up properly. The scope guard is intended to run a given callable (like a function or lambda) when it is destroyed.\nRAII (Resource Acquisition Is Initialization) is a programming idiom used in C++ where the lifetime of an object is bound to the lifetime of its scope (typically represented by a block of code wrapped in curly braces {}).\nHere\u0026rsquo;s a breakdown of RAII:\nResource Acquisition: When an object is created, it acquires a specific resource. Initialization: The resource acquisition is done during the object\u0026rsquo;s construction (i.e., when it\u0026rsquo;s initialized). RAII ensures the following:\nResources are acquired in a deterministic manner (during object creation). Resources are released in a deterministic manner (during object destruction). Exception safety, as resources are automatically cleaned up even if an exception is thrown. Example A simple example of RAII is the use of std::unique_ptr to manage dynamically allocated memory:\nvoid exampleFunction() { std::unique_ptr\u0026lt;int\u0026gt; p(new int(5)); // Resource (memory) is acquired and \u0026#34;owned\u0026#34; by p. // Do some operations with p... } // p goes out of scope and its destructor is called, which deletes the memory. No memory leak! This RAII behavior is contrasted with manual memory management where you\u0026rsquo;d have to remember to call delete:\nvoid nonRAIIExample() { int* p = new int(5); // Memory is acquired. // Do some operations... delete p; // You have to manually release the memory. Risky! } Implementation of a Scope Guard Requirements Three requirements are listed in the following code block for implementing the scope guard.\n#include \u0026lt;cstdio\u0026gt; #include \u0026lt;cassert\u0026gt; #include \u0026lt;stdexcept\u0026gt; #include \u0026lt;iostream\u0026gt; #include \u0026lt;functional\u0026gt; int main() { { // Requirement 0: Support lambda FILE * fp = nullptr; try{ fp = fopen(\u0026#34;test.txt\u0026#34;,\u0026#34;a\u0026#34;); auto guard = scope_guard([\u0026amp;] { fclose(fp); fp = nullptr; }); throw std::runtime_error{\u0026#34;Test\u0026#34;}; } catch(std::exception \u0026amp; e){ puts(e.what()); } assert(fp == nullptr); } puts(\u0026#34;----------\u0026#34;); { // Requirement 1: Support function object invocation // \u0026amp; binding arguments to the callable. struct Test { void operator()(X* x) { delete x; } } t; auto x = new X{}; auto guard = scope_guard(t, x); } puts(\u0026#34;----------\u0026#34;); { // Requirement 2: Support member functions and std::ref. auto x = new X{}; { struct Test { void f(X*\u0026amp; px) { delete px; px = nullptr; } } t; auto guard = scope_guard{\u0026amp;Test::f, \u0026amp;t, std::ref(x)}; } assert(x == nullptr); } Solutions Naive To meet the basic requirement, all you need to do is keep the lambda stored within a std::function:\n// naive solution class scope_guard { public: explicit scope_guard(std::function\u0026lt;void()\u0026gt; onExitScope) : onExitScope_(onExitScope) {} ~scope_guard() { on_exit_scope(); } private: std::function\u0026lt;void()\u0026gt; on_exit_scope; }; Conventional However, for requirements 2 and 3, things get trickier. We need to deal with more complex situations like binding arguments and passing by reference. As a result, we\u0026rsquo;re stepping up our game with an upgraded solution:\n// conventional solution class scope_guard { public: template \u0026lt;typename Callable, typename... Args\u0026gt; scope_guard(Callable\u0026amp;\u0026amp; func, Args\u0026amp;\u0026amp;... args) { on_exit_scope = std::bind(std::forward\u0026lt;Callable\u0026gt;(func), std::forward\u0026lt;Args\u0026gt;(args)...); } ~scope_guard() { on_exit_scope(); } private: std::function\u0026lt;void()\u0026gt; on_exit_scope; }; Fancy However, this simple solution isn\u0026rsquo;t cool anymore. The use of std::bind dates back to the old c++11 days, but we\u0026rsquo;re now in the modern world of c++23. Let\u0026rsquo;s modernize (and over-complicate) the code:\n// fancy solution class scope_guard { public: template\u0026lt;typename Callable, typename... Args\u0026gt; requires std::invocable\u0026lt;Callable, std::unwrap_reference_t\u0026lt;Args\u0026gt;...\u0026gt; scope_guard(Func\u0026amp;\u0026amp; func, Args\u0026amp;\u0026amp;...args) :f{ [func = std::forward\u0026lt;Func\u0026gt;(func), ...args = std::forward\u0026lt;Args\u0026gt;(args)]() mutable { std::invoke(std::forward\u0026lt;std::decay_t\u0026lt;Func\u0026gt;\u0026gt;(func), std::unwrap_reference_t\u0026lt;Args\u0026gt;(std::forward\u0026lt;Args\u0026gt;(args))...); } }{} ~scope_guard() { on_exit_scope(); } // Prevent copying, but allow moves. scope_guard(const scope_guard\u0026amp;) = delete; scope_guard\u0026amp; operator=(const scope_guard\u0026amp;) = delete; scope_guard(scope_guard\u0026amp;\u0026amp;) = default; scope_guard\u0026amp; operator=(scope_guard\u0026amp;\u0026amp;) = default; private: std::function\u0026lt;void()\u0026gt; on_exit_scope; }; ","permalink":"https://yuang-chen.github.io/posts/2023-08-29-scope-guard/","summary":"Background Scope Guard is a concept reminiscent of the RAII (Resource Acquisition Is Initialization) principle in C++. The idea is to manage resources (like memory, files, network sockets, etc.) using object lifetime. When the object goes out of scope, its destructor ensures that the resource is cleaned up properly. The scope guard is intended to run a given callable (like a function or lambda) when it is destroyed.\nRAII (Resource Acquisition Is Initialization) is a programming idiom used in C++ where the lifetime of an object is bound to the lifetime of its scope (typically represented by a block of code wrapped in curly braces {}).","title":"Scope Guard"},{"content":"C++ templates are blueprints and don\u0026rsquo;t represent specific types until they are instantiated with actual types. Once instantiated, the compiler creates a specific version of that template for the provided type. For template classes, each instantiation has its own unique version of the static members, making them distinct for each type the template is instantiated with.\n///////////////////// // Code Block 1 ///////////////////// #include\u0026lt;iostream\u0026gt; class ComponentBase{ protected: // component_type_count is a static variable shared by derived classes static inline size_t component_type_count = 0; }; template\u0026lt;typename T\u0026gt; class Component : public ComponentBase{ public: static size_t component_type_id(){ // ID is the static local variable for a particular type T static size_t ID = component_type_count++; return ID; } }; class A : public Component\u0026lt;A\u0026gt; {}; class B : public Component\u0026lt;B\u0026gt; {}; class C : public Component\u0026lt;C\u0026gt; {}; int main() { std::cout \u0026lt;\u0026lt; A::component_type_id() \u0026lt;\u0026lt; std::endl; // 0 std::cout \u0026lt;\u0026lt; B::component_type_id() \u0026lt;\u0026lt; std::endl; // 1 std::cout \u0026lt;\u0026lt; B::component_type_id() \u0026lt;\u0026lt; std::endl; // 1 std::cout \u0026lt;\u0026lt; A::component_type_id() \u0026lt;\u0026lt; std::endl; // 0 std::cout \u0026lt;\u0026lt; A::component_type_id() \u0026lt;\u0026lt; std::endl; // 0 std::cout \u0026lt;\u0026lt; C::component_type_id() \u0026lt;\u0026lt; std::endl; // 2 } Key Points:\ncomponent_type_count belongs to the base class ComponentBase but shared by all derived classes. A unique ID belongs to every instantiated class (e.g., A, B, C). In code block 1, the component_type_id() function has a static local variable ID. When this function is called for the first time for a particular type T, the ID variable is initialized with the current value of component_type_count and then component_type_count is incremented. For all subsequent calls to this function for the same type T, the ID variable retains its value from the first call and just returns that value. Essentially, each type T gets a unique ID the first time this function is called, and the same ID is returned for all subsequent calls.\n///////////////////// // Code Block 2 ///////////////////// template\u0026lt;typename T\u0026gt; class Component : public ComponentBase{ public: static size_t component_type_id(){ return component_type_count++; } }; // print: 0 1 2 3 4 5 In code block 2, the component_type_id() function increments and returns the value of component_type_count every time it\u0026rsquo;s called, regardless of the type T. So, unlike the first block, every call to component_type_id() for a given type T will return a new, incremented value.\n","permalink":"https://yuang-chen.github.io/posts/2023-08-27-static-local-member/","summary":"C++ templates are blueprints and don\u0026rsquo;t represent specific types until they are instantiated with actual types. Once instantiated, the compiler creates a specific version of that template for the provided type. For template classes, each instantiation has its own unique version of the static members, making them distinct for each type the template is instantiated with.\n///////////////////// // Code Block 1 ///////////////////// #include\u0026lt;iostream\u0026gt; class ComponentBase{ protected: // component_type_count is a static variable shared by derived classes static inline size_t component_type_count = 0; }; template\u0026lt;typename T\u0026gt; class Component : public ComponentBase{ public: static size_t component_type_id(){ // ID is the static local variable for a particular type T static size_t ID = component_type_count++; return ID; } }; class A : public Component\u0026lt;A\u0026gt; {}; class B : public Component\u0026lt;B\u0026gt; {}; class C : public Component\u0026lt;C\u0026gt; {}; int main() { std::cout \u0026lt;\u0026lt; A::component_type_id() \u0026lt;\u0026lt; std::endl; // 0 std::cout \u0026lt;\u0026lt; B::component_type_id() \u0026lt;\u0026lt; std::endl; // 1 std::cout \u0026lt;\u0026lt; B::component_type_id() \u0026lt;\u0026lt; std::endl; // 1 std::cout \u0026lt;\u0026lt; A::component_type_id() \u0026lt;\u0026lt; std::endl; // 0 std::cout \u0026lt;\u0026lt; A::component_type_id() \u0026lt;\u0026lt; std::endl; // 0 std::cout \u0026lt;\u0026lt; C::component_type_id() \u0026lt;\u0026lt; std::endl; // 2 } Key Points:","title":"Static Local Member"},{"content":"We can customize the (printing) format of a given class by using the specialization of formatter.\n#include \u0026lt;format\u0026gt; #include \u0026lt;iostream\u0026gt; struct Frac { int a, b; }; template \u0026lt;\u0026gt; struct std::formatter\u0026lt;Frac\u0026gt; : std::formatter\u0026lt;string_view\u0026gt; { // parse() is inherited from the base class std::formatter\u0026lt;string_view\u0026gt; // * an efficient solution: auto format(const Frac\u0026amp; frac, std::format_context\u0026amp; ctx) const { return std::format_to(ctx.out(), \u0026#34;{}/{}\u0026#34;, frac.a, frac.b); } // the same functionality as above, but inefficient due to the temporary string // auto format(const Frac\u0026amp; frac, std::format_context\u0026amp; ctx) const { // std::string temp; // std::format_to(std::back_inserter(temp), \u0026#34;{}/{}\u0026#34;, // frac.a, frac.b); // return std::formatter\u0026lt;string_view\u0026gt;::format(temp, ctx); // } }; void print(std::string_view fmt,auto\u0026amp;\u0026amp;...args){ std::cout \u0026lt;\u0026lt; std::vformat(fmt, std::make_format_args(std::forward\u0026lt;decltype(args)\u0026gt;(args)...)); } int main() { Frac f{ 1,10 }; print(\u0026#34;{}\u0026#34;, f); // prints \u0026#34;1/10\u0026#34; } ctx: provides access to formatting state consisting of the formatting arguments and the output iterator. ctx.out(): the iterator to the output buffer std::format_to(): append parts of the formatted output to the target destination. ","permalink":"https://yuang-chen.github.io/posts/2023-08-25-formatter-specialization/","summary":"We can customize the (printing) format of a given class by using the specialization of formatter.\n#include \u0026lt;format\u0026gt; #include \u0026lt;iostream\u0026gt; struct Frac { int a, b; }; template \u0026lt;\u0026gt; struct std::formatter\u0026lt;Frac\u0026gt; : std::formatter\u0026lt;string_view\u0026gt; { // parse() is inherited from the base class std::formatter\u0026lt;string_view\u0026gt; // * an efficient solution: auto format(const Frac\u0026amp; frac, std::format_context\u0026amp; ctx) const { return std::format_to(ctx.out(), \u0026#34;{}/{}\u0026#34;, frac.a, frac.b); } // the same functionality as above, but inefficient due to the temporary string // auto format(const Frac\u0026amp; frac, std::format_context\u0026amp; ctx) const { // std::string temp; // std::format_to(std::back_inserter(temp), \u0026#34;{}/{}\u0026#34;, // frac.","title":"Formatter Specialization"},{"content":"User Defined Literals (UDL) produces an object in an interesting way:\nconstexpr auto operator\u0026#34;\u0026#34;_f(const char* fmt, size_t) { return[=]\u0026lt;typename... T\u0026gt;(T\u0026amp;\u0026amp;... Args) { return std::vformat(fmt, std::make_format_args(std::forward\u0026lt;T\u0026gt;(Args)...)); }; } auto s = \u0026#34;example {} see {}\u0026#34;_f(\u0026#34;yep\u0026#34;, 1.1); // s = \u0026#34;example yep 1.1\u0026#34; The UDL _f has the same effect of std::format(\u0026quot;example {} see {}\u0026quot;, \u0026quot;yep\u0026quot;, 1.1). Pretty familiar (as libfmt), right?\nNow, let\u0026rsquo;s break the definition of _f down:\nint x = 10; double y = 3.14; // 0. std::string format( std::format_string\u0026lt;Args...\u0026gt; fmt, Args\u0026amp;\u0026amp;... args ); // basic format string. // it is an error if the format string is not a constant expression, i.e., known at compile-time, // std::vformat can be used in this case. std::string s = std::format(\u0026#34;x is {} and y is {}\u0026#34;, x, y); // 1. std::string vformat( std::string_view fmt, std::format_args args ); // It\u0026#39;s useful in scenarios where you need to pass around the arguments // separately from the format string auto args = std::make_format_args(x, y); std::string s = std::vformat(\u0026#34;x is {} and y is {}\u0026#34;, args); // 2. a variadic lambda template return [=]\u0026lt;typename... T\u0026gt;(T\u0026amp;\u0026amp;... Args) {...} // 3. user-defined literals // this UDL returns variadic lambda template // e.g., auto str = \u0026#34;haha\u0026#34;_f(x, y) // -\u0026gt; auto f = \u0026#34;haha\u0026#34;_f; auto str = f(x,y); constexpr auto operator\u0026#34;\u0026#34;_f(const char* fmt, size_t) { return [=]\u0026lt;typename... T\u0026gt;(T\u0026amp;\u0026amp;... Args) {...} } // 4. BUG! While the lambda itself is constexpr and the captured fmt // is a pointer to a string literal (which is also a compile-time constant), // the usage of fmt inside the lambda isn\u0026#39;t considered a constant expression // in the strictest sense when passed to std::format. for constexpr auto operator\u0026#34;\u0026#34;_f(const char* fmt, size_t) { return [=]\u0026lt;typename... T\u0026gt;(T\u0026amp;\u0026amp;... Args) { return std::format(fmt, std::forward\u0026lt;T\u0026gt;(Args)...); }; } //***************************// //Combining all above, we get: //***************************// constexpr auto operator\u0026#34;\u0026#34;_f(const char* fmt, size_t) { return[=]\u0026lt;typename... T\u0026gt;(T\u0026amp;\u0026amp;... Args) { return std::vformat(fmt, std::make_format_args(std::forward\u0026lt;T\u0026gt;(Args)...)); }; } //e.g., put the \u0026#34;yep\u0026#34; string into the {} of the example string. auto s = \u0026#34;example {}\u0026#34;_f(\u0026#34;yep\u0026#34;); ","permalink":"https://yuang-chen.github.io/posts/2023-08-22-user-defined-literals/","summary":"User Defined Literals (UDL) produces an object in an interesting way:\nconstexpr auto operator\u0026#34;\u0026#34;_f(const char* fmt, size_t) { return[=]\u0026lt;typename... T\u0026gt;(T\u0026amp;\u0026amp;... Args) { return std::vformat(fmt, std::make_format_args(std::forward\u0026lt;T\u0026gt;(Args)...)); }; } auto s = \u0026#34;example {} see {}\u0026#34;_f(\u0026#34;yep\u0026#34;, 1.1); // s = \u0026#34;example yep 1.1\u0026#34; The UDL _f has the same effect of std::format(\u0026quot;example {} see {}\u0026quot;, \u0026quot;yep\u0026quot;, 1.1). Pretty familiar (as libfmt), right?\nNow, let\u0026rsquo;s break the definition of _f down:\nint x = 10; double y = 3.","title":"User Defined Literals"},{"content":"Reference: here.\nThe return of overloaded operator should be a reference, otherwise return-by-code will create a (temporary) rvalue that cannot be passed to the next operation f2 by non-const reference. i.e., rvalue cannot be non-const referenced.\n#include \u0026lt;vector\u0026gt; #include \u0026lt;iostream\u0026gt; #include \u0026lt;functional\u0026gt; template\u0026lt;typename T, typename FN\u0026gt; requires std::invocable\u0026lt;FN, T\u0026amp;\u0026gt; // diff std::invocable? std::vector\u0026lt;T\u0026gt;\u0026amp; operator| (std::vector\u0026lt;T\u0026gt;\u0026amp; vec, FN fn) noexcept { for(auto\u0026amp; e: vec) { fn(e); } return vec; } int main(){ std::vector v{1, 2, 3}; auto f1 = [](int\u0026amp; i) {i *= i; }; std::function f2 {[](const int\u0026amp; i) {std::cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#39; \u0026#39;; } }; v | f1 | f2; }``` ","permalink":"https://yuang-chen.github.io/posts/2023-08-17-operator-overload/","summary":"Reference: here.\nThe return of overloaded operator should be a reference, otherwise return-by-code will create a (temporary) rvalue that cannot be passed to the next operation f2 by non-const reference. i.e., rvalue cannot be non-const referenced.\n#include \u0026lt;vector\u0026gt; #include \u0026lt;iostream\u0026gt; #include \u0026lt;functional\u0026gt; template\u0026lt;typename T, typename FN\u0026gt; requires std::invocable\u0026lt;FN, T\u0026amp;\u0026gt; // diff std::invocable? std::vector\u0026lt;T\u0026gt;\u0026amp; operator| (std::vector\u0026lt;T\u0026gt;\u0026amp; vec, FN fn) noexcept { for(auto\u0026amp; e: vec) { fn(e); } return vec; } int main(){ std::vector v{1, 2, 3}; auto f1 = [](int\u0026amp; i) {i *= i; }; std::function f2 {[](const int\u0026amp; i) {std::cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#39; \u0026#39;; } }; v | f1 | f2; }``` ","title":"Operator Overload"},{"content":"Finally, C++23 allows overload for the subscript operator [] to be multi-dimensional.\nBefore that, we normally either use:\nvector of vector to form a matrix, and access it as mat[i][j] a class containing a big 1-d vector, but behaves as 2-d by overloading the operator (), e.g., mat(i,j) Now, with C++23, we advance the second option (which offers efficient memory access) with better indexing approaching as follow:\ntemplate \u0026lt;typename T, size_t R, size_t C\u0026gt; struct matrix { T\u0026amp; operator[](size_t const r, size_t const c) noexcept { return data_[r * C + c]; } T const\u0026amp; operator[](size_t const r, size_t const c) const noexcept { return data_[r * C + c]; } static constexpr size_t Rows = R; static constexpr size_t Columns = C; private: std::array\u0026lt;T, R * C\u0026gt; data_; }; int main() { matrix\u0026lt;int, 3, 2\u0026gt; m; for(size_t i = 0; i \u0026lt; m.Rows; ++i) { for(size_t j = 0; j \u0026lt; m.Columns; ++j) { m[i, j] = i * m.Columns + (j + 1); } } for(size_t i = 0; i \u0026lt; m.Rows; ++i) { for(size_t j = 0; j \u0026lt; m.Columns; ++j) { std::cout \u0026lt;\u0026lt; m[i, j] \u0026lt;\u0026lt; \u0026#39; \u0026#39;; } std::cout \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; } } ","permalink":"https://yuang-chen.github.io/posts/2023-05-13-multidim-subscript-operator/","summary":"Finally, C++23 allows overload for the subscript operator [] to be multi-dimensional.\nBefore that, we normally either use:\nvector of vector to form a matrix, and access it as mat[i][j] a class containing a big 1-d vector, but behaves as 2-d by overloading the operator (), e.g., mat(i,j) Now, with C++23, we advance the second option (which offers efficient memory access) with better indexing approaching as follow:\ntemplate \u0026lt;typename T, size_t R, size_t C\u0026gt; struct matrix { T\u0026amp; operator[](size_t const r, size_t const c) noexcept { return data_[r * C + c]; } T const\u0026amp; operator[](size_t const r, size_t const c) const noexcept { return data_[r * C + c]; } static constexpr size_t Rows = R; static constexpr size_t Columns = C; private: std::array\u0026lt;T, R * C\u0026gt; data_; }; int main() { matrix\u0026lt;int, 3, 2\u0026gt; m; for(size_t i = 0; i \u0026lt; m.","title":"Multidimensional Subscript Operator []"},{"content":"🦥 An old note.\nBitwise vs Arithmetic running on a vector of size 2^31, bitwise operations are significantly faster than arithmetic counterparts:\nseg = 64; volume = (vec_size - 1)/ seg + 1; unsigned bs = log2(seg); unsigned bv= log2(volume); unsigned bbv = volume - 1; Arithmetic: out[i] = i % volume * seg + i / volume\nBitwise: out[i] = ((i \u0026amp; bbv) \u0026lt;\u0026lt; bs) + (i \u0026gt;\u0026gt; bv)\nThe time difference:\nmethods time (10^-6s) arithmetic 18.92 bitwise 5.58 ","permalink":"https://yuang-chen.github.io/posts/2023-05-07-bitwise-op/","summary":"🦥 An old note.\nBitwise vs Arithmetic running on a vector of size 2^31, bitwise operations are significantly faster than arithmetic counterparts:\nseg = 64; volume = (vec_size - 1)/ seg + 1; unsigned bs = log2(seg); unsigned bv= log2(volume); unsigned bbv = volume - 1; Arithmetic: out[i] = i % volume * seg + i / volume\nBitwise: out[i] = ((i \u0026amp; bbv) \u0026lt;\u0026lt; bs) + (i \u0026gt;\u0026gt; bv)","title":"Bitwise Op"},{"content":"The results look suspicious to me\u0026hellip; But I wrote down this note many days ago 🦥. Maybe I need to evaluate it again.\nMultiple Parallel Regions The cost of constructing parallel region is expensive in OpenMP. Let\u0026rsquo;s use two example for illustration:\nThree loops operating on a vector of size 2^31, e.g.,\nfor(size_t i = 0; i \u0026lt; vec.size(); i++) vec[i] += 1, vec[i] *= 0.9, vec[i] /= 7, Case 1: a large parallel region including the three loops by omp parallel { omp for }\nCase 2: three separate parallel region are built for each loop via omp parallel for.\nThe time difference:\n#parallel region time (ms) one 2.59 three 0.57 The result is contradictory to our intuition, as we expect a big parallel region (case 1) to run faster than three regions (case 2). The contradition results from the expensive overhead of building the big parallel region. By breaking down the performance and measuring the three loops respetively, we obtain:\nloop one three init 2.298 / 1st 0.017 0.057 2nd 0.011 0.032 3rd 0.020 0.030 The initialization of a parallel region is extremely expensive (i.e., 2.298ms), which consumes even more time than the computational tasks in our case. Within the parallel region of case 1, each loop costs shorter than their counterparts in case 2. Thus, together with the initialization phase, the computing tasks in case 1 deliver suboptimal performance than the sum of individual regions.\n","permalink":"https://yuang-chen.github.io/posts/2023-05-02-omp-parallel-region/","summary":"The results look suspicious to me\u0026hellip; But I wrote down this note many days ago 🦥. Maybe I need to evaluate it again.\nMultiple Parallel Regions The cost of constructing parallel region is expensive in OpenMP. Let\u0026rsquo;s use two example for illustration:\nThree loops operating on a vector of size 2^31, e.g.,\nfor(size_t i = 0; i \u0026lt; vec.size(); i++) vec[i] += 1, vec[i] *= 0.9, vec[i] /= 7, Case 1: a large parallel region including the three loops by omp parallel { omp for }","title":"Omp Parallel Region"},{"content":"One of my old-day notes 🦥.\nCollapse of Nested Loops The collapse clause converts a prefect nested loop into a single loop then parallelize it. The condition of a perfect nested loop is that, the inner loop is tightly included by the outer loop, and no other codes lying between:\nfor(int i = 0 ... ) { for(int j = 0 ...) { task[i][j]; } } Such condition is hard to meet. Moreover, it best suits with (1) the static scheduler instead of the dynamic one, and (2) when the parallelism of the outer loop is smaller than the number of threads, i.e., i \u0026lt; num_threads.\nIn the situation where the workload is imbalanced and the parallelism of outer loop is not an issue, the dynamic scheduler still performs better as in our test cases.\nExperimental setting: only change the Scatter phase with the following scheduling policy,\nscheduler time (s) dynamic collapse 2.004 static collapse 1.329 dynamic 1.276 ","permalink":"https://yuang-chen.github.io/posts/2023-05-02-omp-collapse/","summary":"One of my old-day notes 🦥.\nCollapse of Nested Loops The collapse clause converts a prefect nested loop into a single loop then parallelize it. The condition of a perfect nested loop is that, the inner loop is tightly included by the outer loop, and no other codes lying between:\nfor(int i = 0 ... ) { for(int j = 0 ...) { task[i][j]; } } Such condition is hard to meet.","title":"Omp Collapse"},{"content":"Another post recycled from my earlier notes. I really don\u0026rsquo;t have motivation to improve it further 🦥.\nVector vs Array Initilization The Vector is the preferred choice for data storage in mordern C++. It is internally implemented based on the Array. However, the performance gap between the two is indeed obvious.\nThe Vector can be initialized via std::vector\u0026lt;T\u0026gt; vec(size). Meanwhile, an Array is initialized by T* arr = new T[size]\nallocate data field of size 2^63 (initializing to default value Zero), the time difference:\nData Field time (1^-6s) Vector 14.18 Array 3.67 !! smart pointers (e.g., unique_ptr) are as slow as vector.\ncreating multi-dimensional arrays/vectors, the array of arrays still performs as the fastest one:\nArray of Array: T** AoA = new T [size] Array of Vector: vector\u0026lt;T\u0026gt;* AoV = new vector\u0026lt;T\u0026gt; [size] Vector of Vector: vector\u0026lt;vector\u0026lt;T\u0026gt;\u0026gt; = VoV(size) Additionally, Static Array of Vector: vector\u0026lt;T\u0026gt; SAoV [size]. We ignore the static array of (static) array because they are not often used for large volume of data. Given the 1st dimension to be 2^10 and the 2nd dimension to be 2^31, the time difference between those structures are:\nStructure time (1^-6s) AoA 0.005417 SAoA 0.008417 AoV 0.010792 VoV 2.92248 Filling C++ also provides function to fill the array/vector with the same elements, by the function std::fill. Additionally, vector can also be initialized with the same value. Below is the performance variation over different approaches.\nmethod time (1^-6s) fill arr 0.003667 fill vec 0.003667 init vec 0.007917 fill_n arr 0.01225 omp parallel for 8.55099 omp parallel for simd 2.32663 using std::fill achieves the best performance for both array and vector. The initialization of vector with a specific value (i.e., init vec) performs as the second fastest method, which is mainly decelerated by vector\u0026rsquo;s inherent overhead. std::fill_n is (somehow?) substantially slower than the popular std::fill. Do NOT rely on the omp for simple data assignment, as its performance is TOO poor!!!\nUsage of C-style Array employ delete/delete[] when new/new[] are used. The number of delete and new shall be matched.\nAn object is created by new. When using delete, the destructor of the pointed-to object is called before de-allocation.\nSimilarly, an array of objects are allocated using new[]. The destructors of all created objects are called when calling the array version delete[].\nThus, if an array of vector is created using new[], e.g., AoV in the prior section, one just need one delete[] to release all resource. There is NO need to delete/erase/clear any vector objects before deleting the array.\nAs for the vector, which requires no new nor delete, the destructor of a std::vector automatically calls the destructors of the contained objects. Much more convenient (at the cost of speed), as programmers carry no burden of counting the new and delete pairs.\n","permalink":"https://yuang-chen.github.io/posts/2023-05-01-vector-vs-array/","summary":"Another post recycled from my earlier notes. I really don\u0026rsquo;t have motivation to improve it further 🦥.\nVector vs Array Initilization The Vector is the preferred choice for data storage in mordern C++. It is internally implemented based on the Array. However, the performance gap between the two is indeed obvious.\nThe Vector can be initialized via std::vector\u0026lt;T\u0026gt; vec(size). Meanwhile, an Array is initialized by T* arr = new T[size]","title":"Vector vs Array"},{"content":"Writing SIMD code that works across different platforms can be a challenging task. The following log illustrates how a seemingly simple operation in C++ can quickly escalate into a significant problem.\nLet\u0026rsquo;s look into the code below, where the elements of x is accessed through indices specified by idx.\nnormal code std::vector\u0026lt;float\u0026gt; x = /*some data*/ std::vector\u0026lt;int\u0026gt; idx = /* index */ for(auto i: idx) { auto data = x[i]; } Gather with Intel In AVX512, Gather is a specific intrinsic function to transfer data from a data array to a target vec, according to an index vec. This intrinsic vectorizes the example of normal code.\nSIMD code int simd_width = 16; for(size_t i = 0; i \u0026lt; x.size(); i+= simd_width) { __m512i idx_vec = _mm512_loadu_epi32(\u0026amp;idx[i]); __m512 x_vec = _mm512_i32gather_ps(idx_vec, \u0026amp;x[0], sizeof(float)); } With Intel\u0026rsquo;s SIMD, the code snippet gets the data from the vector x based on the index register idx_vec and store the resultant data into the result register x_vec.\nPersonally, after a few days of SIMD coding, I do appreciate such code, and consider this SIMD solution is simple and elegant: two instructions are used, which is nicely aligned with what happens in the normal code:\nloading the data from the idx vector; loading the data from x vector according to the result of the 1st step. A big BUT, the gather (and scatter) operation is not supported by most of other sets \u0026ndash; they simply just do NOT offer these instructions 😮‍💨. To achieve the same data loading task, more efforts are needed.\nCustomized Gather with ARM Using ARM intrinsics, we have to implement our own gather. I found three solutions do so and benchmarked their performances.\nTmp array /* tmp array */ int simd_width = 4; aligns(16) std::array\u0026lt;float,simd_width\u0026gt; tmp; for(size_t i = 0; i \u0026lt; x.size(); i += simd_width) { tmp[0] = x[idx[i]]; tmp[1] = x[idx[i + 1]]; tmp[2] = x[idx[i + 2]]; tmp[3] = x[idx[i + 3]]; float32x4_t tmp_vec = vld1q_f32(tmp.data()); // loading to register vst1q_f32(\u0026amp;buf[i], tmp_vec); } A naive solution (suggested by ChatGPT-4 and many GitHub repos) is to load the idx and x[idx] directly without the help of intrinsics, store the data in a temporary array, and then load to the target register. This solution mixes SIMD and non-SIMD. The indexing accesses (e.g., [i]) to the arrays lets the compilers/CPU do whatever they want, which loses the register-level control.\nUnion union alignas(64) f32x4_union { float32x4_t reg128; std::array\u0026lt;float, 4\u0026gt; f32x4; }; f32x4_union res_vec; for(size_t i = 0; i \u0026lt; size; i += 4) { res_vec.f32x4[0] = x[idx[i]]; res_vec.f32x4[1] = x[idx[i + 1]]; res_vec.f32x4[2] = x[idx[i + 2]]; res_vec.f32x4[3] = x[idx[i + 3]]; vst1q_f32(\u0026amp;buf[i], res_vec.reg128); } By put the array and register into a union, we now have the access to the elements of the register by indexing. Compared to the tmp array solution, the union solution avoids the code of loading data to the register (i.e., vld1q_f32), thus improving the efficiency. However, the indexing access is still under the control of the compiler/CPU.\nget \u0026amp; set uint32x4_t idx_vec; float32x4_t x_vec; for(size_t i = 0; i \u0026lt; size; i += 4) { idx_vec = vld1q_u32(\u0026amp;idx[i]); x_vec = vsetq_lane_f32(x[vgetq_lane_u32(idx_vec, 0)], x_vec, 0); x_vec = vsetq_lane_f32(x[vgetq_lane_u32(idx_vec, 1)], x_vec, 1); x_vec = vsetq_lane_f32(x[vgetq_lane_u32(idx_vec, 2)], x_vec, 2); x_vec = vsetq_lane_f32(x[vgetq_lane_u32(idx_vec, 3)], x_vec, 3); vst1q_f32(\u0026amp;buf[i], x_vec); } This solution combines the get and set intrinsics to mimic the advanced gather operation. The code is \u0026hellip; ugly, but efficient. It makes sure that idx_vec and x_vec are carefully reused, allowing the finest control in the registers.\nget \u0026amp; tmp array alignas(16) std::array\u0026lt;float, 4\u0026gt; values; for(size_t i = 0; i \u0026lt; size; i += 4) { uint32x4_t idx_vec = vld1q_u32(\u0026amp;idx[i]); values[0] = x[vgetq_lane_u32(idx_vec, 0)]; values[1] = x[vgetq_lane_u32(idx_vec, 1)]; values[2] = x[vgetq_lane_u32(idx_vec, 2)]; values[3] = x[vgetq_lane_u32(idx_vec, 3)]; float32x4_t x_vec = vld1q_f32(values.data()); vst1q_f32(\u0026amp;buf[i], x_vec); } The last solution mixes get with tmp array. It is an intermediate between the 1nd and 3rd solution in terms of the use of registers.\nBenchmarking With data size of 1\u0026lt;\u0026lt;27, the performance of the four solutions are:\nperformance union tmp get\u0026amp;set get\u0026amp;tmp time (ms) 703 639 583 648 assembly code lines 16 16 18 18 The union solution yields the shortest assembly code but the longest execution time. This short code piece is reasonable, as it eliminates one line code compared with the tmp method. But why so long time? The key reason is that, the writing to the union elements by indexing (res_vec.f32x4[i] = ...) is inefficient, compared to the use of intrinsic vld1q_f32(*ptr). Explicit control on the registers promises better performance! The get\u0026amp;set facilitates the finest control of registers, and thus gives the shortest time.\nA weird result is acquired by get\u0026amp;tmp. It actually slightly slower than tmp and I do not understand why. I feed the assembly code of the last three solutions to ChatGPT-4, and this is its analysis:\ntmp \u0026ndash; L10 (first loop): This loop involves more register manipulation (bfi, fmov, ins) compared to the other loops. It may contribute to higher register pressure and could potentially limit instruction-level parallelism, affecting performance.\nget\u0026amp;set \u0026ndash; L11 (second loop): This loop uses ld1 and ld1q instructions to load the required values into SIMD registers directly from memory. This reduces the amount of register manipulation required compared to L10, which could lead to better performance.\nget\u0026amp;tmp \u0026ndash; L12 (third loop): This loop uses a similar approach to L10, using a mix of bfi, fmov, and ins instructions to manipulate registers. However, it also involves an additional ldr instruction for each iteration, which increases the amount of memory operations per iteration compared to L10. This could potentially explain why L12 is slower than L10.\nIn conclusion, the second loop (L11) has the least amount of register manipulation and memory operations per iteration, which may contribute to its better performance. The third loop (L12) has more memory operations per iteration compared to the first loop (L10), which could lead to a slower execution time.\nTo wrap up, the get\u0026amp;set invokes the least number of instructions, thus yielding the best performance. This reason seems to make sense.\nThe complete code is here\n","permalink":"https://yuang-chen.github.io/posts/2023-04-27-gather-simd/","summary":"Writing SIMD code that works across different platforms can be a challenging task. The following log illustrates how a seemingly simple operation in C++ can quickly escalate into a significant problem.\nLet\u0026rsquo;s look into the code below, where the elements of x is accessed through indices specified by idx.\nnormal code std::vector\u0026lt;float\u0026gt; x = /*some data*/ std::vector\u0026lt;int\u0026gt; idx = /* index */ for(auto i: idx) { auto data = x[i]; } Gather with Intel In AVX512, Gather is a specific intrinsic function to transfer data from a data array to a target vec, according to an index vec.","title":" Gather with SIMD"},{"content":"Writing code with SIMD for vectorization is painful. It deserves a blog series to record all sorts of pains I have encountered and (partially) overcome.\nIndeed, once the pain of coding and debugging is finished, the program is lightning-faster. Nonetheless, I am here to complain instead of praising. Let me state why writing SIMD code is causing me emotional damage:\na single line of normal c++ code could be easily inflated to a dozen lines of code. when the code comes with data dependency across loop iterations, the SIMD would hit right at my front head and give me massive headache (for debugging). the usage of SIMD require low-level C coding SIMD intrinsics are often not compatible across different platforms, and even different CPU models. SIMD intrinsics are available in ARM, Intel, AMD and Nvidia chips, but GPU/CUDA opens another genre of SIMD programming paradigm so I will not discuss here. AMD, for the x86 arch, offers the same intrinsic set as Intel does. Thus, only the intrinsics of ARM and Intel are really concerned.\nNotation Before going any further, I would like firstly clarify the terms of \u0026ldquo;vector\u0026rdquo; used in this blog, which unfortunately can be used to name two distinct matters.\nvector in C++, is a container with variable size holding dynamically allocated data in heap. vector in SIMD, is a type specifying the data stored in registers, with fixed sizes such as 128, 256, 512 bits. In following context, vector refers to as the container, and vec denotes the data in register.\nIntel Intrinsic Let\u0026rsquo;s talk about the Intel firstly, the (aging) boss of CPU.\nIntel provides a number of intrinsic sets to us, such as SSE, AVX2, \u0026hellip;, AVX512. I only use AVX512 because it is the newest and widest set.\nnew means that AVX512 has something other sets do not have, for instance, the scatter and gather operations. The two counterparts are very useful, which is further discussed in another log.\nwide means AVX512 has 512-bit width vec. It is not obvious to see from the name at all.\nARM Intrinsics The computing capacity of ARM chip is weaker than that of Intel\u0026rsquo;s chip \u0026ndash; I derive this personal and irresponsible conclusion based on the fact that the SIMD width of ARM is merely 128 bits and sometimes is even 64 bits. Why so short? I guess ARM prefers to 8-bit or 16-bit data type, sacrificing a little precision for efficiency, which makes the shorter vec more reasonable.\nAnother shortcoming of ARM intrinsics is the lack of masked operations. It happens all the time when the input data cannot be exactly fitted in the SIMD vec, or I just need a portion of data. The mask in Intel intrinsics allows us to easily extract/fill the imperfectly aligned vec. For ARM, sorry, we have to find alternative solutions, as described in this [log]\n","permalink":"https://yuang-chen.github.io/posts/2023-04-25-simd-pain-intro/","summary":"Writing code with SIMD for vectorization is painful. It deserves a blog series to record all sorts of pains I have encountered and (partially) overcome.\nIndeed, once the pain of coding and debugging is finished, the program is lightning-faster. Nonetheless, I am here to complain instead of praising. Let me state why writing SIMD code is causing me emotional damage:\na single line of normal c++ code could be easily inflated to a dozen lines of code.","title":"SIMD is Pain"},{"content":"The content of this post is extracted from my previous random notes. I am too lazy to update and organize it 🦥.\nC++17 new feature \u0026ndash; parallel algorithms The parallel algorithms and execution policies are introduced in C++17. Unfortuantely, according to CppReference, only GCC and Intel support these features. Clang still leaves them unimplemented.\nA blog about it.\nThe parallel library brough by C++17 requires the usage of Intel\u0026rsquo;s oneTBB for multithreading.\nHowever, there are version conflicts between gcc and oneTBB, as mentioned in issue1 and issue2. Thus, we need to match the gcc with oneTBB for correct version:\ng++11 with oneTBB.2021 (tested). g++9/10 with oneTBB.2019 (untested). Moreoever, the TBB-backboned parallel algorithms does not promise superior performance (perphase due to the implementation overheads), according to the discussion here. Programmers may need to implement their own parallel algorithms to achieve optimal speed.\nFast Parallel Algorithms For implementation, we test std::, tbb-based std::parallel with par and par_unseq, and gnu_parallel for performance evaluation. gnu_parallel performs as the fastest toolkits.\nTODO: I should implement all those algorithms by myself in the near future.\nSorting gnu_parallel is favored by someones\nWhen operating on a vector of size 2^31, the performance of various implementations are:\nmethods time (10^-6s) std:: 15.87 par 2373.95 par_unseq 11.50 gnu_parallel 6.54 Prefix Sum It is also a well-studied algorithm, as descripted by link.\nWhen operating on a vector of size 2^31, the performance of various implementations are:\nmethods time (10^-6s) std:: 5.08 par 5.58 par_unseq 5.42 gnu_parallel 4.25 Conclusion libstdc++ offers built-in parallel implementations for a variety of algorithms, including sort and partial_sum. The parallel mode is implicitly enabled during the compilation with -fopenmp and _GLIBCXX_PARALLEL.\nMoreover, the parallel components called by e.g., std::sort are in fact the gnu_parallel codes. We can also explicitly call gnu_parallel by including the header, e.g.,\u0026lt;parallel/algorithm\u0026gt;. Compared with the parallel std::, gnu_parallel incurs smaller overhead and thus delivers (slightly) better performance.\nThe tbb-based parallel methods, which is the new feature of C++17, are unsatisfactory. The par policy behaves extremely poorly in Sorting and a bit bad in Prefix Sum. Suprisingly, the par_unseq policy (parallelism + vectorization) is rather good in Sorting, only second to gnu_parallel. The TBB and its optimization strategies remain to be explored in the future.\nMore details regarding gnu_parallel can be found on this page.\n","permalink":"https://yuang-chen.github.io/posts/2023-04-25-par-algo/","summary":"The content of this post is extracted from my previous random notes. I am too lazy to update and organize it 🦥.\nC++17 new feature \u0026ndash; parallel algorithms The parallel algorithms and execution policies are introduced in C++17. Unfortuantely, according to CppReference, only GCC and Intel support these features. Clang still leaves them unimplemented.\nA blog about it.\nThe parallel library brough by C++17 requires the usage of Intel\u0026rsquo;s oneTBB for multithreading.","title":"Parallel Algorithms from Libraries"},{"content":"I am now a Postdoc at CUHK, and graduated from CUHK on Shenzhen campus for my PhD.\nMy research focuses on High-Performance Graph Processing on Multicore Systems, where the irregularrity of graph workloads are exploited to maximize the computing efficiency of modern multicore machines. I am broadly interesed in graph-related problems on diverse computing platforms, including CPUs (OpenMP), GPUs (CUDA), and clusters (MPI, RMDA). The core of my reseach is to investigate the interaction between graphs and underlying hardware.\nSpecifically, I am looking into three types of research problems:\nBoosting generic graph algorithms on CPUs, e,g, PageRank, BFS, Triangle Counting, etc. Accelerating graph matrix multiplication (SpMV + SpMM) on CPUs and GPUs, which is a fundamental building block in scientific computing. GNN speedup. Also, I am a fan of modern C++ programming. I follow closely to C++20 and spent a lot of time in meta programming.\nIn 2018, I received my dual master degrees in Multicore Systems from TU Eindhoven, Netherlands and TU Berlin, Germany with full scholarship granted by European Union. In 2017-2018, I worked as Masterarbeitor (i.e., intern) in Frauhofer FOKUS to build Germany\u0026rsquo;s e-health infrastructure, based on which I developed my master thesis \u0026ldquo;Providing the Infrastructure for SICCT Protocol Tests with Focus on Service Discovery and Pairing\u0026rdquo;.\nIn 2015, I obtained my bachelor degree at Huazhong University of Science and Technology (HUST). Also, in 2014-2015, being sponsored by CSC scholarship, I studied as an exchange student in RWTH Aachen, Germany.\nMy CV is da.\nPublications YuAng Chen and Yeh-Ching Chung, “Workload Balancing via Graph Reordering on Multicore Systems,” IEEE Transactions on Parallel and Distributed Systems (TPDS), Vol. 33, No. 5, 2022, pp. 1231-1245.\nYuAng Chen and Yeh-Ching Chung, “HiPa: Hierarchical Partitioning for Fast Page Rank on Multicore Systems,” Proceedings of IEEE International conference on Parallel Processing (ICPP), Article No. 24, 2021, pp. 1-10.\nYuAng Chen and Yeh-Ching Chung, “Corder: Cache-Aware Reordering For Optimizing Graph Analytics,” Proceedings of ACM International conference on Principles and Practice of Parallel Programming (PPoPP), 2021, pp.472-473.\nYuAng Chen and Yeh-Ching Chung, \u0026ldquo;A Unequal Caching Strategy for Shared-Memory Graph Analytics\u0026rdquo;, to apear in IEEE Transactions on Parallel and Distributed Systems (TPDS).\nOngoing Work\nYuAng Chen and Yeh-Ching Chung, \u0026ldquo;Co-Design of Mixed Matrix Format and Processing Engine for Efficient SpMV on Graphs\u0026rdquo;, in praparation\nYuAng Chen and Yeh-Ching Chung, \u0026quot; Locality Extraction \u0026amp; Blocking for Graph Adjacency Matrix Multiplications\u0026quot;, in preparation\nTeaching 2020 Spring : Introduction to Programming Methodology\n2020 Fall : Operating System\n2021 Spring : Compiler Design\n2021 Fall : Operating System\n2022 Spring : Computer Architecture\n","permalink":"https://yuang-chen.github.io/about/aboutme/","summary":"I am now a Postdoc at CUHK, and graduated from CUHK on Shenzhen campus for my PhD.\nMy research focuses on High-Performance Graph Processing on Multicore Systems, where the irregularrity of graph workloads are exploited to maximize the computing efficiency of modern multicore machines. I am broadly interesed in graph-related problems on diverse computing platforms, including CPUs (OpenMP), GPUs (CUDA), and clusters (MPI, RMDA). The core of my reseach is to investigate the interaction between graphs and underlying hardware.","title":"About Me"}]