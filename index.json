[{"content":"Writing SIMD code that works across different platforms can be a challenging task. The following log illustrates how a seemingly simple operation in C++ can quickly escalate into a significant problem.\nLet\u0026rsquo;s look into the code below, where the elements of x is accessed through indices specified by idx.\nnormal code std::vector\u0026lt;float\u0026gt; x = /*some data*/ std::vector\u0026lt;int\u0026gt; idx = /* index */ for(auto i: idx) { auto data = x[i]; } Gather with Intel In AVX512, Gather is a specific intrinsic function to transfer data from a data array to a target vec, according to an index vec. This intrinsic vectorizes the example of normal code.\nSIMD code int simd_width = 16; for(size_t i = 0; i \u0026lt; x.size(); i+= simd_width) { __m512i idx_vec = _mm512_loadu_epi32(\u0026amp;idx[i]); __m512 x_vec = _mm512_i32gather_ps(idx_vec, \u0026amp;x[0], sizeof(float)); } With Intel\u0026rsquo;s SIMD, the code snippet gets the data from the vector x based on the index register idx_vec and store the resultant data into the result register x_vec.\nPersonally, after a few days of SIMD coding, I do appreciate such code, and consider this SIMD solution is simple and elegant: two instructions are used, which is nicely aligned with what happens in the normal code:\nloading the data from the idx vector; loading the data from x vector according to the result of the 1st step. A big BUT, the gather (and scatter) operation is not supported by most of other sets \u0026ndash; they simply just do NOT offer these instructions üòÆ‚Äçüí®. To achieve the same data loading task, more efforts are needed.\nCustomized Gather with ARM Using ARM intrinsics, we have to implement our own gather. I found three solutions do so and benchmarked their performances.\nTmp array /* tmp array */ int simd_width = 4; aligns(16) std::array\u0026lt;float,simd_width\u0026gt; tmp; for(size_t i = 0; i \u0026lt; x.size(); i += simd_width) { tmp[0] = x[idx[i]]; tmp[1] = x[idx[i + 1]]; tmp[2] = x[idx[i + 2]]; tmp[3] = x[idx[i + 3]]; float32x4_t tmp_vec = vld1q_f32(tmp.data()); // loading to register vst1q_f32(\u0026amp;buf[i], tmp_vec); } A naive solution (suggested by ChatGPT-4 and many GitHub repos) is to load the idx and x[idx] directly without the help of intrinsics, store the data in a temporary array, and then load to the target register. This solution mixes SIMD and non-SIMD. The indexing accesses (e.g., [i]) to the arrays lets the compilers/CPU do whatever they want, which loses the register-level control.\nUnion union alignas(64) f32x4_union { float32x4_t reg128; std::array\u0026lt;float, 4\u0026gt; f32x4; }; f32x4_union res_vec; for(size_t i = 0; i \u0026lt; size; i += 4) { res_vec.f32x4[0] = x[idx[i]]; res_vec.f32x4[1] = x[idx[i + 1]]; res_vec.f32x4[2] = x[idx[i + 2]]; res_vec.f32x4[3] = x[idx[i + 3]]; vst1q_f32(\u0026amp;buf[i], res_vec.reg128); } By put the array and register into a union, we now have the access to the elements of the register by indexing. Compared to the tmp array solution, the union solution avoids the code of loading data to the register (i.e., vld1q_f32), thus improving the efficiency. However, the indexing access is still under the control of the compiler/CPU.\nget \u0026amp; set uint32x4_t idx_vec; float32x4_t x_vec; for(size_t i = 0; i \u0026lt; size; i += 4) { idx_vec = vld1q_u32(\u0026amp;idx[i]); x_vec = vsetq_lane_f32(x[vgetq_lane_u32(idx_vec, 0)], x_vec, 0); x_vec = vsetq_lane_f32(x[vgetq_lane_u32(idx_vec, 1)], x_vec, 1); x_vec = vsetq_lane_f32(x[vgetq_lane_u32(idx_vec, 2)], x_vec, 2); x_vec = vsetq_lane_f32(x[vgetq_lane_u32(idx_vec, 3)], x_vec, 3); vst1q_f32(\u0026amp;buf[i], x_vec); } This solution combines the get and set intrinsics to mimic the advanced gather operation. The code is \u0026hellip; ugly, but efficient. It makes sure that idx_vec and x_vec are carefully reused, allowing the finest control in the registers.\nget \u0026amp; tmp array alignas(16) std::array\u0026lt;float, 4\u0026gt; values; for(size_t i = 0; i \u0026lt; size; i += 4) { uint32x4_t idx_vec = vld1q_u32(\u0026amp;idx[i]); values[0] = x[vgetq_lane_u32(idx_vec, 0)]; values[1] = x[vgetq_lane_u32(idx_vec, 1)]; values[2] = x[vgetq_lane_u32(idx_vec, 2)]; values[3] = x[vgetq_lane_u32(idx_vec, 3)]; float32x4_t x_vec = vld1q_f32(values.data()); vst1q_f32(\u0026amp;buf[i], x_vec); } The last solution mixes get with tmp array. It is an intermediate between the 1nd and 3rd solution in terms of the use of registers.\nBenchmarking With data size of 1\u0026lt;\u0026lt;27, the performance of the four solutions are:\nperformance union tmp get\u0026amp;set get\u0026amp;tmp time (ms) 703 639 583 648 assembly code lines 16 16 18 18 The union solution yields the shortest assembly code but the longest execution time. This short code piece is reasonable, as it eliminates one line code compared with the tmp method. But why so long time? The key reason is that, the writing to the union elements by indexing (res_vec.f32x4[i] = ...) is inefficient, compared to the use of intrinsic vld1q_f32(*ptr). Explicit control on the registers promises better performance! The get\u0026amp;set facilitates the finest control of registers, and thus gives the shortest time.\nA weird result is acquired by get\u0026amp;tmp. It actually slightly slower than tmp and I do not understand why. I feed the assembly code of the last three solutions to ChatGPT-4, and this is its analysis:\ntmp \u0026ndash; L10 (first loop): This loop involves more register manipulation (bfi, fmov, ins) compared to the other loops. It may contribute to higher register pressure and could potentially limit instruction-level parallelism, affecting performance.\nget\u0026amp;set \u0026ndash; L11 (second loop): This loop uses ld1 and ld1q instructions to load the required values into SIMD registers directly from memory. This reduces the amount of register manipulation required compared to L10, which could lead to better performance.\nget\u0026amp;tmp \u0026ndash; L12 (third loop): This loop uses a similar approach to L10, using a mix of bfi, fmov, and ins instructions to manipulate registers. However, it also involves an additional ldr instruction for each iteration, which increases the amount of memory operations per iteration compared to L10. This could potentially explain why L12 is slower than L10.\nIn conclusion, the second loop (L11) has the least amount of register manipulation and memory operations per iteration, which may contribute to its better performance. The third loop (L12) has more memory operations per iteration compared to the first loop (L10), which could lead to a slower execution time.\nMake sense?\nThe complete code is here\n","permalink":"https://yuang-chen.github.io/posts/2023-04-27-gather-simd/","summary":"Writing SIMD code that works across different platforms can be a challenging task. The following log illustrates how a seemingly simple operation in C++ can quickly escalate into a significant problem.\nLet\u0026rsquo;s look into the code below, where the elements of x is accessed through indices specified by idx.\nnormal code std::vector\u0026lt;float\u0026gt; x = /*some data*/ std::vector\u0026lt;int\u0026gt; idx = /* index */ for(auto i: idx) { auto data = x[i]; } Gather with Intel In AVX512, Gather is a specific intrinsic function to transfer data from a data array to a target vec, according to an index vec.","title":" Gather with SIMD"},{"content":"The content of this post is extracted from my previous random notes. I am too lazy to update and organize it ü¶•.\nC++17 new feature \u0026ndash; parallel algorithms The parallel algorithms and execution policies are introduced in C++17. Unfortuantely, according to CppReference, only GCC and Intel support these features. Clang still leaves them unimplemented.\nA blog about it.\nThe parallel library brough by C++17 requires the usage of Intel\u0026rsquo;s oneTBB for multithreading.\nHowever, there are version conflicts between gcc and oneTBB, as mentioned in issue1 and issue2. Thus, we need to match the gcc with oneTBB for correct version:\ng++11 with oneTBB.2021 (tested). g++9/10 with oneTBB.2019 (untested). Moreoever, the TBB-backboned parallel algorithms does not promise superior performance (perphase due to the implementation overheads), according to the discussion here. Programmers may need to implement their own parallel algorithms to achieve optimal speed.\nFast Parallel Algorithms For implementation, we test std::, tbb-based std::parallel with par and par_unseq, and gnu_parallel for performance evaluation. gnu_parallel performs as the fastest toolkits.\nTODO: I should implement all those algorithms by myself in the near future.\nSorting gnu_parallel is favored by someones\nWhen operating on a vector of size 2^31, the performance of various implementations are:\nmethods time (10^-6s) std:: 15.87 par 2373.95 par_unseq 11.50 gnu_parallel 6.54 Prefix Sum It is also a well-studied algorithm, as descripted by link.\nWhen operating on a vector of size 2^31, the performance of various implementations are:\nmethods time (10^-6s) std:: 5.08 par 5.58 par_unseq 5.42 gnu_parallel 4.25 Conclusion libstdc++ offers built-in parallel implementations for a variety of algorithms, including sort and partial_sum. The parallel mode is implicitly enabled during the compilation with -fopenmp and _GLIBCXX_PARALLEL.\nMoreover, the parallel components called by e.g., std::sort are in fact the gnu_parallel codes. We can also explicitly call gnu_parallel by including the header, e.g.,\u0026lt;parallel/algorithm\u0026gt;. Compared with the parallel std::, gnu_parallel incurs smaller overhead and thus delivers (slightly) better performance.\nThe tbb-based parallel methods, which is the new feature of C++17, are unsatisfactory. The par policy behaves extremely poorly in Sorting and a bit bad in Prefix Sum. Suprisingly, the par_unseq policy (parallelism + vectorization) is rather good in Sorting, only second to gnu_parallel. The TBB and its optimization strategies remain to be explored in the future.\nMore details regarding gnu_parallel can be found on this page.\n","permalink":"https://yuang-chen.github.io/posts/2023-04-25-par-algo/","summary":"The content of this post is extracted from my previous random notes. I am too lazy to update and organize it ü¶•.\nC++17 new feature \u0026ndash; parallel algorithms The parallel algorithms and execution policies are introduced in C++17. Unfortuantely, according to CppReference, only GCC and Intel support these features. Clang still leaves them unimplemented.\nA blog about it.\nThe parallel library brough by C++17 requires the usage of Intel\u0026rsquo;s oneTBB for multithreading.","title":"Parallel Algorithms from Libraries"},{"content":"","permalink":"https://yuang-chen.github.io/posts/2023-04-21-overview/","summary":"","title":"2023 04 21 Overview"},{"content":"I am now a Postdoc at CUHK, and graduated from CUHK on Shenzhen campus for my PhD.\nMy research focuses on High-Performance Graph Processing on Multicore Systems, where the irregularrity of graph workloads are exploited to maximize the computing efficiency of modern multicore machines. I am broadly interesed in graph-related problems on diverse computing platforms, including CPUs (OpenMP), GPUs (CUDA), and clusters (MPI, RMDA). The core of my reseach is to investigate the interaction between graphs and underlying hardware.\nSpecifically, I am looking into three types of research problems:\nBoosting generic graph algorithms on CPUs, e,g, PageRank, BFS, Triangle Counting, etc. Accelerating graph matrix multiplication (SpMV + SpMM) on CPUs and GPUs, which is a fundamental building block in scientific computing. GNN speedup. Also, I am a fan of modern C++ programming. I follow closely to C++20 and spent a lot of time in meta programming.\nIn 2018, I received my dual master degrees in Multicore Systems from TU Eindhoven, Netherlands and TU Berlin, Germany with full scholarship granted by European Union. In 2017-2018, I worked as Masterarbeitor (i.e., intern) in Frauhofer FOKUS to build Germany\u0026rsquo;s e-health infrastructure, based on which I developed my master thesis \u0026ldquo;Providing the Infrastructure for SICCT Protocol Tests with Focus on Service Discovery and Pairing\u0026rdquo;.\nIn 2015, I obtained my bachelor degree at Huazhong University of Science and Technology (HUST). Also, in 2014-2015, being sponsored by CSC scholarship, I studied as an exchange student in RWTH Aachen, Germany.\nMy CV is da.\nPublications YuAng Chen and Yeh-Ching Chung, ‚ÄúWorkload Balancing via Graph Reordering on Multicore Systems,‚Äù IEEE Transactions on Parallel and Distributed Systems (TPDS), Vol. 33, No. 5, 2022, pp. 1231-1245.\nYuAng Chen and Yeh-Ching Chung, ‚ÄúHiPa: Hierarchical Partitioning for Fast Page Rank on Multicore Systems,‚Äù Proceedings of IEEE International conference on Parallel Processing (ICPP), Article No. 24, 2021, pp. 1-10.\nYuAng Chen and Yeh-Ching Chung, ‚ÄúCorder: Cache-Aware Reordering For Optimizing Graph Analytics,‚Äù Proceedings of ACM International conference on Principles and Practice of Parallel Programming (PPoPP), 2021, pp.472-473.\nYuAng Chen and Yeh-Ching Chung, \u0026ldquo;A Unequal Caching Strategy for Shared-Memory Graph Analytics\u0026rdquo;, to apear in IEEE Transactions on Parallel and Distributed Systems (TPDS).\nOngoing Work\nYuAng Chen and Yeh-Ching Chung, \u0026ldquo;Co-Design of Mixed Matrix Format and Processing Engine for Efficient SpMV on Graphs\u0026rdquo;, in praparation\nYuAng Chen and Yeh-Ching Chung, \u0026quot; Locality Extraction \u0026amp; Blocking for Graph Adjacency Matrix Multiplications\u0026quot;, in preparation\nTeaching 2020¬†Spring : Introduction to Programming Methodology\n2020¬†Fall¬†: Operating System\n2021 Spring : Compiler Design\n2021 Fall : Operating System\n2022 Spring : Computer Architecture\n","permalink":"https://yuang-chen.github.io/about/aboutme/","summary":"I am now a Postdoc at CUHK, and graduated from CUHK on Shenzhen campus for my PhD.\nMy research focuses on High-Performance Graph Processing on Multicore Systems, where the irregularrity of graph workloads are exploited to maximize the computing efficiency of modern multicore machines. I am broadly interesed in graph-related problems on diverse computing platforms, including CPUs (OpenMP), GPUs (CUDA), and clusters (MPI, RMDA). The core of my reseach is to investigate the interaction between graphs and underlying hardware.","title":"About Me"}]