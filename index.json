[{"content":"The content of this post is extracted from my previous random notes. I am too lazy to update and organize it ü¶•.\nC++17 new feature \u0026ndash; parallel algorithms The parallel algorithms and execution policies are introduced in C++17. Unfortuantely, according to CppReference, only GCC and Intel support these features. Clang still leaves them unimplemented.\nA blog about it.\nThe parallel library brough by C++17 requires the usage of Intel\u0026rsquo;s oneTBB for multithreading.\nHowever, there are version conflicts between gcc and oneTBB, as mentioned in issue1 and issue2. Thus, we need to match the gcc with oneTBB for correct version:\ng++11 with oneTBB.2021 (tested). g++9/10 with oneTBB.2019 (untested). Moreoever, the TBB-backboned parallel algorithms does not promise superior performance (perphase due to the implementation overheads), according to the discussion here. Programmers may need to implement their own parallel algorithms to achieve optimal speed.\nFast Parallel Algorithms For implementation, we test std::, tbb-based std::parallel with par and par_unseq, and gnu_parallel for performance evaluation. gnu_parallel performs as the fastest toolkits.\nTODO: I should implement all those algorithms by myself in the near future.\nSorting gnu_parallel is favored by someones\nWhen operating on a vector of size 2^31, the performance of various implementations are:\nmethods time (10^-6s) std:: 15.87 par 2373.95 par_unseq 11.50 gnu_parallel 6.54 Prefix Sum It is also a well-studied algorithm, as descripted by link.\nWhen operating on a vector of size 2^31, the performance of various implementations are:\nmethods time (10^-6s) std:: 5.08 par 5.58 par_unseq 5.42 gnu_parallel 4.25 Conclusion libstdc++ offers built-in parallel implementations for a variety of algorithms, including sort and partial_sum. The parallel mode is implicitly enabled during the compilation with -fopenmp and _GLIBCXX_PARALLEL.\nMoreover, the parallel components called by e.g., std::sort are in fact the gnu_parallel codes. We can also explicitly call gnu_parallel by including the header, e.g.,\u0026lt;parallel/algorithm\u0026gt;. Compared with the parallel std::, gnu_parallel incurs smaller overhead and thus delivers (slightly) better performance.\nThe tbb-based parallel methods, which is the new feature of C++17, are unsatisfactory. The par policy behaves extremely poorly in Sorting and a bit bad in Prefix Sum. Suprisingly, the par_unseq policy (parallelism + vectorization) is rather good in Sorting, only second to gnu_parallel. The TBB and its optimization strategies remain to be explored in the future.\nMore details regarding gnu_parallel can be found on this page.\n","permalink":"https://yuang-chen.github.io/posts/2023-04-25-par-algo/","summary":"The content of this post is extracted from my previous random notes. I am too lazy to update and organize it ü¶•.\nC++17 new feature \u0026ndash; parallel algorithms The parallel algorithms and execution policies are introduced in C++17. Unfortuantely, according to CppReference, only GCC and Intel support these features. Clang still leaves them unimplemented.\nA blog about it.\nThe parallel library brough by C++17 requires the usage of Intel\u0026rsquo;s oneTBB for multithreading.","title":"Parallel Algorithms from Libraries"},{"content":"","permalink":"https://yuang-chen.github.io/posts/2023-04-21-overview/","summary":"","title":"2023 04 21 Overview"},{"content":"I am now a Postdoc at CUHK, and graduated from CUHK on Shenzhen campus for my PhD.\nMy research focuses on High-Performance Graph Processing on Multicore Systems, where the irregularrity of graph workloads are exploited to maximize the computing efficiency of modern multicore machines. I am broadly interesed in graph-related problems on diverse computing platforms, including CPUs (OpenMP), GPUs (CUDA), and clusters (MPI, RMDA). The core of my reseach is to investigate the interaction between graphs and underlying hardware.\nSpecifically, I am looking into three types of research problems:\nBoosting generic graph algorithms on CPUs, e,g, PageRank, BFS, Triangle Counting, etc. Accelerating graph matrix multiplication (SpMV + SpMM) on CPUs and GPUs, which is a fundamental building block in scientific computing. GNN speedup. Also, I am a fan of modern C++ programming. I follow closely to C++20 and spent a lot of time in meta programming.\nIn 2018, I received my dual master degrees in Multicore Systems from TU Eindhoven, Netherlands and TU Berlin, Germany with full scholarship granted by European Union. In 2017-2018, I worked as Masterarbeitor (i.e., intern) in Frauhofer FOKUS to build Germany\u0026rsquo;s e-health infrastructure, based on which I developed my master thesis \u0026ldquo;Providing the Infrastructure for SICCT Protocol Tests with Focus on Service Discovery and Pairing\u0026rdquo;.\nIn 2015, I obtained my bachelor degree at Huazhong University of Science and Technology (HUST). Also, in 2014-2015, being sponsored by CSC scholarship, I studied as an exchange student in RWTH Aachen, Germany.\nMy CV is da.\nPublications YuAng Chen and Yeh-Ching Chung, ‚ÄúWorkload Balancing via Graph Reordering on Multicore Systems,‚Äù IEEE Transactions on Parallel and Distributed Systems (TPDS), Vol. 33, No. 5, 2022, pp. 1231-1245.\nYuAng Chen and Yeh-Ching Chung, ‚ÄúHiPa: Hierarchical Partitioning for Fast Page Rank on Multicore Systems,‚Äù Proceedings of IEEE International conference on Parallel Processing (ICPP), Article No. 24, 2021, pp. 1-10.\nYuAng Chen and Yeh-Ching Chung, ‚ÄúCorder: Cache-Aware Reordering For Optimizing Graph Analytics,‚Äù Proceedings of ACM International conference on Principles and Practice of Parallel Programming (PPoPP), 2021, pp.472-473.\nYuAng Chen and Yeh-Ching Chung, \u0026ldquo;A Unequal Caching Strategy for Shared-Memory Graph Analytics\u0026rdquo;, to apear in IEEE Transactions on Parallel and Distributed Systems (TPDS).\nOngoing Work\nYuAng Chen and Yeh-Ching Chung, \u0026ldquo;Co-Design of Mixed Matrix Format and Processing Engine for Efficient SpMV on Graphs\u0026rdquo;, in praparation\nYuAng Chen and Yeh-Ching Chung, \u0026quot; Locality Extraction \u0026amp; Blocking for Graph Adjacency Matrix Multiplications\u0026quot;, in preparation\nTeaching 2020¬†Spring : Introduction to Programming Methodology\n2020¬†Fall¬†: Operating System\n2021 Spring : Compiler Design\n2021 Fall : Operating System\n2022 Spring : Computer Architecture\n","permalink":"https://yuang-chen.github.io/about/aboutme/","summary":"I am now a Postdoc at CUHK, and graduated from CUHK on Shenzhen campus for my PhD.\nMy research focuses on High-Performance Graph Processing on Multicore Systems, where the irregularrity of graph workloads are exploited to maximize the computing efficiency of modern multicore machines. I am broadly interesed in graph-related problems on diverse computing platforms, including CPUs (OpenMP), GPUs (CUDA), and clusters (MPI, RMDA). The core of my reseach is to investigate the interaction between graphs and underlying hardware.","title":"About Me"}]