[{"content":"Modern Features in C++17 Non-virtual runtime polymorphism can be achieved with modern C++ (e.g., C++17) features std::any and std::variant as described in the table below.\nNotice std::tuple is not used for polymorphism; it offers a structured way to manage multiple values of different types simultaneously, such as in function return types, or parameter packs. It is put here because of its usage is a bit similar to std::any and std::variant.\nFeature Paired With Purpose Use Case Special Notes std::any std::any_cast To store an instance of any type and safely retrieve the stored value. Use std::any when the exact type is unknown or may change, and std::any_cast for retrieval. Type-safe storage for single values. std::any_cast throws an exception if the cast is to the wrong type. std::variant std::visit To store one of several predefined types and apply a function to the variant\u0026rsquo;s content. Use std::variant when the value can be one of a few known types and std::visit for operations. Type-safe and more constrained than std::any. std::visit requires a callable applicable to all types in the variant. std::tuple std::apply To store a fixed-size collection of heterogeneous values and apply a function to them. Use std::tuple for grouping known types and std::apply to call a function with tuple elements. Accessed by index using std::get. std::apply useful for calling a function with tuple elements as arguments. Code Example std::any #include \u0026lt;any\u0026gt; #include \u0026lt;iostream\u0026gt; #include \u0026lt;string\u0026gt; int main() { std::any a = 1; // Store an int std::cout \u0026lt;\u0026lt; std::any_cast\u0026lt;int\u0026gt;(a) \u0026lt;\u0026lt; std::endl; // Extract the int a = std::string(\u0026#34;Hello, World!\u0026#34;); // Store a string std::cout \u0026lt;\u0026lt; std::any_cast\u0026lt;std::string\u0026gt;(a) \u0026lt;\u0026lt; std::endl; // Extract the string return 0; } std::variant #include \u0026lt;variant\u0026gt; #include \u0026lt;iostream\u0026gt; #include \u0026lt;string\u0026gt; int main() { // A variant that can hold either an int, double, or string std::variant\u0026lt;int, double, std::string\u0026gt; var; var = 10; // Assigning an integer // Visit the variant and apply a lambda function std::visit([](auto\u0026amp;\u0026amp; arg) { std::cout \u0026lt;\u0026lt; arg \u0026lt;\u0026lt; std::endl; }, var); var = \u0026#34;Hello, World!\u0026#34;; // Now assign a string // Visit the variant again std::visit([](auto\u0026amp;\u0026amp; arg) { std::cout \u0026lt;\u0026lt; arg \u0026lt;\u0026lt; std::endl; }, var); return 0; } std::tupple #include \u0026lt;tuple\u0026gt; #include \u0026lt;iostream\u0026gt; // A function to be applied to a tuple void print(int i, const std::string\u0026amp; s, double d) { std::cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#34;, \u0026#34; \u0026lt;\u0026lt; s \u0026lt;\u0026lt; \u0026#34;, \u0026#34; \u0026lt;\u0026lt; d \u0026lt;\u0026lt; std::endl; } int main() { std::tuple\u0026lt;int, std::string, double\u0026gt; t = std::make_tuple(42, \u0026#34;Hello\u0026#34;, 3.14); // Apply the function \u0026#39;print\u0026#39; to the tuple \u0026#39;t\u0026#39; std::apply(print, t); return 0; } CRTP in C++98 The Curiously Recurring Template Pattern (CRTP) is an advanced C++ idiom that involves a particular form of inheritance that leverages templates. It achieves static polymorphism, which allows for polymorphic behavior without the overhead of dynamic polymorphism (such as virtual functions).\nIn CRTP, a template class named Base is designed to take a derived class as its template parameter. When defining a derived class, such as Derived, it inherits from Base by passing itself as the template argument (Base\u0026lt;Derived\u0026gt;). This allows Base to define an interface which Derived implements, enabling Base to invoke these methods directly.\nThe CRTP enables static (compile-time) polymorphism. By using templates, the base class can call methods of the derived class without needing virtual functions. This is because the base class knows the type of the derived class at compile time.\nUnlike dynamic polymorphism, which uses virtual functions to resolve method calls at runtime, CRTP resolves them at compile time. This eliminates the overhead associated with dynamic dispatch (like v-table lookups).\ntemplate\u0026lt;typename Derived\u0026gt; class Base { public: void interface() { // ... common pre-processing ... // Call to the derived class\u0026#39;s implementation static_cast\u0026lt;Derived*\u0026gt;(this)-\u0026gt;implementation(); // ... common post-processing ... } }; class Derived : public Base\u0026lt;Derived\u0026gt; { public: void implementation() { // Specific implementation } }; Explicit Object Parameter in C++23 Thanks to the explicit object parameter (Deducing this), the C and the R can be removed from the CRTP acronym:\nC++23\u0026rsquo;s explicit object parameter feature allows the class name to be referenced explicitly in the parameter lists of non-static member functions. With Deducing this, the type of the explicit object parameter can be deduced to the derived type and perfectly forwarded. As a result, template non-static member functions can access the derived class, eliminating the need to specialize the object over the derived class.\nclass Base { public: template\u0026lt;typename T\u0026gt; void interface(this T\u0026amp;\u0026amp; self) { // ... common pre-processing ... // Call to the derived class\u0026#39;s implementation self.implementation(); // ... common post-processing ... } }; class Derived : public Base { public: void implementation() { // Specific implementation } }; ","permalink":"https://yuang-chen.github.io/posts/2024-01-24-non-virtual-polymorphism/","summary":"Modern Features in C++17 Non-virtual runtime polymorphism can be achieved with modern C++ (e.g., C++17) features std::any and std::variant as described in the table below.\nNotice std::tuple is not used for polymorphism; it offers a structured way to manage multiple values of different types simultaneously, such as in function return types, or parameter packs. It is put here because of its usage is a bit similar to std::any and std::variant.","title":"Non-Virtual Polymorphism"},{"content":"Layout 0 1 2 3 4 5 6 7 0 0 0 0 0 0 0 0 8 9 10 11 12 13 14 15 0 0 0 0 0 0 0 0 16 17 18 19 20 21 22 23 0 0 0 0 0 0 0 0 24 25 26 27 28 29 30 31 0 0 0 0 0 0 0 0 32 33 34 35 36 37 38 39 0 0 0 0 0 0 0 0 40 41 42 43 44 45 46 47 0 0 0 0 0 0 0 0 48 49 50 51 52 53 54 55 0 0 0 0 0 0 0 0 56 57 58 59 60 61 62 63 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8 16 24 32 40 48 56 0 0 0 0 0 0 0 0 1 9 17 25 33 41 49 57 0 0 0 0 0 0 0 0 2 10 18 26 34 42 50 58 0 0 0 0 0 0 0 0 3 11 19 27 35 43 51 59 0 0 0 0 0 0 0 0 4 12 20 28 36 44 52 60 0 0 0 0 0 0 0 0 5 13 21 29 37 45 53 61 0 0 0 0 0 0 0 0 6 14 22 30 38 46 54 62 0 0 0 0 0 0 0 0 7 15 23 31 39 47 55 63 Code on V100 int half_elements = a_frag.num_elements / 2; for (int i = 0; i \u0026lt; half_elements; i++) { if (lid \u0026lt; 4) a_frag.x[i] = (lid \u0026lt;\u0026lt; 3) + i; // lid * 8 + i else if (lid \u0026gt;= 16 \u0026amp;\u0026amp; lid \u0026lt; 20) a_frag.x[i] = (lid \u0026lt;\u0026lt; 3) - 96 + i; // lid * 8 - 96 + i } for (int i = half_elements; i \u0026lt; a_frag.num_elements; i++) { if (lid \u0026gt;= 12 \u0026amp;\u0026amp; lid \u0026lt; 16) a_frag.x[i] = lid - 76 + (i \u0026lt;\u0026lt; 3); // lid - 76 + i * 8 else if (lid \u0026gt;= 28 \u0026amp;\u0026amp; lid \u0026lt; 32) a_frag.x[i] = lid - 88 + (i \u0026lt;\u0026lt; 3); // lid - 88 + i * 8 } ","permalink":"https://yuang-chen.github.io/posts/2024-01-21-tensor-core-register-layout/","summary":"Layout 0 1 2 3 4 5 6 7 0 0 0 0 0 0 0 0 8 9 10 11 12 13 14 15 0 0 0 0 0 0 0 0 16 17 18 19 20 21 22 23 0 0 0 0 0 0 0 0 24 25 26 27 28 29 30 31 0 0 0 0 0 0 0 0 32 33 34 35 36 37 38 39 0 0 0 0 0 0 0 0 40 41 42 43 44 45 46 47 0 0 0 0 0 0 0 0 48 49 50 51 52 53 54 55 0 0 0 0 0 0 0 0 56 57 58 59 60 61 62 63 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8 16 24 32 40 48 56 0 0 0 0 0 0 0 0 1 9 17 25 33 41 49 57 0 0 0 0 0 0 0 0 2 10 18 26 34 42 50 58 0 0 0 0 0 0 0 0 3 11 19 27 35 43 51 59 0 0 0 0 0 0 0 0 4 12 20 28 36 44 52 60 0 0 0 0 0 0 0 0 5 13 21 29 37 45 53 61 0 0 0 0 0 0 0 0 6 14 22 30 38 46 54 62 0 0 0 0 0 0 0 0 7 15 23 31 39 47 55 63 Code on V100 int half_elements = a_frag.","title":"Tensor Core Register Layout"},{"content":"Note An independent set in a graph is a set of vertices, no two of which are adjacent. A maximal independent set is an independent set that is not a subset of any other independent set in the graph. Here\u0026rsquo;s a basic approach to find a Maximal Independent Set:\nStart with an empty set S. Iterate over all vertices of the graph. For each vertex: If the vertex and its neighbors are not in S, add the vertex to S. The set S at the end of this process is a Maximal Independent Set. This algorithm does not necessarily find the maximum independent set (the largest possible), as that problem is NP-hard, but it finds a set that is maximal, meaning it cannot be extended by adding more vertices.\nCode #include \u0026lt;iostream\u0026gt; #include \u0026lt;unordered_set\u0026gt; #include \u0026lt;vector\u0026gt; auto max_independent_set(const std::vector\u0026lt;int\u0026gt; \u0026amp;row_ptr, const std::vector\u0026lt;int\u0026gt; \u0026amp;col_idx) { int num_nodes = row_ptr.size() - 1; std::unordered_set\u0026lt;int\u0026gt; mis; std::vector\u0026lt;bool\u0026gt; in_set(num_nodes, false); for (int i = 0; i \u0026lt; num_nodes; i++) { if (in_set[i]) continue; bool can_add = true; // make sure that none of the neighbors are in the set for (int j = row_ptr[i]; j \u0026lt; row_ptr[i + 1]; j++) { auto neighbor = col_idx[j]; if (in_set[neighbor]) { can_add = false; break; } } if (can_add) { mis.insert(i); in_set[i] = true; // mark all the neighbors as in the set for (int j = row_ptr[i]; j \u0026lt; row_ptr[i + 1]; j++) { auto neighbor = col_idx[j]; std::cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; neighbor \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; in_set[neighbor] = true; } } } return mis; } int main() { /* 0 - 1 - 3 - 5 | | 2 - 4 */ // CSR representation of the graph std::vector\u0026lt;int\u0026gt; row_ptr = {0, 2, 5, 7, 9, 11, 12}; std::vector\u0026lt;int\u0026gt; col_idx = {1, 2, 0, 3, 4, 0, 4, 1, 5, 1, 2, 3}; auto mis = max_independent_set(row_ptr, col_idx); std::cout \u0026lt;\u0026lt; \u0026#34;Maximal Independent Set: \u0026#34;; for (int v : mis) { std::cout \u0026lt;\u0026lt; v \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } std::cout \u0026lt;\u0026lt; std::endl; return 0; } ","permalink":"https://yuang-chen.github.io/posts/2023-12-13-maximal-independent-set/","summary":"Note An independent set in a graph is a set of vertices, no two of which are adjacent. A maximal independent set is an independent set that is not a subset of any other independent set in the graph. Here\u0026rsquo;s a basic approach to find a Maximal Independent Set:\nStart with an empty set S. Iterate over all vertices of the graph. For each vertex: If the vertex and its neighbors are not in S, add the vertex to S.","title":"Maximal Independent Set"},{"content":"Note The Matching algorithm is a graph algorithm that finds a matching in a graph, where a matching is a set of edges without common vertices. In other words, a subset of the edges is a matching if each vertex appears in at most one edge of that matching.\nA Maximal matching is a matching that cannot have any more edges added to it without violating the matching property.\nA maximum matching is a matching that contains the largest possible number of edges.\nA maximal matching is not necessarily the maximum matching,\nMore on Perfect Matching and Near-Perfect Matching: link\nRequire: Undirected Unweighted Graph\nCode #include \u0026lt;algorithm\u0026gt; #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; auto maximal_matching(const std::vector\u0026lt;int\u0026gt; \u0026amp;row_ptr, const std::vector\u0026lt;int\u0026gt; \u0026amp;col_idx) { int num_nodes = row_ptr.size() - 1; std::vector\u0026lt;bool\u0026gt; matched(num_nodes, false); std::vector\u0026lt;std::pair\u0026lt;int, int\u0026gt;\u0026gt; edges; for (int source = 0; source \u0026lt; num_nodes; source++) { if (matched[source]) continue; for (int j = row_ptr[source]; j \u0026lt; row_ptr[source + 1]; j++) { auto target = col_idx[j]; if (matched[target]) continue; matched[source] = true; matched[target] = true; edges.emplace_back(source, target); break; } } return edges; } int main() { /* Graph: 0 -- 1 | | 3 -- 2 \\ / 4 */ std::vector\u0026lt;int\u0026gt; rowPtr = {0, 2, 4, 6, 8, 10}; std::vector\u0026lt;int\u0026gt; colIndex = {1, 3, 0, 2, 1, 4, 0, 4, 2, 3}; auto edges = maximal_matching(rowPtr, colIndex); // Print the result for (const auto \u0026amp;edge : edges) { std::cout \u0026lt;\u0026lt; edge.first \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; edge.second \u0026lt;\u0026lt; std::endl; } return 0; } ","permalink":"https://yuang-chen.github.io/posts/2023-12-05-maximal-matching/","summary":"Note The Matching algorithm is a graph algorithm that finds a matching in a graph, where a matching is a set of edges without common vertices. In other words, a subset of the edges is a matching if each vertex appears in at most one edge of that matching.\nA Maximal matching is a matching that cannot have any more edges added to it without violating the matching property.\nA maximum matching is a matching that contains the largest possible number of edges.","title":"Maximal Matching"},{"content":"What is Observable Behavior \u0026amp; Related Issues The term observable behavior, according to the standard, means the following:\n— Accesses (reads and writes) to volatile objects occur strictly according to the semantics of the expressions in which they occur. In particular, they are not reordered with respect to other volatile accesses on the same thread.\n— At program termination, all data written into files shall be identical to one of the possible results that execution of the program according to the abstract semantics would have produced.\n— The input and output dynamics of interactive devices shall take place in such a fashion that prompting output is actually delivered before a program waits for input. What constitutes an interactive device is implementation-defined.\nThe “as-if rule” is strongly related, in short, any code transformation is allowed that does not change the observable behavior of the program.\nThe C++ standard precisely defines the observable behavior of every C++ program that does not fall into one of the following classes:\nill-formed ill-formed, no diagnostic required implementation-defined behaviour unspecified behaviour undefined behaviour Ill-Formed: The program has syntax errors and/or diagnosable semantic errors. The compiler will tell you about them. The violated rules are written in the standard with either shall, shall not or ill-formed.\nIll-Formed, No Diagnostic Required (IFNDR): There will be no compiler errors. The program doesn’t have syntactic errors, only semantic ones, but in general, they are not diagnosable by the compiler. These semantic errors are either detectable at link time, or if the program is executed, it results in undefined behavior.\nImplementation-Defined Behavior: This is behavior that the C++ standard does not define explicitly, leaving it up to the compiler or the runtime environment to decide how to implement it. The compiler documentation typically documents how it handles these situations. Examples include the size of an int or the rounding behavior of floating-point types.\nUnspecified Behavior: This refers to behavior that the standard allows to vary between different implementations, but unlike implementation-defined behavior, the compiler is not required to document how it chooses to act. For instance, the order in which function arguments are evaluated is unspecified.\nUndefined Behavior (UB): This is a term for behavior that the C++ standard does not define at all, often resulting from code errors such as accessing out-of-bounds array elements or dereferencing null pointers. When a program exhibits undefined behavior, anything can happen, from seemingly normal operation to crashes or corrupt data.\nThe reasons behind undefined behavior’s existence? In essence, undefined behavior (UB) allows for high-performance, system-specific optimizations and maintains the language\u0026rsquo;s practicality and legacy support, at the cost of shifting the responsibility for avoiding it onto the programmer.\nThe concept of undefined behavior was not introduced by C++. It was already there in C.\nFor example, if you allocate an array in C, the data is unspecified. In Java (a language does not have UB), all bytes must be initialized to 0 (or some other specified value). This means the runtime must pass over the array (an O(n) operation), while C can perform the allocation in an instant. So C will always be faster for such operations.\nIf the code using the array is going to populate it anyway before reading, this is basically wasted effort for Java. But in the case where the code read first, you get predictable results in Java but unpredictable results in C.\nFor example, when compiling a loop that could theoretically overflow an integer:\nvoid copy_array(int* dst, const int* src, size_t size) { for (size_t i = 0; i \u0026lt; size; ++i) { dst[i] = src[i]; } } The compiler can assume i will never overflow (as that would be undefined behavior), hence it can optimize without inserting overflow checks, resulting in faster loop execution.\nHow to avoid undefined behavior? Using try-catch blocks will not going to work, UB is not about exceptions handled in the wrong way.\nSo what can you do against undefined behaviour?\nTurn on whatever warnings you can (-Wall, -Wextra, -Wpedantic) and treat them as errors. Use a sanitizer, both g++ and clang offer some. Follow coding best practices and naming guidelines. Understand the concepts behind the language. Practice contractual programming (e.g., use and read document of the standard library) Share the knowledge What is iterator invalidation? Iterator invalidation refers to the loss of validity of an iterator due to certain modifications in the data structure it points to. In C++, iterators are often used to traverse containers like vectors, lists, maps, etc. If the container is modified (elements added, removed, or reallocated), existing iterators may no longer point to the correct elements or may point to memory that is no longer part of the container. Using invalidated iterators can lead to undefined behavior.\nRefer to this table to have a full list of when a container is invalidated.\n","permalink":"https://yuang-chen.github.io/posts/2023-12-02-observable-behaviors/","summary":"What is Observable Behavior \u0026amp; Related Issues The term observable behavior, according to the standard, means the following:\n— Accesses (reads and writes) to volatile objects occur strictly according to the semantics of the expressions in which they occur. In particular, they are not reordered with respect to other volatile accesses on the same thread.\n— At program termination, all data written into files shall be identical to one of the possible results that execution of the program according to the abstract semantics would have produced.","title":"Observable Behaviors"},{"content":"Note Graph coloring is a way of assigning colors to the vertices of a graph so that no two adjacent vertices share the same color. This is a classical problem in the field of graph theory and has applications in various domains like scheduling, map coloring, and solving Sudoku puzzles.\nThe simplest form of graph coloring is vertex coloring, where the aim is to minimize the number of colors used. This problem is NP-hard, meaning there is no known algorithm that can solve all instances of the problem efficiently (in polynomial time).\nGreedy Coloring Algorithm:\nAssign the first color to the first vertex. Proceed to the next vertex and assign the lowest numbered color that hasn\u0026rsquo;t been used by its adjacent vertices. Repeat this process for all vertices. Code #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; std::vector\u0026lt;int\u0026gt; greedy_coloring(const std::vector\u0026lt;int\u0026gt; \u0026amp;row_ptr, const std::vector\u0026lt;int\u0026gt; \u0026amp;col_idx) { int num_nodes = row_ptr.size() - 1; std::vector\u0026lt;int\u0026gt; colors(num_nodes, -1); // one element is modified by every node std::vector\u0026lt;bool\u0026gt; used(num_nodes, false); // reset by every node colors[0] = 0; for (int i = 1; i \u0026lt; num_nodes; i++) { for (int j = row_ptr[i]; j \u0026lt; row_ptr[i + 1]; j++) { auto neighbor = col_idx[j]; // mark the neighbor\u0026#39;s color as used if (colors[neighbor] != -1) { used[colors[neighbor]] = true; } } int color = 0; // find the first unused color for (; color \u0026lt; num_nodes; color++) { if (!used[color]) { break; } } colors[i] = color; for (int j = row_ptr[i]; j \u0026lt; row_ptr[i + 1]; j++) { auto neighbor = col_idx[j]; // reset the used array if (colors[neighbor] != -1) { used[colors[neighbor]] = false; } } } return colors; } int main() { /* Graph: 0 -- 1 | | 3 -- 2 \\ / 4 */ std::vector\u0026lt;int\u0026gt; rowPtr = {0, 2, 4, 6, 8, 10}; std::vector\u0026lt;int\u0026gt; colIndex = {1, 3, 0, 2, 1, 4, 0, 4, 2, 3}; std::vector\u0026lt;int\u0026gt; colors = greedy_coloring(rowPtr, colIndex); // Print the result for (int i = 0; i \u0026lt; colors.size(); ++i) { std::cout \u0026lt;\u0026lt; \u0026#34;Vertex \u0026#34; \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#34; ---\u0026gt; Color \u0026#34; \u0026lt;\u0026lt; colors[i] \u0026lt;\u0026lt; std::endl; } return 0; } ","permalink":"https://yuang-chen.github.io/posts/2023-11-29-graph-coloring/","summary":"Note Graph coloring is a way of assigning colors to the vertices of a graph so that no two adjacent vertices share the same color. This is a classical problem in the field of graph theory and has applications in various domains like scheduling, map coloring, and solving Sudoku puzzles.\nThe simplest form of graph coloring is vertex coloring, where the aim is to minimize the number of colors used. This problem is NP-hard, meaning there is no known algorithm that can solve all instances of the problem efficiently (in polynomial time).","title":"Graph Coloring"},{"content":"Note Biconnectivity in graphs is an important concept used to identify biconnected components (BCCs). A graph is biconnected if it is connected and does not have any articulation points, meaning removing any single vertex will not disconnect the graph. The biconnected components of a graph are maximal biconnected subgraphs.\nStrict Definition: A BCC should contain at least three vertices in a cycle, ensuring that the removal of any single vertex does not disconnect the component.\nIn practical implementations:\nComputational Interpretation: An edge connecting two vertices (without forming part of a larger cycle) is sometimes considered a trivial BCC for completeness in algorithms. This interpretation is adopted for simplicity and ensures that all edges are included in the identification of BCCs.\nBCC vs SCC The concepts of Biconnected Components (BCCs) and Strongly Connected Components (SCCs) are fundamentally different, and thus the algorithms to find them differ significantly. Both concepts are related to graph theory but apply to different types of graphs and have different implications.\nAspect Strongly Connected Components (SCCs) Biconnected Components (BCCs) Graph Type Directed Graphs Undirected Graphs Definition A maximal subset of vertices where for every pair ( U ) and ( V ), there is a directed path from ( U ) to ( V ) and from ( V ) to ( U ). A maximal subgraph where the removal of any single vertex does not disconnect the rest of the subgraph. Involves cycles or edges. Key Focus Vertex connectivity considering directionality Edge connectivity without considering directionality Typical Algorithms Kosaraju\u0026rsquo;s, Tarjan\u0026rsquo;s, Gabow\u0026rsquo;s algorithms Tarjan\u0026rsquo;s algorithm for articulation points and bridges Applications Detecting cycles, strong relationships in directed networks like web pages, social networks Network reliability, circuit design, understanding the resilience of networks to node failure Code #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;stack\u0026gt; std::vector\u0026lt;std::vector\u0026lt;int\u0026gt;\u0026gt; tarjan(int num_vertices, const std::vector\u0026lt;int\u0026gt;\u0026amp; row_ptr, const std::vector\u0026lt;int\u0026gt;\u0026amp; col_idx) { std::vector\u0026lt;int\u0026gt; discovery_time(num_vertices, -1), low_time(num_vertices, -1), parent(num_vertices, -1); std::vector\u0026lt;bool\u0026gt; visited(num_vertices, false); std::stack\u0026lt;int\u0026gt; stack; std::vector\u0026lt;std::vector\u0026lt;int\u0026gt;\u0026gt; bcc; int time = 0; auto dfs = [\u0026amp;](int source, auto\u0026amp;\u0026amp; dfs_ref) -\u0026gt; void { discovery_time[source] = low_time[source] = time++; visited[source] = true; stack.push(source); for (int i = row_ptr[source]; i \u0026lt; row_ptr[source + 1]; ++i) { int target = col_idx[i]; if (!visited[target]) { parent[target] = source; dfs_ref(target, dfs_ref); low_time[source] = std::min(low_time[source], low_time[target]); if (low_time[target] \u0026gt;= discovery_time[source]) { std::vector\u0026lt;int\u0026gt; component; while (stack.top() != target) { component.push_back(stack.top()); stack.pop(); } component.push_back(target); stack.pop(); component.push_back(source); bcc.push_back(component); } } else if (target != parent[source]) { low_time[source] = std::min(low_time[source], discovery_time[target]); } } }; for (int i = 0; i \u0026lt; num_vertices; ++i) { if (!visited[i]) { dfs(i, dfs); if (!stack.empty()) { std::vector\u0026lt;int\u0026gt; component; while (!stack.empty()) { component.push_back(stack.top()); stack.pop(); } bcc.push_back(component); } } } return bcc; } int main() { int num_vertices = 6; std::vector\u0026lt;int\u0026gt; row_ptr = {0, 2, 4, 7, 10, 12, 14}; std::vector\u0026lt;int\u0026gt; col_idx = {1, 2, 0, 2, 0, 1, 3, 2, 4, 5, 3, 5, 3, 4}; auto bccs = tarjan(num_vertices, row_ptr, col_idx); // Print the BCCs for (const auto\u0026amp; component : bccs) { std::cout \u0026lt;\u0026lt; \u0026#34;BCC: \u0026#34;; for (int v : component) { std::cout \u0026lt;\u0026lt; v \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } std::cout \u0026lt;\u0026lt; std::endl; } return 0; } ","permalink":"https://yuang-chen.github.io/posts/2023-11-20-biconnected-components/","summary":"Note Biconnectivity in graphs is an important concept used to identify biconnected components (BCCs). A graph is biconnected if it is connected and does not have any articulation points, meaning removing any single vertex will not disconnect the graph. The biconnected components of a graph are maximal biconnected subgraphs.\nStrict Definition: A BCC should contain at least three vertices in a cycle, ensuring that the removal of any single vertex does not disconnect the component.","title":"Biconnected Components"},{"content":"Note The Low-Diameter Decomposition (LDD) algorithm is a graph partitioning algorithm that decomposes a graph into several connected subgraphs (or components) such that each subgraph has a low diameter. The diameter of a subgraph is defined as the maximum shortest path distance between any two nodes within the subgraph.\nThe LDD algorithm works as follows:\nStart with an empty decomposition and an empty queue. Pick an unvisited node u and create a new set containing only u. Perform a BFS starting from u, adding nodes to u\u0026rsquo;s set until the diameter of the set reaches a specified threshold beta. Mark all nodes in u\u0026rsquo;s set as visited. Repeat steps 2-4 until all nodes have been visited and assigned to a set. The LDD algorithm is essentially performing a BFS on each unvisited node, with the additional constraint that the diameter of each connected component formed during the BFS should be below a specified threshold value (denoted as beta).\nCode #include \u0026lt;iostream\u0026gt; #include \u0026lt;queue\u0026gt; #include \u0026lt;vector\u0026gt; std::vector\u0026lt;int\u0026gt; lowDiameterDecomposition(const std::vector\u0026lt;int\u0026gt; row_ptr, const std::vector\u0026lt;int\u0026gt; col_idx, const int beta) { int num_nodes = row_ptr.size() - 1; std::vector\u0026lt;int\u0026gt; components(num_nodes, -1); std::vector\u0026lt;bool\u0026gt; visited(num_nodes, false); std::queue\u0026lt;int\u0026gt; bfs_queue; int set_id = 0; for (int source = 0; source \u0026lt; num_nodes; source++) { if (!visited[source]) { int set_size = 0; bfs_queue.push(source); visited[source] = true; components[source] = set_id; while (!bfs_queue.empty()) { auto node = bfs_queue.front(); bfs_queue.pop(); for (auto i = row_ptr[node]; i \u0026lt; row_ptr[node + 1]; i++) { auto target = col_idx[i]; if (visited[target]) { continue; } bfs_queue.push(target); visited[target] = true; components[target] = set_id; set_size++; if (++set_size \u0026gt;= beta) { // goto to the breakout of the nested loop goto break_out; // break; } } // if (set_size \u0026gt;= beta) { // break; // } } break_out: set_id++; } } return components; } int main() { // A cycle of 3 nodes: 0 -\u0026gt; 1 -\u0026gt; 2 -\u0026gt; 0 // A cycle of 3 nodes: 3 -\u0026gt; 4 -\u0026gt; 5 -\u0026gt; 3 std::vector\u0026lt;int\u0026gt; row_ptr = {0, 1, 2, 3, 4, 5, 6}; std::vector\u0026lt;int\u0026gt; col_idx = {1, 2, 0, 4, 5, 3}; int beta = 2; auto components = lowDiameterDecomposition(row_ptr, col_idx, beta); // Print the decomposition for (int i = 0; i \u0026lt; components.size(); ++i) { std::cout \u0026lt;\u0026lt; \u0026#34;Node \u0026#34; \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#34; is in set \u0026#34; \u0026lt;\u0026lt; components[i] \u0026lt;\u0026lt; std::endl; } return 0; } } ","permalink":"https://yuang-chen.github.io/posts/2023-11-02-low-diameter-decomposition/","summary":"Note The Low-Diameter Decomposition (LDD) algorithm is a graph partitioning algorithm that decomposes a graph into several connected subgraphs (or components) such that each subgraph has a low diameter. The diameter of a subgraph is defined as the maximum shortest path distance between any two nodes within the subgraph.\nThe LDD algorithm works as follows:\nStart with an empty decomposition and an empty queue. Pick an unvisited node u and create a new set containing only u.","title":"Low Diameter Decomposition"},{"content":"Trivial Class vs Aggregate Structure Trivial Class A trivial class is a class that:\nHas a trivial default constructor. Has a trivial copy constructor. Has a trivial move constructor (since C++11). Has a trivial copy assignment operator. Has a trivial move assignment operator (since C++11). Has a trivial destructor. Has no virtual functions or virtual base classes. The trivial constructors/operations/destructor means they are not user-provided (i.e., is implicitly-defined or defaulted on its first declaration).\nTrivial classes are basically those that allow bit-wise copy. The operations on these classes are expected to be fast because they can be replaced with simple memory operations like memcpy or memcmp.\nAggregate Structure An aggregate is arrays or a class (typically a struct, but possibly a class as well) that:\nHas no user-declared constructors. Has no private or protected non-static data members. Has no virtual functions. Has no virtual, private, or protected base classes. Has no user-defined destructor. Property Trivial Class Aggregate Structure Notes User-Declared Constructors Allowed Not Allowed Aggregate cannot have user-declared constructors. Private/Protected Data Members Allowed Not Allowed Aggregates can\u0026rsquo;t have private or protected non-static members. Virtual Functions Not Allowed Not Allowed Neither can have virtual functions. Virtual, Private, Protected Bases Not Allowed Not Allowed Neither can have these types of base classes. User-Defined Destructor Allowed Not Allowed Aggregate cannot have a user-defined destructor. Bitwise Copy-able Yes Yes Both can usually be copied using memcpy. Aggregate Initialization Sometimes Yes Only if the trivial class is also an aggregate. Allows Optimizations Yes Yes Both allow for various compiler optimizations. Reference https://en.cppreference.com/w/cpp/language/classes\nhttps://en.cppreference.com/w/cpp/language/aggregate_initialization\n","permalink":"https://yuang-chen.github.io/posts/2023-11-01-trivial-class-vs-aggregate-structure/","summary":"Trivial Class vs Aggregate Structure Trivial Class A trivial class is a class that:\nHas a trivial default constructor. Has a trivial copy constructor. Has a trivial move constructor (since C++11). Has a trivial copy assignment operator. Has a trivial move assignment operator (since C++11). Has a trivial destructor. Has no virtual functions or virtual base classes. The trivial constructors/operations/destructor means they are not user-provided (i.e., is implicitly-defined or defaulted on its first declaration).","title":"Trivial Class vs Aggregate Structure"},{"content":"The table summarizes how brackets {} and () are related to list-initialization in various contexts. The column Allows Narrowing Conversion indicates whether implicit type conversions that lose information are allowed. The column Allows Explicit Constructors indicates whether the syntax can call constructors marked as explicit. The columns Use for Aggregates and Use for User-Defined Types show the applicability of each initialization type for aggregates like arrays (e.g., int x[3][4]) and structs, and user-defined types like classes, respectively.\nInitialization Type Syntax Allows Narrowing Conversion Allows Explicit Constructors Use for Aggregates Use for User-Defined Types Remarks Direct Initialization (()) int a(42); Yes No No Yes Traditional C++ syntax Direct-list-initialization int a{42}; No Yes Yes Yes C++11 onwards; type-safe Copy Initialization (=) int a = 42; Yes No No Yes Traditional C++ syntax Copy-list-initialization int a = {42}; No No Yes Yes C++11 onwards; type-safe ","permalink":"https://yuang-chen.github.io/posts/2023-10-29-initialization-with-brackets/","summary":"The table summarizes how brackets {} and () are related to list-initialization in various contexts. The column Allows Narrowing Conversion indicates whether implicit type conversions that lose information are allowed. The column Allows Explicit Constructors indicates whether the syntax can call constructors marked as explicit. The columns Use for Aggregates and Use for User-Defined Types show the applicability of each initialization type for aggregates like arrays (e.g., int x[3][4]) and structs, and user-defined types like classes, respectively.","title":"Initialization With Brackets"},{"content":"Note The SCAN (Structural Clustering Algorithm for Networks) algorithm is used for detecting clusters in graphs. It also looks at the structural similarity between nodes:\n$$ s(A, B) = \\frac{|N(A) \\cap N(B)|}{\\sqrt{|N(A)| \\times |N(B)|}} $$\nCompute Structural Similarity: For each edge (A,B)(A,B), compute its structural similarity score. Identify Strong Relations: Mark edges as \u0026lsquo;strong\u0026rsquo; if their structural similarity is above **eps. Identify Core Nodes: For each node, count its strong relationships. If it has more than mu strong relationships, mark it as a core node. Cluster Formation: Perform a DFS starting from each core node to form clusters. Classify Non-Core Nodes: Nodes that are not core nodes but are connected to core nodes are considered as border nodes. Nodes that are neither core nodes nor connected to core nodes are considered as outliers. Complexity Analysis: Computing Structural Similarity: In the worst case, this step has a time complexity of $O(∣E∣×d)$, where $∣E∣$ is the number of edges and $d$ is the average degree of the nodes. Identifying Strong Relations and Core Nodes: This can be done in $O(∣E∣+∣V∣)$ time, where ∣V∣ is the number of vertices. Cluster Formation: The DFS steps have a time complexity of $O(∣E∣+∣V∣)$ So, the overall worst-case time complexity is $O(∣E∣×d+∣V∣)$\nThe space complexity is $O(∣E∣+∣V∣)$ to store the graph and additional data structures.\nCode #include \u0026lt;cmath\u0026gt; #include \u0026lt;iostream\u0026gt; #include \u0026lt;unordered_set\u0026gt; #include \u0026lt;vector\u0026gt; auto SCAN(const std::vector\u0026lt;int\u0026gt; row_ptr, const std::vector\u0026lt;int\u0026gt; col_idx, double eps, int mu) { // \u0026lt;\u0026lt; Compute the structural similarity between two nodes auto structure_similarity = [\u0026amp;](int source, int target) { auto source_begin = row_ptr[source]; auto target_begin = row_ptr[target]; const auto source_end = row_ptr[source + 1]; const auto target_end = row_ptr[target + 1]; const auto source_degree = source_end - source_begin; const auto target_degree = target_end - target_begin; if (source_degree == 0 || target_degree == 0) { return 0.0; } int common_neighbors = 0; while (source_begin \u0026lt; source_end \u0026amp;\u0026amp; target_begin \u0026lt; target_end) { if (col_idx[source_begin] == col_idx[target_begin]) { ++common_neighbors; ++source_begin; ++target_begin; } else if (col_idx[source_begin] \u0026lt; col_idx[target_begin]) { ++source_begin; } else { ++target_begin; } } return static_cast\u0026lt;double\u0026gt;(common_neighbors) / std::sqrt(source_degree * target_degree); }; // \u0026lt;\u0026lt; Construct a new cluster from the core node std::function\u0026lt;void(int, std::unordered_set\u0026lt;int\u0026gt; \u0026amp;, const std::vector\u0026lt;std::unordered_set\u0026lt;int\u0026gt;\u0026gt; \u0026amp;)\u0026gt; dfs = [\u0026amp;dfs]( int source, std::unordered_set\u0026lt;int\u0026gt; \u0026amp;new_cluster, const std::vector\u0026lt;std::unordered_set\u0026lt;int\u0026gt;\u0026gt; \u0026amp;strong_neighbors) { new_cluster.insert(source); for (const auto target : strong_neighbors[source]) { if (new_cluster.find(target) == new_cluster.end()) { dfs(target, new_cluster, strong_neighbors); } } }; //\u0026gt;\u0026gt; Find Core Nodes const auto num_nodes = row_ptr.size() - 1; std::vector\u0026lt;std::unordered_set\u0026lt;int\u0026gt;\u0026gt; strong_neighbors(num_nodes); std::unordered_set\u0026lt;int\u0026gt; core_nodes; for (int source = 0; source \u0026lt; num_nodes; source++) { for (int j = row_ptr[source]; j \u0026lt; row_ptr[source + 1]; j++) { auto target = col_idx[j]; // 1. For each vertex, compute its structural similarity with its // neighbors. if (structure_similarity(source, target) \u0026gt; eps) { // 2. If the structural similarity is above a certain threshold // (eps), mark the edge as \u0026#39;strong\u0026#39;. strong_neighbors[source].insert(target); } } // 3. For each vertex, if the number of strong neighbors is above a // certain threshold (mu), mark it as a \u0026#39;core\u0026#39; vertex. if (strong_neighbors[source].size() \u0026gt;= mu) { core_nodes.insert(source); } } // std::cout \u0026lt;\u0026lt; \u0026#34;Strong Neighbors: \u0026#34; \u0026lt;\u0026lt; std::endl; // for (int i = 0; i \u0026lt; strong_neighbors.size(); ++i) { // std::cout \u0026lt;\u0026lt; \u0026#34;Node \u0026#34; \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#34;: \u0026#34;; // for (const auto \u0026amp;neighbor : strong_neighbors[i]) { // std::cout \u0026lt;\u0026lt; neighbor \u0026lt;\u0026lt; \u0026#34; \u0026#34;; // } // std::cout \u0026lt;\u0026lt; std::endl; // } // std::cout \u0026lt;\u0026lt; \u0026#34;Core Nodes: \u0026#34; \u0026lt;\u0026lt; std::endl; // for (const auto \u0026amp;node : core_nodes) { // std::cout \u0026lt;\u0026lt; node \u0026lt;\u0026lt; \u0026#34; \u0026#34;; // } // std::cout \u0026lt;\u0026lt; std::endl; // \u0026gt;\u0026gt; Find Clusters std::vector\u0026lt;std::unordered_set\u0026lt;int\u0026gt;\u0026gt; clusters; std::unordered_set\u0026lt;int\u0026gt; visited; // 4. Perform a DFS to find clusters // starting from each core node for (const auto core : core_nodes) { if (visited.find(core) == visited.end()) { std::unordered_set\u0026lt;int\u0026gt; new_cluster; dfs(core, new_cluster, strong_neighbors); clusters.push_back(new_cluster); visited.insert(new_cluster.begin(), new_cluster.end()); } } return clusters; } int main() { std::vector\u0026lt;int\u0026gt; row_ptr = {0, 4, 8, 12, 16, 17, 21, 25, 29, 33, 34}; std::vector\u0026lt;int\u0026gt; col_idx = {1, 2, 3, 0, 0, 2, 3, 1, 0, 1, 3, 2, 0, 1, 2, 3, 4, 5, 6, 7, 8, 5, 6, 7, 8, 5, 6, 7, 8, 5, 6, 7, 8, 9}; // Parameters for SCAN double eps = 0.7; int mu = 2; auto clusters = SCAN(row_ptr, col_idx, eps, mu); for (const auto \u0026amp;cluster : clusters) { for (const auto node : cluster) { std::cout \u0026lt;\u0026lt; node \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } std::cout \u0026lt;\u0026lt; std::endl; } return 0; } ","permalink":"https://yuang-chen.github.io/posts/2023-10-22-scan-clustering/","summary":"Note The SCAN (Structural Clustering Algorithm for Networks) algorithm is used for detecting clusters in graphs. It also looks at the structural similarity between nodes:\n$$ s(A, B) = \\frac{|N(A) \\cap N(B)|}{\\sqrt{|N(A)| \\times |N(B)|}} $$\nCompute Structural Similarity: For each edge (A,B)(A,B), compute its structural similarity score. Identify Strong Relations: Mark edges as \u0026lsquo;strong\u0026rsquo; if their structural similarity is above **eps. Identify Core Nodes: For each node, count its strong relationships.","title":"SCAN Clustering"},{"content":"The core reason for my re-implementing the standard containers is the Priority Queue (or namely Max Heap). It combines algorithms and fundamental data structures to create a sophisticated yet highly efficient data structure. My current focus on reinventing these containers has temporarily paused here. Similar containers, like flat_set, are slated for release in C++23. When they become available, I plan to continue this series by attempting to re-implement them.\nDescription A priority queue is a container adapter offering constant time access to the largest (by default) element, albeit at the cost of logarithmic time insertion and extraction. It adheres to two principal characteristics.\nStructure Property A typical std::priority_queue leverages a binary heap as its core data structure. A binary heap manifests as a complete binary tree, wherein all levels, barring potentially the last, are fully populated, with all nodes shifted as left as possible.\nOrder Property In a max heap, the value of each node is std::less\u0026lt;T\u0026gt; than the value of its parent. The largest element is at the root.\nThe left child resides at index 2i + 1. The right child is located at index 2i + 2. The parent is found at index (i - 1)/2 (integer division). Thus, the parent and its children are ordered from left to right in the vector.\nBuild A Heap The priority queue is built upon a key function:\nheapify: This function upholds the heap attributes for the subtree rooted at index by executing swaps and moving down (left → right, parent → children) through the vector. Drawing upon heapify, three auxiliary heap functions are devised:\nmake_heap: Transforms a vector into a heap, proceeding backwards (right → left) from the vector\u0026rsquo;s midpoint to its front. pop_heap: Swaps the front (max) element with the last, followed by heapifying the vector without the swapped last element. push_heap: Swaps and move from the end to the front (right → left, child → parent) to restore the heap properties. Code #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; template \u0026lt;typename T, typename Container = std::vector\u0026lt;T\u0026gt;, typename Compare = std::less\u0026lt;T\u0026gt;\u0026gt; class PriorityQueue { public: PriorityQueue() { make_heap(0, container.size() - 1, 0); } T\u0026amp; top() { return container.front(); } void push(const T\u0026amp; value) { container.push_back(value); push_heap(0, container.size() - 1); } void pop() { if(container.empty()) { return; } pop_heap(); container.pop_back(); } private: Container container; Compare compare; //restores the heap property in a subtree rooted at a specified node index. // It performs the necessary swaps moving down the heap until the heap property is restored. void heapify(size_t first, size_t last, size_t index) { const auto size = last - first + 1; auto left = 2 * index + 1; auto right = 2 * index + 2; auto largest = index; if(left \u0026lt; size \u0026amp;\u0026amp; compare(container[largest], container[left])) { largest = left; } if(right \u0026lt; size \u0026amp;\u0026amp; compare(container[largest], container[right])) { largest = right; } if(largest != index) { std::swap(container[index], container[largest]); heapify(first, last, largest); } } //builds a (max) heap from an unsorted range of elements. // The loop starts from the last non-leaf node (first + (size / 2) - 1) // and moves towards the root node (first), calling heapify for each node along the way. void make_heap(size_t first, size_t last, size_t index) { const auto size = last - first + 1; for(int i = size / 2 - 1; i \u0026gt;= 0; i--) // start from the middle node! { heapify(first, last, first + i); } } // swaps the top element with the last element, removes the last element // (which was previously the top element), and then calls heapify to restore the heap property. void pop_heap() { if(container.empty()) { return; } std::swap(container.front(), container.back()); heapify(0, container.size() - 2, 0); // reconstruct the heap, excluding the last element } // ensures the heap property is maintained after a new element has been inserted. //It performs the necessary swaps moving up the heap until the heap property is restored. void push_heap(size_t first, size_t last) { auto index = last; auto parent = first + (last - 1) / 2; while(index \u0026gt; first \u0026amp;\u0026amp; compare(container[parent], container[index])) { std::swap(container[index], container[parent]); index = parent; parent = first + (index - 1) / 2; } } }; int main() { PriorityQueue\u0026lt;int\u0026gt; pq; pq.push(10); pq.push(20); pq.push(15); std::cout \u0026lt;\u0026lt; pq.top() \u0026lt;\u0026lt; std::endl; // Output: 20 pq.pop(); std::cout \u0026lt;\u0026lt; pq.top() \u0026lt;\u0026lt; std::endl; // Output: 15 pq.pop(); std::cout \u0026lt;\u0026lt; pq.top() \u0026lt;\u0026lt; std::endl; // Output: 10 return 0; } ","permalink":"https://yuang-chen.github.io/posts/2023-10-14-priority-queue/","summary":"The core reason for my re-implementing the standard containers is the Priority Queue (or namely Max Heap). It combines algorithms and fundamental data structures to create a sophisticated yet highly efficient data structure. My current focus on reinventing these containers has temporarily paused here. Similar containers, like flat_set, are slated for release in C++23. When they become available, I plan to continue this series by attempting to re-implement them.\nDescription A priority queue is a container adapter offering constant time access to the largest (by default) element, albeit at the cost of logarithmic time insertion and extraction.","title":"Priority Queue"},{"content":"Description Strongly Connected Components operates the directed graph in which there is a directed path from each vertex to every other vertex.\nWeakly Connected Component (the one we discussed before) ignores the direction of the edges. WCC is commonly considered the \u0026ldquo;default\u0026rdquo; CC algorithm, if there isn\u0026rsquo;t a specification for Strongly or Weakly.\nKosaraju\u0026rsquo;s Algorithm: Run 1st DFS to get finishing times of each vertex (i.e., postordering of DFS). [Backtracking] Run 2nd DFS on the transposed graph, starting with the visited vertices in Reverse Post-Order Each DFS tree in step 2 is an SCC. Reverse Post-Order ensures that, in the second pass, every vertex is processed before any of its dependent vertices.\nTarjan\u0026rsquo;s Algorithm: Run DFS while maintaining the stack of DFS tree, the pre-order of nodes and low-link values. Find Back-Edge, and update low-link values based on traversed children. [Backtracking] if a vertex has a low-link value equal to its ID, pop the stack to form an SCC. Summary Feature Kosaraju\u0026rsquo;s Algorithm Tarjan\u0026rsquo;s Algorithm Time Complexity O(V + E) O(V + E) Space Complexity O(V) O(V) DFS Traversals Two One Graph Representations Needs both original and transposed graph Doesn\u0026rsquo;t need a transposed graph Backtracking 2nd DFS SCC construction Both algorithms employ DFS (alternatively, BFS for parallel implementations) and recorder the traversing order, alongside backtracking to determine the SCC. When utilizing DFS with backtracking, a stack is always used to keep track of the DFS tree.\nCode #include \u0026lt;iostream\u0026gt; #include \u0026lt;stack\u0026gt; #include \u0026lt;queue\u0026gt; #include \u0026lt;vector\u0026gt; auto Kosaraju(const std::vector\u0026lt;int\u0026gt;\u0026amp; csr_pointer, const std::vector\u0026lt;int\u0026gt;\u0026amp; csr_index, const std::vector\u0026lt;int\u0026gt;\u0026amp; csc_pointer, const std::vector\u0026lt;int\u0026gt;\u0026amp; csc_index) { const auto num_nodes = csr_pointer.size() - 1; std::vector\u0026lt;bool\u0026gt; visited(num_nodes, false); std::stack\u0026lt;int\u0026gt; post_order_stack; //! First DFS // traverse CSR graph std::function\u0026lt;void(int)\u0026gt; dfs_1st = [\u0026amp;](int source) { visited[source] = true; for(auto i = csr_pointer[source]; i \u0026lt; csr_pointer[source + 1]; i++) { auto target = csr_index[i]; if(!visited[target]) { dfs_1st(target); } } post_order_stack.push(source); }; for(int i = 0; i \u0026lt; num_nodes; i++) { if(!visited[i]) { dfs_1st(i); } } //! Second DFS std::function\u0026lt;void(int, std::vector\u0026lt;int\u0026gt;\u0026amp;)\u0026gt; dfs_2nd = [\u0026amp;](int target, std::vector\u0026lt;int\u0026gt;\u0026amp; SCC) { visited[target] = false; SCC.push_back(target); for(auto i = csc_pointer[target]; i \u0026lt; csc_pointer[target + 1]; i++) { auto source = csc_index[i]; if(visited[source]) { dfs_2nd(source, SCC); } } }; // traverse CSC graph -- backtracking std::vector\u0026lt;std::vector\u0026lt;int\u0026gt;\u0026gt; all_SCCs; while(!post_order_stack.empty()) { auto node = post_order_stack.top(); post_order_stack.pop(); if(visited[node]) // go through the visited nodes { std::vector\u0026lt;int\u0026gt; SCC; dfs_2nd(node, SCC); all_SCCs.push_back(SCC); } } return all_SCCs; } auto Tarjan(const std::vector\u0026lt;int\u0026gt;\u0026amp; csr_pointer, const std::vector\u0026lt;int\u0026gt;\u0026amp; csr_index) { auto num_nodes = csr_pointer.size() - 1; // The depth-first search order in which the vertices are discovered std::vector\u0026lt;int\u0026gt; pre_order(num_nodes, -1); // This represents the smallest pre_order value reachable from vertex [i] std::vector\u0026lt;int\u0026gt; low_link(num_nodes, 0); std::vector\u0026lt;bool\u0026gt; in_stack(num_nodes, false); std::stack\u0026lt;int\u0026gt; dfs_tree_stack; std::vector\u0026lt;std::vector\u0026lt;int\u0026gt;\u0026gt; all_SCCs; std::function\u0026lt;void(int, int, std::vector\u0026lt;int\u0026gt;\u0026amp;, std::vector\u0026lt;int\u0026gt;\u0026amp;, std::vector\u0026lt;bool\u0026gt;\u0026amp;, std::stack\u0026lt;int\u0026gt;\u0026amp;, std::vector\u0026lt;std::vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp;)\u0026gt; dfs = [\u0026amp;csr_pointer, \u0026amp;csr_index, \u0026amp;dfs](int source, int count, std::vector\u0026lt;int\u0026gt;\u0026amp; pre_order, std::vector\u0026lt;int\u0026gt;\u0026amp; low_link, std::vector\u0026lt;bool\u0026gt;\u0026amp; in_stack, std::stack\u0026lt;int\u0026gt;\u0026amp; dfs_tree_stack, std::vector\u0026lt;std::vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; all_SCCs) { pre_order[source] = count++; low_link[source] = pre_order[source]; in_stack[source] = true; dfs_tree_stack.push(source); // Back Edge: A back edge is an edge that connects a vertex to one of its ancestors in the DFS tree, creating a cycle. for(auto i = csr_pointer[source]; i \u0026lt; csr_pointer[source + 1]; i++) { auto target = csr_index[i]; // if target is unvisited, perform DFS if(pre_order[target] == -1) { dfs(target, count, pre_order, low_link, in_stack, dfs_tree_stack, all_SCCs); } // When a back edge from \u0026#34;source\u0026#34; to \u0026#34;target\u0026#34; is encountered and \u0026#34;target\u0026#34; is still in the stack, // low_link[source] is updated to min(low_link[source], dfn[target]). This is because \u0026#34;target\u0026#34; is an ancestor // of \u0026#34;source\u0026#34; in the DFS tree. The update indicates that there is a way to reach back to \u0026#34;target\u0026#34; // from \u0026#34;source,\u0026#34; thereby forming a cycle. if(in_stack[target]) { low_link[source] = std::min(low_link[source], low_link[target]); } } // \u0026gt;\u0026gt; Backtracking, after visiting a root vertex and all its descendants, // check if current vertex is the root vertex // if true, the DFS tree is a SCC if(pre_order[source] == low_link[source]) { std::vector\u0026lt;int\u0026gt; scc; while(true) { auto node = dfs_tree_stack.top(); dfs_tree_stack.pop(); in_stack[node] = false; scc.push_back(node); if(node == source) { break; } } all_SCCs.push_back(scc); } }; for(int i = 0; i \u0026lt; num_nodes; i++) { if(pre_order[i] == -1) { dfs(i, 0, pre_order, low_link, in_stack, dfs_tree_stack, all_SCCs); } } return all_SCCs; } int main() { // CSR representation for the graph std::vector\u0026lt;int\u0026gt; csr_pointer = { 0, 2, 3, 4, 5, 6 }; std::vector\u0026lt;int\u0026gt; csr_index = { 1, 3, 2, 0, 4, 3 }; // CSC representation for the transpose of the graph std::vector\u0026lt;int\u0026gt; csc_pointer = { 0, 1, 2, 3, 5, 6 }; std::vector\u0026lt;int\u0026gt; csc_index = { 2, 0, 1, 0, 4, 3 }; auto all_SCCs = Kosaraju2(csr_pointer, csr_index, csc_pointer, csc_index); for(const auto\u0026amp; scc : all_SCCs) { std::cout \u0026lt;\u0026lt; \u0026#34;SCC: \u0026#34;; for(int vertex : scc) { std::cout \u0026lt;\u0026lt; vertex \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } std::cout \u0026lt;\u0026lt; std::endl; } std::cout \u0026lt;\u0026lt; \u0026#34;------------------\u0026#34; \u0026lt;\u0026lt; std::endl; all_SCCs = Tarjan(csr_pointer, csr_index); for(const auto\u0026amp; scc : all_SCCs) { std::cout \u0026lt;\u0026lt; \u0026#34;SCC: \u0026#34;; for(int vertex : scc) { std::cout \u0026lt;\u0026lt; vertex \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } std::cout \u0026lt;\u0026lt; std::endl; } return 0; } ","permalink":"https://yuang-chen.github.io/posts/2023-10-12-strongly-connected-components/","summary":"Description Strongly Connected Components operates the directed graph in which there is a directed path from each vertex to every other vertex.\nWeakly Connected Component (the one we discussed before) ignores the direction of the edges. WCC is commonly considered the \u0026ldquo;default\u0026rdquo; CC algorithm, if there isn\u0026rsquo;t a specification for Strongly or Weakly.\nKosaraju\u0026rsquo;s Algorithm: Run 1st DFS to get finishing times of each vertex (i.e., postordering of DFS). [Backtracking] Run 2nd DFS on the transposed graph, starting with the visited vertices in Reverse Post-Order Each DFS tree in step 2 is an SCC.","title":"Strongly Connected Components"},{"content":"Description Both std::queue and std::stack are container adaptors that rely on an underlying container to provide specific functionality. For example:\nstd::queue implements a First-In-First-Out (FIFO) flow, making it efficient to remove the front element. It can use std::deque (by default) or std::list as the underlying container. std::stack follows a Last-In-First-Out (LIFO) flow, where the back element needs efficient modification. By default, it uses std::deque but can also be based on std::list or std::vector. The implementations of std::queue and std::stack are rather straightforward, as we just need to wrap around existing member functions of underlying containers.\nCode #include \u0026lt;deque\u0026gt; #include \u0026lt;iostream\u0026gt; template \u0026lt;typename T, typename Container = std::deque\u0026lt;T\u0026gt;\u0026gt; class Queue { public: using container_type = Container; using value_type = typename Container::value_type; using size_type = typename Container::size_type; using reference = typename Container::reference; using const_reference = typename Container::const_reference; T\u0026amp; front() { return container.front(); } T\u0026amp; back() { return container.back(); } bool empty() { return container.empty(); } size_type size() { return container.size(); } void push(const T\u0026amp; value) { container.push_back(value); } void push(const T\u0026amp;\u0026amp; value) { container.push_back(std::move(value)); } void pop() { container.pop_front(); } private: Container container; }; template \u0026lt;typename T, typename Container = std::deque\u0026lt;T\u0026gt;\u0026gt; class Stack { public: using container_type = Container; using value_type = typename Container::value_type; using size_type = typename Container::size_type; using reference = typename Container::reference; using const_reference = typename Container::const_reference; T\u0026amp; top() { return container.back(); } bool empty() { return container.empty(); } size_type size() { return container.size(); } void push(const T\u0026amp; value) { container.push_back(value); } void push(const T\u0026amp;\u0026amp; value) { container.push_back(std::move(value)); } void pop() { container.pop_back(); } private: Container container; }; int main() { // Declare an empty queue of integers Queue\u0026lt;int\u0026gt; que; // Check if the queue is empty if(que.empty()) { std::cout \u0026lt;\u0026lt; \u0026#34;Queue is empty\\n\u0026#34;; } // Push elements onto the queue for(int i = 1; i \u0026lt;= 5; ++i) { std::cout \u0026lt;\u0026lt; \u0026#34;Pushing \u0026#34; \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#34; onto the queue\\n\u0026#34;; que.push(i); } // Display size of the queue std::cout \u0026lt;\u0026lt; \u0026#34;Queue size: \u0026#34; \u0026lt;\u0026lt; que.size() \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; // Access front and back elements std::cout \u0026lt;\u0026lt; \u0026#34;Front element: \u0026#34; \u0026lt;\u0026lt; que.front() \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; std::cout \u0026lt;\u0026lt; \u0026#34;Back element: \u0026#34; \u0026lt;\u0026lt; que.back() \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; // Pop elements from the queue while(!que.empty()) { std::cout \u0026lt;\u0026lt; \u0026#34;Popping \u0026#34; \u0026lt;\u0026lt; que.front() \u0026lt;\u0026lt; \u0026#34; from the queue\\n\u0026#34;; que.pop(); } // Check if the queue is empty again if(que.empty()) { std::cout \u0026lt;\u0026lt; \u0026#34;Queue is now empty\\n\u0026#34;; } // Declare an empty stack of integers Stack\u0026lt;int\u0026gt; stack; // Check if the stack is empty if(stack.empty()) { std::cout \u0026lt;\u0026lt; \u0026#34;Stack is empty\\n\u0026#34;; } // Push elements onto the stack for(int i = 1; i \u0026lt;= 5; ++i) { std::cout \u0026lt;\u0026lt; \u0026#34;Pushing \u0026#34; \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#34; onto the stack\\n\u0026#34;; stack.push(i); } // Display size of the stack std::cout \u0026lt;\u0026lt; \u0026#34;Stack size: \u0026#34; \u0026lt;\u0026lt; stack.size() \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; // Access top element std::cout \u0026lt;\u0026lt; \u0026#34;Top element: \u0026#34; \u0026lt;\u0026lt; stack.top() \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; // Pop elements from the stack while(!stack.empty()) { std::cout \u0026lt;\u0026lt; \u0026#34;Popping \u0026#34; \u0026lt;\u0026lt; stack.top() \u0026lt;\u0026lt; \u0026#34; from the stack\\n\u0026#34;; stack.pop(); } // Check if the stack is empty again if(stack.empty()) { std::cout \u0026lt;\u0026lt; \u0026#34;Stack is now empty\\n\u0026#34;; } return 0; } ","permalink":"https://yuang-chen.github.io/posts/2023-10-05-queue-stack/","summary":"Description Both std::queue and std::stack are container adaptors that rely on an underlying container to provide specific functionality. For example:\nstd::queue implements a First-In-First-Out (FIFO) flow, making it efficient to remove the front element. It can use std::deque (by default) or std::list as the underlying container. std::stack follows a Last-In-First-Out (LIFO) flow, where the back element needs efficient modification. By default, it uses std::deque but can also be based on std::list or std::vector.","title":"Queue \u0026 Stack"},{"content":"Description A Minimum Spanning Tree (MST) of a weighted, connected, undirected graph is a tree that spans all the vertices in the graph and has the minimum possible total edge weight among all the trees that can be created from the graph. In simpler terms, it\u0026rsquo;s a subgraph that includes all the vertices, is a tree (meaning it has no cycles), and the sum of its edge weights is as small as possible.\nThere are two techniques that can be employed to find the MST of a graph: the min heap (implemented as a priority queue in C++) and the union-find. The algorithms for finding MST using these two techniques are known as Prim\u0026rsquo;s algorithm and Kruskal\u0026rsquo;s algorithm, respectively. The code structure for implementing these algorithms is quite similar to that of previous graph algorithms such as SSSP and Connected Components\nMin Heap Prim\u0026rsquo;s algorithm is based on min heap, which is highly similar to Dijkstra\u0026rsquo;s algorithm for SSSP. Both use a priory_queue to keep the minimum distance and a visited frontier to avoid redundant traversal. Even the code structure are almost the same as well as time complexity $O((V+E)log(V))$.\nThe key difference lies in the definition, evaluation and calculation of distance.\nPrim’s : distance[target] = weight; In each step, the edge with the smallest weight that connects a vertex in the MST to a vertex outside it is chosen. Consequently, a Minimum Spanning Tree is produced, which is a subgraph that includes all vertices and has (V - 1) edges. Dijkstra’s: distance[target] = distance[curr] + weight; In each step, the vertex with the smallest distance from the root vertex is chosen, and its neighbors are updated. Consequently, a Shortest-Path Tree is produced, which may not include all vertices and edges but will provide the shortest path from the source vertex to all reachable vertices. Union Find Kruskal\u0026rsquo;s algorithm employs the union-find approach, which is commonly used by (Weakly) Connected Components. The key distinction is that MST iterates through the edges sorted by weight, whereas CC can traverse the unweighted edge in any order.\nMin Heap vs Union Find The following table provides a comparison between the min heap and union-find approaches for finding the MST.\nCriteria Prim\u0026rsquo;s Algorithm Kruskal\u0026rsquo;s Algorithm Technique Min Heap (Priority Queue) Union-Find Graph Type Connected Connected or Disconnected Initialization Single vertex Edge List sorted by weights Time Complexity $O((V + E) \\log V)$ $O(E \\log E)$ Output Parent Array Edge List Code #include \u0026lt;iostream\u0026gt; #include \u0026lt;queue\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;numeric\u0026gt; //------------// // Priority Queue // Prim\u0026#39;s algorithm to find the minimum spanning tree //------------// auto Prim(const std::vector\u0026lt;int\u0026gt;\u0026amp; row_pointer, const std::vector\u0026lt;int\u0026gt;\u0026amp; column_index, const std::vector\u0026lt;float\u0026gt;\u0026amp; values) { // the same setting as Dijkstra\u0026#39;s SSSP const auto num_nodes = row_pointer.size() - 1; std::vector\u0026lt;float\u0026gt; distance(num_nodes, std::numeric_limits\u0026lt;float\u0026gt;::max()); std::vector\u0026lt;int\u0026gt; parent(num_nodes, -1); std::vector\u0026lt;bool\u0026gt; visited(num_nodes, false); std::priority_queue\u0026lt; std::pair\u0026lt;float, int\u0026gt;, std::vector\u0026lt;std::pair\u0026lt;float, int\u0026gt;\u0026gt;, std::greater\u0026lt;std::pair\u0026lt;float, int\u0026gt;\u0026gt;\u0026gt; min_heap; // start from vertex 0 min_heap.push({ 0.0, 0 }); distance[0] = 0.0; while(!min_heap.empty()) { auto [dist, source] = min_heap.top(); min_heap.pop(); visited[source] = true; // iterate over all outgoing edges for(auto i = row_pointer[source]; i \u0026lt; row_pointer[source + 1]; i++) { auto target = column_index[i]; auto weight = values[i]; if(!visited[target] \u0026amp;\u0026amp; weight \u0026lt; distance[target]) { distance[target] = weight; parent[target] = source; min_heap.push({ weight, target }); } } } return parent; } //------------// // Union Find // //------------// auto Kruskal(const std::vector\u0026lt;int\u0026gt;\u0026amp; row_pointer, const std::vector\u0026lt;int\u0026gt;\u0026amp; column_index, const std::vector\u0026lt;float\u0026gt;\u0026amp; values){ auto num_nodes = row_pointer.size() - 1; auto num_edges = column_index.size(); std::vector\u0026lt;std::tuple\u0026lt;int, int, float\u0026gt;\u0026gt; edges; std::vector\u0026lt;std::tuple\u0026lt;int, int, float\u0026gt;\u0026gt; tree; for(int i = 0; i \u0026lt; num_nodes; i++) { for(auto j = row_pointer[i]; j \u0026lt; row_pointer[i + 1]; j++){ edges.emplace_back(i, column_index[j], values[j]); } } // O(ElogE) -- the most expensive part std::sort(edges.begin(), edges.end(), [](auto\u0026amp; a, auto\u0026amp; b){ return std::get\u0026lt;2\u0026gt;(a) \u0026lt; std::get\u0026lt;2\u0026gt;(b); }); std::vector\u0026lt;int\u0026gt; parent(num_nodes); std::vector\u0026lt;int\u0026gt; rank(num_nodes); // keep tree relatively balanced std::iota(parent.begin(), parent.end(), 0); // find auto find = [\u0026amp;parent](int i) { while(parent[i] != i){ i = parent[i]; } return i; }; // union by rank auto unite = [\u0026amp;find, \u0026amp;rank, \u0026amp;parent](int i, int j) { i = find(i); j = find(j); if(rank[i] \u0026gt; rank[j]) { std::swap(i, j); } parent[i] = j; if(rank[i] == rank[j]) { rank[j]++; } }; // O(ElogV) for(const auto [source, target, weight] : edges) { if(find(source) != find(target)) { unite(source, target); tree.emplace_back(source, target, weight); } } return tree; } int main() { std::vector\u0026lt;int\u0026gt; row_pointer = { 0, 3, 5, 7, 10, 12, 14 }; std::vector\u0026lt;int\u0026gt; column_index = { 1, 2, 3, 0, 2, 0, 1, 0, 4, 5, 3, 5, 3, 4 }; std::vector\u0026lt;float\u0026gt; values = { 1.2, 3.4, 0.5, 1.2, 4.1, 3.4, 4.1, 0.5, 2.8, 1.9, 2.8, 4.7, 1.9, 4.7}; auto parent = Prim(row_pointer, column_index, values); for(size_t i = 0; i \u0026lt; parent.size(); i++) { std::cout \u0026lt;\u0026lt; parent[i] \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } std::cout \u0026lt;\u0026lt; std::endl; auto mst = Kruskal(row_pointer, column_index, values); for (const auto\u0026amp; [u, v, w] : mst) { std::cout \u0026lt;\u0026lt; u \u0026lt;\u0026lt; \u0026#34; - \u0026#34; \u0026lt;\u0026lt; v \u0026lt;\u0026lt; \u0026#34; (Weight: \u0026#34; \u0026lt;\u0026lt; w \u0026lt;\u0026lt; \u0026#34;) , \u0026#34;; } std::cout \u0026lt;\u0026lt; std::endl; } ","permalink":"https://yuang-chen.github.io/posts/2023-09-29-minimum-spanning-tree/","summary":"Description A Minimum Spanning Tree (MST) of a weighted, connected, undirected graph is a tree that spans all the vertices in the graph and has the minimum possible total edge weight among all the trees that can be created from the graph. In simpler terms, it\u0026rsquo;s a subgraph that includes all the vertices, is a tree (meaning it has no cycles), and the sum of its edge weights is as small as possible.","title":"Minimum Spanning Tree"},{"content":"Description The implementation of unordered containers rely on hashing techniques and utilize buckets for storing elements. Each bucket is essentially a vector containing a (singly) linked list. The following steps outline how elements are located, whether for finding, inserting, or erasing:\nCompute the hash value of the key. Determine the bucket index by taking the remainder of the hash value divided by the bucket size, e.g., index = {hash value} % {bucket size}. Bucket[index] represents the first element of the linked list. Traverse through the linked list to locate the element by key. While these operations have an average constant-time complexity, they are not necessarily lightweight. They involve computationally expensive overhead, including hash computation and linked list traversal.\nHashtable The underlying hash table mechanisms for std::unordered_set, std::unordered_map, std::unordered_multiset, and std::unordered_multimap are fundamentally the same. Their differences lie in the specific features of the hash table they employ. In this post, I only focus on the simplest implementation, which is unordered_set.\nFeature unordered_set unordered_map unordered_multiset unordered_multimap Element Type Single Element Key-Value Pair Single Element Key-Value Pair Allows Duplicates No No (Unique Keys) Yes Yes Hash Table Bucket 0 or 1 Element 0 or 1 Key-Value Pair Multiple Elements Multiple Key-Value Pairs Code #include \u0026lt;forward_list\u0026gt; #include \u0026lt;functional\u0026gt; #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; template \u0026lt;typename Key, typename Hash = std::hash\u0026lt;Key\u0026gt;, typename KeyEqual = std::equal_to\u0026lt;Key\u0026gt;\u0026gt; class HashTable { public: HashTable(size_t bucket_count = 10) : bucket_count_(bucket_count), element_count_(0), buckets_(bucket_count) { } void insert(const Key\u0026amp; key) { check_load_factor(); size_t bucket_index = Hash{}(key) % bucket_count_; for(const auto\u0026amp; element : buckets_[bucket_index]) { if(KeyEqual{}(element, key)) { return; } } buckets_[bucket_index].push_front(key); ++element_count_; } bool find(const Key\u0026amp; key) // stl\u0026#39;find returns an iterator, and stl\u0026#39;count returns 1 or 0 { size_t bucket_index = Hash{}(key) % bucket_count_; for(const auto\u0026amp; element : buckets_[bucket_index]) { if(KeyEqual{}(element, key)) { return true; } } return false; } void erase(const Key\u0026amp; key) { size_t bucket_index = Hash{}(key) % bucket_count_; // take advantage of forward_list\u0026#39;s remove_if buckets_[bucket_index].remove_if([\u0026amp;key](const auto\u0026amp; element) { return KeyEqual{}(element, key); }); --element_count_; } size_t size() const { return element_count_; } private: void check_load_factor() { double load_factor = static_cast\u0026lt;double\u0026gt;(element_count_) / static_cast\u0026lt;double\u0026gt;(bucket_count_); if(load_factor \u0026gt; 0.7) { rehash(bucket_count_ * 2); } } void rehash(size_t new_bucket_count) { std::vector\u0026lt;std::forward_list\u0026lt;Key\u0026gt;\u0026gt; new_buckets(new_bucket_count); for(const auto\u0026amp; bucket : buckets_) { for(const auto\u0026amp; element : bucket) { size_t new_bucket_index = Hash{}(element) % new_bucket_count; new_buckets[new_bucket_index].push_front(element); } } std::swap(new_buckets, buckets_); bucket_count_ = new_bucket_count; } size_t bucket_count_; size_t element_count_; std::vector\u0026lt;std::forward_list\u0026lt;Key\u0026gt;\u0026gt; buckets_; }; template \u0026lt;typename Key, typename Hash = std::hash\u0026lt;Key\u0026gt;, typename KeyEqual = std::equal_to\u0026lt;Key\u0026gt;\u0026gt; class UnorderedSet { private: HashTable\u0026lt;Key, Hash, KeyEqual\u0026gt; table; public: void insert(const Key\u0026amp; key) { table.insert(key); } bool find(const Key\u0026amp; key) { return table.find(key); } void erase(const Key\u0026amp; key) { table.erase(key); } size_t size() const { return table.size(); } }; template \u0026lt;typename Key, typename Value\u0026gt; struct Pair { using Key = Key; Key key; Value value; Pair(const Key\u0026amp; k, const Value\u0026amp; v) : key(k), value(v) {} }; template \u0026lt; typename Pair, typename Hash = std::hash\u0026lt;Pair::Key\u0026gt;, typename KeyEqual = std::equal_to\u0026lt;Pair::Key\u0026gt;\u0026gt; class UnorderedMap { private: HashTable\u0026lt;Pair, Hash, KeyEqual\u0026gt; table; }; int main() { auto set = HashTable\u0026lt;int\u0026gt;(); set.insert(19); set.insert(5); set.insert(3); set.erase(5); std::cout \u0026lt;\u0026lt; set.find(5) \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; set.size() \u0026lt;\u0026lt; std::endl; } ","permalink":"https://yuang-chen.github.io/posts/2023-09-27-unordered-set/","summary":"Description The implementation of unordered containers rely on hashing techniques and utilize buckets for storing elements. Each bucket is essentially a vector containing a (singly) linked list. The following steps outline how elements are located, whether for finding, inserting, or erasing:\nCompute the hash value of the key. Determine the bucket index by taking the remainder of the hash value divided by the bucket size, e.g., index = {hash value} % {bucket size}.","title":"Unordered {Set|Map|Multiset|Multimap}"},{"content":"Description Both std::set and std::map are underpinned by red-black trees (RBT). RBTs are self-balancing binary trees, albeit not perfectly balanced. In this structure, it\u0026rsquo;s ensured that the values (for std::set) or keys (for std::map) adhere to the following condition: node→left \u0026lt; node \u0026lt; node→right. Consequently, the RBT are considered ordered, so std::set and std::map are called ordered containers.\nRBT are characterized as follows:\nProperty\nA node is either red or black. The root and leaves (nil, nullptr node) are always black. If a node is red, then its children are black. All paths from a node to its leaves contain the same number of black nodes. The longest path from the root to a leaf is no more than twice as long as the shortest path from the root to a leaf. \u0026ndash; The shortest path: all black nodes. \u0026ndash; The longest path: alternate red and black nodes. Basic Operations:\nInsert $O(logN)$, requires rotation Remove $O(logN)$, requires rotation Search $O(logN)$ Rotation:\nalters the structure of the tree by rearranging subtrees goal is to decrease the height of the tree \u0026ndash; maximum height: log(N) \u0026ndash; larger subtrees up, smaller subtrees down left rotation: clockwise rotation right rotation: counterclockwise rotation Insertion todo\nDeletion todo\nDue to the inefficiency of RBT, std::set and std::map are usually replaced with std::unordered_set and std::unordered_map, which utilize a hashing strategy to perform operations in constant time, $O(1)$.\nCode The implementation of RBT can indeed be complex and intricate, making it challenging to remember all the details. With the assistance of ChatGPT, here is an example code snippet:\n#include \u0026lt;iostream\u0026gt; enum Color { RED, BLACK }; template \u0026lt;typename T\u0026gt; struct Node { //K key; for map T value; Color color; Node* left; Node* right; Node* parent; Node(Color c, Node* p = nullptr, T val) : color(c), left(nullptr), right(nullptr), parent(p), value(val) { } }; template \u0026lt;typename T\u0026gt; class RedBlackTree { private: Node* root; void deleteTree(Node* node) { if(node) { deleteTree(node-\u0026gt;left); deleteTree(node-\u0026gt;right); delete node; } } //! very annoying to write // GOAL: node x become the [left child] of its [right child] y // totally six pointers need to be updated void rotateLeft(Node* x) { Node* y = x-\u0026gt;right; x-\u0026gt;right = y-\u0026gt;left; //the left child of y will become the new right child of x. if(y-\u0026gt;left != nullptr) { y-\u0026gt;left-\u0026gt;parent = x; // if y had a left child, update its parent pointer to x. } y-\u0026gt;parent = x-\u0026gt;parent; // update y\u0026#39;s parent to x\u0026#39;s parent. if(x-\u0026gt;parent == nullptr) { root = y; // if x was the root, then y becomes the new root. } else if(x == x-\u0026gt;parent-\u0026gt;left) { //if x was the left child of its parent, then update the left child of x\u0026#39;s parent to y x-\u0026gt;parent-\u0026gt;left = y; } else { //if x was the right child of its parent, update the right child of x\u0026#39;s parent to y x-\u0026gt;parent-\u0026gt;right = y; } y-\u0026gt;left = x; // make x the left child of y x-\u0026gt;parent = y; // update the parent of x to y. } // symmetric operations of rotateLeft // GOAL: node y become the [right child] of its [left child] x void rotateRight(Node* y) { Node* x = y-\u0026gt;left; y-\u0026gt;left = x-\u0026gt;right; if(x-\u0026gt;right != nullptr) { x-\u0026gt;right-\u0026gt;parent = y; } x-\u0026gt;parent = y-\u0026gt;parent; if(y-\u0026gt;parent == nullptr) { root = x; } else if(y == y-\u0026gt;parent-\u0026gt;left) { y-\u0026gt;parent-\u0026gt;left = x; } else { y-\u0026gt;parent-\u0026gt;right = x; } x-\u0026gt;right = y; y-\u0026gt;parent = x; } void fixViolations(Node* x) { Node* parent = nullptr; Node* grandparent = nullptr; while((x != root) \u0026amp;\u0026amp; (x-\u0026gt;color != BLACK) \u0026amp;\u0026amp; (x-\u0026gt;parent-\u0026gt;color == RED)) { parent = x-\u0026gt;parent; grandparent = parent-\u0026gt;parent; // Case A: Parent is left child of grandparent if(parent == grandparent-\u0026gt;left) { Node* uncle = grandparent-\u0026gt;right; if(uncle \u0026amp;\u0026amp; uncle-\u0026gt;color == RED) // uncle is RED { grandparent-\u0026gt;color = RED; parent-\u0026gt;color = BLACK; uncle-\u0026gt;color = BLACK; x = grandparent; } else { if(x == parent-\u0026gt;right) // uncle is BLACK and x is right child { rotateLeft(parent); x = parent; parent = x-\u0026gt;parent; } rotateRight(grandparent); std::swap(parent-\u0026gt;color, grandparent-\u0026gt;color); x = parent; } } else { // Case B: Parent is right child of grandparent Node* uncle = grandparent-\u0026gt;left; if(uncle \u0026amp;\u0026amp; uncle-\u0026gt;color == RED) // uncle is RED { grandparent-\u0026gt;color = RED; parent-\u0026gt;color = BLACK; uncle-\u0026gt;color = BLACK; x = grandparent; } else { if(x == parent-\u0026gt;left) // uncle is BLACK and x is left child { rotateRight(parent); x = parent; parent = x-\u0026gt;parent; } rotateLeft(grandparent); std::swap(parent-\u0026gt;color, grandparent-\u0026gt;color); x = parent; } } } root-\u0026gt;color = BLACK; } void insert(Node*\u0026amp; node, Node* parent, const T\u0026amp; value) { if(node == nullptr) { node = new Node(RED, parent, value); // insert the node and color it RED fixViolations(node); return; } if(value == node-\u0026gt;value) { return; // no need to insert } if(value \u0026lt; node-\u0026gt;value) { insert(node-\u0026gt;left, node, value); } else { insert(node-\u0026gt;right, node, value); } } public: RedBlackTree() : root(nullptr) {} ~RedBlackTree() { deleteTree(root); } }; template \u0026lt;typename T\u0026gt; class set { private: RedBlackTree\u0026lt;T\u0026gt; tree; public: void insert(const T\u0026amp; value) { tree.insert(value); } bool find(const T\u0026amp; value) { return tree.find(value); } }; template \u0026lt;typename K, typename V\u0026gt; struct Pair { K key; V value; Pair(const K\u0026amp; k, const V\u0026amp; v) : key(k), value(v) {} bool operator\u0026lt;(const Pair\u0026amp; rhs) const { return key \u0026lt; rhs.key; } }; template \u0026lt;typename K, typename V\u0026gt; class map { private: RedBlackTree\u0026lt;Pair\u0026lt;K, V\u0026gt;\u0026gt; tree; public: void insert(const K\u0026amp; key, const V\u0026amp; value) { tree.insert(Pair\u0026lt;K, V\u0026gt;(key, value)); } void find(const K\u0026amp; key) { return tree.find(Pair\u0026lt;K, V\u0026gt;(key, V())); } }; int main() { set\u0026lt;int\u0026gt; my_set; my_set.insert(5); my_set.insert(3); my_set.insert(7); return 0; } ","permalink":"https://yuang-chen.github.io/posts/2023-09-26-set-map/","summary":"Description Both std::set and std::map are underpinned by red-black trees (RBT). RBTs are self-balancing binary trees, albeit not perfectly balanced. In this structure, it\u0026rsquo;s ensured that the values (for std::set) or keys (for std::map) adhere to the following condition: node→left \u0026lt; node \u0026lt; node→right. Consequently, the RBT are considered ordered, so std::set and std::map are called ordered containers.\nRBT are characterized as follows:\nProperty\nA node is either red or black.","title":"Set \u0026 Map"},{"content":" count how many triangles can be formed inside the graph undirected graph, and each triangle would be counted for three times, once per node. $O(n^3)$ #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; // Reference: https://github.com/georgegito/vertexwise-triangle-counting/blob/master/src/v3/v3_seq.cpp // allow for parallelism auto bfs_tc(const std::vector\u0026lt;int\u0026gt;\u0026amp; rowPtr, const std::vector\u0026lt;int\u0026gt;\u0026amp; colIdx) { int numTriangles = 0; const auto numVertices = rowPtr.size() - 1; // check if two nodes have an edge between them with binary search (require sorted colIdx) auto intersect = [\u0026amp;](int first, int second) -\u0026gt; bool { // std::find is O(N), assuming the iterator is a forward iterator // auto first_begin = colIdx.begin() + rowPtr[first]; // auto first_end = colIdx.begin() + rowPtr[first + 1]; // std::find(first_begin, first_end, second) != first_end; auto first_begin = rowPtr[first]; auto first_end = rowPtr[first + 1] - 1; auto first_mid = (first_begin + first_end) / 2; while(first_begin \u0026lt;= first_end) { if(colIdx[first_mid] == second) { return true; } else if(colIdx[first_mid] \u0026lt; second) { first_begin = first_mid + 1; } else { first_end = first_mid - 1; } first_mid = (first_begin + first_end) / 2; } return false; }; for(int first = 0; first \u0026lt; numVertices; first++) { for(int i = rowPtr[first]; i \u0026lt; rowPtr[first + 1]; i++) { for(int j = i + 1; j \u0026lt; rowPtr[first + 1]; j++) { const auto second = colIdx[i]; const auto third = colIdx[j]; if(intersect(second, third)) { numTriangles++; } } } } return numTriangles / 3; } int main() { std::vector\u0026lt;int\u0026gt; rowPtr = { 0, 3, 5, 7, 10, 12, 14 }; std::vector\u0026lt;int\u0026gt; colIdx = { 1, 2, 3, 0, 2, 0, 1, 0, 4, 5, 3, 5, 3, 4 }; const auto numVertices = rowPtr.size() - 1; for(int i = 0; i \u0026lt; numVertices; i++) { std::sort(colIdx.begin() + rowPtr[i], colIdx.begin() + rowPtr[i + 1]); } // unweighted graph for simplicity auto triangles = bfs_tc(rowPtr, colIdx); std::cout \u0026lt;\u0026lt; \u0026#34;number of trianlges: \u0026#34; \u0026lt;\u0026lt; triangles \u0026lt;\u0026lt; std::endl; } ","permalink":"https://yuang-chen.github.io/posts/2023-09-23-triangle-counting/","summary":"count how many triangles can be formed inside the graph undirected graph, and each triangle would be counted for three times, once per node. $O(n^3)$ #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; // Reference: https://github.com/georgegito/vertexwise-triangle-counting/blob/master/src/v3/v3_seq.cpp // allow for parallelism auto bfs_tc(const std::vector\u0026lt;int\u0026gt;\u0026amp; rowPtr, const std::vector\u0026lt;int\u0026gt;\u0026amp; colIdx) { int numTriangles = 0; const auto numVertices = rowPtr.size() - 1; // check if two nodes have an edge between them with binary search (require sorted colIdx) auto intersect = [\u0026amp;](int first, int second) -\u0026gt; bool { // std::find is O(N), assuming the iterator is a forward iterator // auto first_begin = colIdx.","title":"Triangle Counting"},{"content":"The betweenness centrality for each vertex is the number of these shortest paths that pass through the vertex.\nperform BFS (or SSSP if weighted graphs) for each vertex keep a stack of path for backtracking, i.e., traversing the graph in reverse BFS order #include \u0026lt;iostream\u0026gt; #include \u0026lt;queue\u0026gt; #include \u0026lt;stack\u0026gt; #include \u0026lt;vector\u0026gt; auto brandes(const std::vector\u0026lt;int\u0026gt;\u0026amp; rowPtr, const std::vector\u0026lt;int\u0026gt;\u0026amp; colIdx) { const auto numVertices = rowPtr.size() - 1; std::vector\u0026lt;float\u0026gt; betweenness(numVertices, 0.0f); //For each vertex s, perform a BFS to establish levels and predecessors //! The time complexity is O(V^3) for(int i = 0; i \u0026lt; numVertices; i++) { std::vector\u0026lt;int\u0026gt; distance(numVertices, -1); // Initialize distance from s std::vector\u0026lt;int\u0026gt; pathCount(numVertices, 0); // Initialize path count std::vector\u0026lt;int\u0026gt; DepScore(numVertices, 0); // Dependency score for each vertex std::queue\u0026lt;int\u0026gt; pathQueue; // for BFS std::stack\u0026lt;int\u0026gt; pathStack; std::vector\u0026lt;std::vector\u0026lt;int\u0026gt;\u0026gt; predecessor(numVertices); distance[i] = 0; pathCount[i] = 1; pathQueue.push(i); // BFS traversal // for a single source vertex, the time complexity is O(V+E), //! so the overall time complexity is O(V+E) for each vertex i while(!pathQueue.empty()) { const auto source = pathQueue.front(); pathQueue.pop(); pathStack.push(source); for(auto i = rowPtr[source]; i \u0026lt; rowPtr[source + 1]; i++) { const auto target = colIdx[i]; if(distance[target] \u0026lt; 0) { pathQueue.push(target); distance[target] = distance[source] + 1; } // shortest path to target via source? if(distance[target] == distance[source] + 1) { pathCount[target] += pathCount[source]; predecessor[target].push_back(source); } } } // traverse the graph in reverse BFS order // backtrack to propagate the dependency scores (delta) back through the graph, // to calculate the BC value for each node. //! The time complexity is O(V*V) for each vertex i //! There are V vertices in the stack, each of which has at most V-1 predecessors while(!pathStack.empty()) { const auto curr = pathStack.top(); pathStack.pop(); for(auto prev : predecessor[curr]) { DepScore[prev] += (static_cast\u0026lt;float\u0026gt;(pathCount[prev]) / pathCount[curr]) * (1 + DepScore[curr]); } if(curr != i) { betweenness[curr] += DepScore[curr]; } } } return betweenness; } int main() { std::vector\u0026lt;int\u0026gt; rowPtr = { 0, 2, 4, 6, 8, 10, 12 }; std::vector\u0026lt;int\u0026gt; colIdx = { 1, 2, 0, 3, 0, 1, 4, 1, 2, 5, 3, 5 }; // Compute betweenness centrality using Brandes\u0026#39;s algorithm // unweighted graph for simplicity auto betweenness = brandes(rowPtr, colIdx); // Print betweenness centrality std::cout \u0026lt;\u0026lt; \u0026#34;Betweenness centrality:\u0026#34; \u0026lt;\u0026lt; std::endl; for(int i = 0; i \u0026lt; betweenness.size(); i++) { std::cout \u0026lt;\u0026lt; \u0026#34;Vertex \u0026#34; \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#34;: \u0026#34; \u0026lt;\u0026lt; betweenness[i] \u0026lt;\u0026lt; std::endl; } } ","permalink":"https://yuang-chen.github.io/posts/2023-09-18-betweenness-centrality/","summary":"The betweenness centrality for each vertex is the number of these shortest paths that pass through the vertex.\nperform BFS (or SSSP if weighted graphs) for each vertex keep a stack of path for backtracking, i.e., traversing the graph in reverse BFS order #include \u0026lt;iostream\u0026gt; #include \u0026lt;queue\u0026gt; #include \u0026lt;stack\u0026gt; #include \u0026lt;vector\u0026gt; auto brandes(const std::vector\u0026lt;int\u0026gt;\u0026amp; rowPtr, const std::vector\u0026lt;int\u0026gt;\u0026amp; colIdx) { const auto numVertices = rowPtr.size() - 1; std::vector\u0026lt;float\u0026gt; betweenness(numVertices, 0.0f); //For each vertex s, perform a BFS to establish levels and predecessors //!","title":"Betweenness Centrality"},{"content":"Description Three different variants of Connected Component (CC) algorithms are implemented, and the comparisons are provided as follows:\nAlgorithm Time Complexity Parallelism Techniques DFS $(O(V + E))$ Poor Recursive Traversal Union-Find $(O(V + E \\alpha(V)))$ Poor Path Compression, Union by Rank Shiloach-Vishkin $(O(\\log^* V))$ Highly Parallel Pointer Jumping Here, $( \\log^* )$ is the iterated logarithm, which is extremely slow-growing, making the algorithm very fast. $( \\alpha(V) )$ is the inverse Ackermann function, practically a constant for all feasible input sizes.\nCode #include \u0026lt;iostream\u0026gt; #include \u0026lt;numeric\u0026gt; #include \u0026lt;unordered_set\u0026gt; #include \u0026lt;vector\u0026gt; /******************** * DFS + Label Propagation ********************/ auto dfs_cc(const std::vector\u0026lt;int\u0026gt;\u0026amp; rowPtr, const std::vector\u0026lt;int\u0026gt;\u0026amp; colIdx) { int labelCount = 0; const auto numVertices = rowPtr.size() - 1; std::vector\u0026lt;int\u0026gt; label(numVertices, -1); std::function\u0026lt;void(int)\u0026gt; dfs = [\u0026amp;dfs, \u0026amp;rowPtr, \u0026amp;colIdx, \u0026amp;label](int curr) { auto currentLabel = label[curr]; for(auto i = rowPtr[curr]; i \u0026lt; rowPtr[curr + 1]; i++) { auto next = colIdx[i]; if(label[next] == -1) { label[next] = currentLabel; dfs(next); // propagate the label } } }; for(int i = 0; i \u0026lt; numVertices; i++) { if(label[i] == -1) { label[i] = labelCount++; dfs(i); } } return label; } /******************** * Union-Find CC ********************/ auto union_find(const std::vector\u0026lt;int\u0026gt;\u0026amp; rowPtr, const std::vector\u0026lt;int\u0026gt;\u0026amp; colIdx) { const auto numVertices = rowPtr.size() - 1; std::vector\u0026lt;int\u0026gt; parent(numVertices); std::vector\u0026lt;int\u0026gt; rank(numVertices); // keep tree relatively balanced std::iota(parent.begin(), parent.end(), 0); //! Find // the resurive find function is not very clever // we can use `while(parent[i] != i) { i = parent[i]; }` instead std::function\u0026lt;int(int)\u0026gt; find = [\u0026amp;find, \u0026amp;parent](int i) { if(parent[i] == i) { return i; } // return find(parent[i]); // if without path compression, ends here //! Optimization: Path Compression parent[i] = find(parent[i]); return parent[i]; }; std::function\u0026lt;void(int, int)\u0026gt; unite = [\u0026amp;find, \u0026amp;parent, \u0026amp;rank](int i, int j) { i = find(i); j = find(j); // if(i != j) // { // parent[j] = i; // } // if without rank, ends here //! Optimization 2: Union by Rank if(i == j) { return; } //Attach the smaller rank tree under root of the larger rank tree if(rank[i] \u0026gt; rank[j]) { std::swap(i, j); } parent[i] = j; // rank[i] \u0026lt; rank[j] if(rank[i] == rank[j]) { rank[j]++; } }; for(int i = 0; i \u0026lt; numVertices; i++) { for(auto j = rowPtr[i]; j \u0026lt; rowPtr[i + 1]; j++) { unite(i, colIdx[j]); } } return parent; } /******************** * Shiloach-Vishkin CC ********************/ auto shiloach_vishkin(const std::vector\u0026lt;int\u0026gt;\u0026amp; rowPtr, const std::vector\u0026lt;int\u0026gt;\u0026amp; colIdx) { const auto numVertices = rowPtr.size() - 1; std::vector\u0026lt;int\u0026gt; parent(numVertices); std::iota(parent.begin(), parent.end(), 0); bool update = true; while(update) { update = false; //? Hooking Phase //! allowing for parallelism for(int i = 0; i \u0026lt; numVertices; i++) { auto curr_parent = parent[i]; for(auto j = rowPtr[i]; j \u0026lt; rowPtr[i + 1]; j++) { auto next = colIdx[j]; // lower ID wins independent of direction if(parent[next] \u0026gt; curr_parent) { update = true; parent[next] = curr_parent; } } } //? Shortcuting / Compressing / Jumping Phase //! allowing for parallelism for(int i = 0; i \u0026lt; numVertices; i++) { while(parent[i] != parent[parent[i]]) { parent[i] = parent[parent[i]]; update = true; } } } return parent; } int main() { std::vector\u0026lt;int\u0026gt; rowPtr = { 0, 1, 2, 3, 4, 5, 6 }; std::vector\u0026lt;int\u0026gt; colIdx = { 1, 2, 0, 4, 5, 3 }; auto label = dfs_cc(rowPtr, colIdx); std::cout \u0026lt;\u0026lt; \u0026#34;Labels of dfs_cc: \u0026#34;; for(auto\u0026amp; l : label) { std::cout \u0026lt;\u0026lt; l \u0026lt;\u0026lt; \u0026#39; \u0026#39;; } label = union_find(rowPtr, colIdx); std::cout \u0026lt;\u0026lt; \u0026#34;\\n\\nLabels of Union Find: \u0026#34;; for(auto\u0026amp; l : label) { std::cout \u0026lt;\u0026lt; l \u0026lt;\u0026lt; \u0026#39; \u0026#39;; } label = shiloach_vishkin(rowPtr, colIdx); std::cout \u0026lt;\u0026lt; \u0026#34;\\n\\nLabels of Shiloach Vishkin: \u0026#34;; for(auto\u0026amp; l : label) { std::cout \u0026lt;\u0026lt; l \u0026lt;\u0026lt; \u0026#39; \u0026#39;; } return 0; } ","permalink":"https://yuang-chen.github.io/posts/2023-09-12-connected-components/","summary":"Description Three different variants of Connected Component (CC) algorithms are implemented, and the comparisons are provided as follows:\nAlgorithm Time Complexity Parallelism Techniques DFS $(O(V + E))$ Poor Recursive Traversal Union-Find $(O(V + E \\alpha(V)))$ Poor Path Compression, Union by Rank Shiloach-Vishkin $(O(\\log^* V))$ Highly Parallel Pointer Jumping Here, $( \\log^* )$ is the iterated logarithm, which is extremely slow-growing, making the algorithm very fast. $( \\alpha(V) )$ is the inverse Ackermann function, practically a constant for all feasible input sizes.","title":"Connected Components"},{"content":"Description STL indeed offers std::list and std::forward_list, which are essentially double-linked list and single-linked list, respectively. std::list provides operations like push_back/front, pop_back/front with a time complexity of O(1), and supports bidirectional iterators. On the other hand, std::forward_list only allows fronting operations with O(1) and insert/erase_after for backing operations, which have a time complexity of O(n); also, it only supports forward iterators.\nA valuable feature of lists is that they prohibit iterator invalidation compared to some other data structures.\n// code example for(auto it = container.begin(); it != container.end(); ++it ) { container.push_back(new_element); // which might cause memory re-allocation } When elements are inserted or deleted in a list, it doesn\u0026rsquo;t affect the memory addresses or links of the other elements. So, one can manipulate elements within a list without worrying about the iterators becoming invalidated (i.e., pointing to unexpected or erroneous locations in memory). Thus, it is safe for a list to run the code example.\nIn contrast, some other data structures, like vectors, set or unordered_set, might need to reallocate memory or rehash the elements when elements are added or removed, leading to potential iterator invalidation. Do not employ the above code to those containers!\nCode Basic implementations of list and forward_list:\n#include \u0026lt;iostream\u0026gt; /********************/ /*Double linked list*/ /********************/ template \u0026lt;typename T\u0026gt; class List { private: struct Node { T data_; Node* next_; Node* prev_; Node(T data, Node* next, Node* prev) : data_(data), next_(next), prev_(prev) {} }; Node* head_; Node* tail_; size_t size_; public: List() : head_(nullptr), tail_(nullptr), size_(0) {} ~List() { Node* curr = head_; while(curr != nullptr) { Node* next = curr-\u0026gt;next_; delete curr; curr = next; } } void push_back(T value) { Node* new_node = new Node(value, nullptr, nullptr); if(tail_) { tail_-\u0026gt;next_ = new_node; new_node-\u0026gt;prev_ = tail_; tail_ = new_node; } else { head_ = new_node; tail_ = new_node; } size_++; } void push_front(T value) { Node* new_node = new Node(value, nullptr, nullptr); if(head_) { head_-\u0026gt;prev_ = new_node; new_node-\u0026gt;next_ = head_; head_ = new_node; } else { head_ = new_node; tail_ = new_node; } size_++; } void pop_back() { if(tail_) { Node* temp = tail_; tail_ = tail_-\u0026gt;prev_; delete tail_; size_--; } } void pop_front() { if(head_) { Node* temp = head_; head_ = head_-\u0026gt;next_; delete head_; size_--; } } T\u0026amp; back() { return tail_-\u0026gt;data_; } T\u0026amp; front() { return head_-\u0026gt;data_; } // the standard library use iterator to index the position void insert(size_t pos, T value) {} size_t size() const { return size_; } }; /********************/ /*Single linked list*/ /********************/ template \u0026lt;typename T\u0026gt; class ForwardList { private: struct Node { T data_; Node* next_; Node(T data, Node* next) : data_(data), next_(next) {} }; Node* head_; size_t size_; public: ForwardList() : head_(nullptr), size_(0) {} ~ForwardList() { Node* curr = head_; while(curr != nullptr) { Node* next = curr-\u0026gt;next_; delete curr; curr = next; } } // stl library uses iterator to index the position void insert_after(size_t pos, T value) { if(pos \u0026gt; size_) { return; } Node* new_node = new Node(value, nullptr); Node* curr = head_; for(size_t i = 0; i \u0026lt; pos - 1; i++) { curr = curr-\u0026gt;next_; } Node* temp = curr-\u0026gt;next_; curr-\u0026gt;next_ = new_node; new_node-\u0026gt;next_ = temp; size_++; } void push_front(T value) { Node* new_node = new Node(value, nullptr); if(head_) { new_node-\u0026gt;next_ = head_; head_ = new_node; } else { head_ = new_node; } size_++; } void erase_after(size_t pos) { if(pos \u0026gt; size_) { return; } Node* curr = head_; for(size_t i = 0; i \u0026lt; pos - 1; i++) { curr = curr-\u0026gt;next_; } Node* temp = curr-\u0026gt;next_; curr-\u0026gt;next_ = temp-\u0026gt;next_; delete temp; } void pop_front() { if(head_) { Node* temp = head_; head_ = head_-\u0026gt;next_; delete temp; size_--; } } T\u0026amp; front() { return head_-\u0026gt;data_; } // the standard library use iterator to index the position void insert(size_t pos, T value) {} size_t size() const { return size_; } }; int main() { List\u0026lt;int\u0026gt; list; list.push_back(1); list.push_back(2); list.push_back(3); list.push_back(4); list.push_front(-1); list.push_front(-2); list.push_front(-3); list.push_front(-4); std::cout \u0026lt;\u0026lt; \u0026#34;list size_: \u0026#34; \u0026lt;\u0026lt; list.size() \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;list head_: \u0026#34; \u0026lt;\u0026lt; list.front() \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;list tail_: \u0026#34; \u0026lt;\u0026lt; list.back() \u0026lt;\u0026lt; std::endl; ForwardList\u0026lt;int\u0026gt; forward_list; forward_list.push_front(6); forward_list.push_front(3); forward_list.push_front(2); forward_list.push_front(1); forward_list.insert_after(3, 4); forward_list.insert_after(4, 5); while(forward_list.size() \u0026gt; 0) { std::cout \u0026lt;\u0026lt; forward_list.front() \u0026lt;\u0026lt; std::endl; forward_list.pop_front(); } return 0; } Reference https://en.cppreference.com/w/cpp/container/list https://en.cppreference.com/w/cpp/container/forward_list\n","permalink":"https://yuang-chen.github.io/posts/2023-09-11-list/","summary":"Description STL indeed offers std::list and std::forward_list, which are essentially double-linked list and single-linked list, respectively. std::list provides operations like push_back/front, pop_back/front with a time complexity of O(1), and supports bidirectional iterators. On the other hand, std::forward_list only allows fronting operations with O(1) and insert/erase_after for backing operations, which have a time complexity of O(n); also, it only supports forward iterators.\nA valuable feature of lists is that they prohibit iterator invalidation compared to some other data structures.","title":"List"},{"content":"Two variants of Single-Source Shortest Path (SSSP) have been implemented as follows. Bellman-Ford is the one that is widely implemented in parallel graph frameworks. This is because the use of a heap in Dijkstra\u0026rsquo;s algorithm can limit the parallelism of the code.\nCriteria Dijkstra\u0026rsquo;s Algorithm Bellman-Ford Algorithm Type Greedy Dynamic Programming Usage Positive weights Negative weights OK Time Complexity O((V + E) * log(V)) O(V * E) Negative Cycles No Yes (Detectable) Data Structures Priority Queue None (Arrays) Initialization Start node: 0, rest ∞ Start node: 0, rest ∞ Relaxation Decrease Key Relaxation BellmanFord BellmanFord: Perform numVertices - 1 iterations of graph traversal to find the shortest path an additional iteration checks if negative cycles exist $O(|V| * |E|)$ time complexity Code #include \u0026lt;iostream\u0026gt; #include \u0026lt;queue\u0026gt; #include \u0026lt;vector\u0026gt; std::vector\u0026lt;int\u0026gt; bellmanFord(const int root, const std::vector\u0026lt;int\u0026gt;\u0026amp; rowPtr, const std::vector\u0026lt;int\u0026gt;\u0026amp; colIdx, const std::vector\u0026lt;float\u0026gt;\u0026amp; weight) { const auto numVertices = rowPtr.size() - 1; std::vector\u0026lt;float\u0026gt; distance(numVertices, std::numeric_limits\u0026lt;float\u0026gt;::max()); std::vector\u0026lt;int\u0026gt; parent(numVertices, -1); distance[root] = 0.0f; // The shortest path between any two vertices in a graph can contain at most numVertices - 1 edges. // If there are more edges in the path, it means there must be a cycle in the path, // hence, numVertices - 1 iterations of edge relaxation are required for(size_t i = 0; i \u0026lt; numVertices - 1; i++) { for(size_t curr = 0; curr \u0026lt; numVertices; ++curr) { for(auto j = rowPtr[curr]; j \u0026lt; rowPtr[curr + 1]; j++) { const auto next = colIdx[j]; const auto wgt = weight[j]; if(distance[curr] != std::numeric_limits\u0026lt;float\u0026gt;::max() \u0026amp;\u0026amp; (distance[curr] + wgt \u0026lt; distance[next])) // check if curr is valid { distance[next] = distance[curr] + wgt; parent[next] = curr; } } } } // check for negative cycles // The sum of the edge weights along the cycle is negative. // If you detect a change in the distance of any vertex during this additional iteration, // it implies the presence of a negative-weight cycle in the graph. // The reason is that the negative cycle allows you to keep going around it, reducing the distance with each iteration. // in the absence of negative-weight cycles, the distance of each vertex should have stabilized after V-1 iterations. for(size_t curr = 0; curr \u0026lt; numVertices; curr++) { for(auto j = rowPtr[curr]; j \u0026lt; rowPtr[curr + 1]; j++) { const auto next = colIdx[j]; const auto wgt = weight[j]; if(distance[curr] != std::numeric_limits\u0026lt;float\u0026gt;::max() \u0026amp;\u0026amp; (distance[curr] + wgt \u0026lt; distance[next])) { std::cout \u0026lt;\u0026lt; \u0026#34;Negative Cycle Detected\\n\u0026#34;; return {}; } } } // Print shortest distances from the curr // std::cout \u0026lt;\u0026lt; \u0026#34;Shortest distances from vertex \u0026#34; \u0026lt;\u0026lt; root \u0026lt;\u0026lt; \u0026#34;:\u0026#34; \u0026lt;\u0026lt; std::endl; // for(int i = 0; i \u0026lt; numVertices; i++) // { // std::cout \u0026lt;\u0026lt; \u0026#34;Vertex \u0026#34; \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#34;: \u0026#34; \u0026lt;\u0026lt; distance[i] \u0026lt;\u0026lt; std::endl; // } return parent; } int main() { std::vector\u0026lt;int\u0026gt; rowPtr = { 0, 2, 4, 6, 8, 10, 12 }; std::vector\u0026lt;int\u0026gt; colIdx = { 1, 2, 0, 3, 0, 1, 4, 1, 2, 5, 3, 5 }; std::vector\u0026lt;float\u0026gt; weight = { 1.2, 2.3, 0.5, 3.1, 4.4, 0.7, 2.8, 1.9, 0.8, 2.0, 1.5, 3.3 }; auto path = bellmanFord(0, rowPtr, colIdx, weight); if(path.empty()) { std::cout \u0026lt;\u0026lt; \u0026#34;Path Not Found\\n\u0026#34;; } else { std::cout \u0026lt;\u0026lt; \u0026#34;Path from Root to next: \u0026#34;; for(size_t i = 0; i \u0026lt; path.size(); i++) { std::cout \u0026lt;\u0026lt; path[i] \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } } std::cout \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; return 0; } Dijkstra Use priority_queue to select the vertex with smallest distance Assuming non-negative weights $O((|E| + |V|) * log(|V|))$ time complexity Code auto dijkstra(const int root, const std::vector\u0026lt;int\u0026gt;\u0026amp; rowPtr, const std::vector\u0026lt;int\u0026gt;\u0026amp; colIdx, const std::vector\u0026lt;float\u0026gt;\u0026amp; weight) { const auto numVertices = rowPtr.size() - 1; std::vector\u0026lt;float\u0026gt; distance(numVertices, std::numeric_limits\u0026lt;float\u0026gt;::max()); std::vector\u0026lt;int\u0026gt; parent(numVertices, -1); std::priority_queue\u0026lt;std::pair\u0026lt;float, int\u0026gt;, std::vector\u0026lt;std::pair\u0026lt;float, int\u0026gt;\u0026gt;, std::greater\u0026lt;std::pair\u0026lt;float, int\u0026gt;\u0026gt; min_heap; distance[root] = 0.0f; min_heap.push(std::make_pair(0.0f, root)); while(!min_heap.empty()) { const auto [dist, curr] = min_heap.top(); min_heap.pop(); // a shorter path to vertex curr has already been discovered and processed // further exploration of u is unnecessary. if(dist \u0026gt; distance[curr]) { continue; } for(auto i = rowPtr[curr]; i \u0026lt; rowPtr[curr + 1]; i++) { const auto next = colIdx[i]; const auto wgt = weight[i]; if(distance[curr] != std::numeric_limits\u0026lt;float\u0026gt;::max() \u0026amp;\u0026amp; (distance[curr] + wgt \u0026lt; distance[next])) { distance[next] = distance[curr] + wgt; parent[next] = curr; min_heap.emplace(distance[next], next); } } } // Print shortest distances from the curr // std::cout \u0026lt;\u0026lt; \u0026#34;Shortest distances from vertex \u0026#34; \u0026lt;\u0026lt; root \u0026lt;\u0026lt; \u0026#34;:\u0026#34; \u0026lt;\u0026lt; std::endl; // for(int i = 0; i \u0026lt; numVertices; i++) // { // std::cout \u0026lt;\u0026lt; \u0026#34;Vertex \u0026#34; \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#34;: \u0026#34; \u0026lt;\u0026lt; distance[i] \u0026lt;\u0026lt; std::endl; // } return parent; } int main() { std::vector\u0026lt;int\u0026gt; rowPtr = { 0, 2, 4, 6, 8, 10, 12 }; std::vector\u0026lt;int\u0026gt; colIdx = { 1, 2, 0, 3, 0, 1, 4, 1, 2, 5, 3, 5 }; std::vector\u0026lt;float\u0026gt; weight = { 1.2, 2.3, 0.5, 3.1, 4.4, 0.7, 2.8, 1.9, 0.8, 2.0, 1.5, 3.3 }; auto path = dijkstra(0, rowPtr, colIdx, weight); if(path.empty()) { std::cout \u0026lt;\u0026lt; \u0026#34;Path Not Found\\n\u0026#34;; } else { std::cout \u0026lt;\u0026lt; \u0026#34;Path from Root to next: \u0026#34;; for(size_t i = 0; i \u0026lt; path.size(); i++) { std::cout \u0026lt;\u0026lt; path[i] \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } } std::cout \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; return 0; } ","permalink":"https://yuang-chen.github.io/posts/2023-09-09-sssp/","summary":"Two variants of Single-Source Shortest Path (SSSP) have been implemented as follows. Bellman-Ford is the one that is widely implemented in parallel graph frameworks. This is because the use of a heap in Dijkstra\u0026rsquo;s algorithm can limit the parallelism of the code.\nCriteria Dijkstra\u0026rsquo;s Algorithm Bellman-Ford Algorithm Type Greedy Dynamic Programming Usage Positive weights Negative weights OK Time Complexity O((V + E) * log(V)) O(V * E) Negative Cycles No Yes (Detectable) Data Structures Priority Queue None (Arrays) Initialization Start node: 0, rest ∞ Start node: 0, rest ∞ Relaxation Decrease Key Relaxation BellmanFord BellmanFord: Perform numVertices - 1 iterations of graph traversal to find the shortest path an additional iteration checks if negative cycles exist $O(|V| * |E|)$ time complexity Code #include \u0026lt;iostream\u0026gt; #include \u0026lt;queue\u0026gt; #include \u0026lt;vector\u0026gt; std::vector\u0026lt;int\u0026gt; bellmanFord(const int root, const std::vector\u0026lt;int\u0026gt;\u0026amp; rowPtr, const std::vector\u0026lt;int\u0026gt;\u0026amp; colIdx, const std::vector\u0026lt;float\u0026gt;\u0026amp; weight) { const auto numVertices = rowPtr.","title":"SSSP"},{"content":"Description std::deque extends the interfaces of std::vector with push_front, pop_front, etc., such that elements can be inserted or removed at the end or beginning at constant time.\nI\u0026rsquo;ve hardly ever incorporated std::deque in my own coding projects, and it\u0026rsquo;s a rarity in other people\u0026rsquo;s work as well.\nCode std::deque is essentially a sequence of individually allocated fixed-size arrays. The real challenge lies in the bookkeeping. Four variables are relied on to keep track of data:\nblock_front block_back index_front inside the block_front index_back inside the block_back #include \u0026lt;iostream\u0026gt; template \u0026lt;typename T\u0026gt; class Deque { private: T** blocks; size_t block_size; size_t num_blocks; size_t blocks_used; size_t block_front; size_t block_back; size_t index_front; size_t index_back; size_t size_; public: Deque(); ~Deque(); void push_back(T elem); void push_front(T elem); void pop_back(); void pop_front(); T front(); T back(); bool empty(); T\u0026amp; at(size_t index); T\u0026amp; operator[](size_t index); size_t size() const; }; template \u0026lt;typename T\u0026gt; Deque\u0026lt;T\u0026gt;::Deque() { num_blocks = 5; block_size = 8; blocks = new T*[num_blocks]; for(int i = 0; i \u0026lt; num_blocks; i++) { blocks[i] = new T[block_size]; // elements are not initialized } // the starting block is the middle block block_front = num_blocks / 2; block_back = num_blocks / 2; // the starting index of element is at the middle of the block index_front = block_size / 2; index_back = block_size / 2 - 1; // ensures start with empty blocks_used = 0; size_ = 0; } template \u0026lt;typename T\u0026gt; Deque\u0026lt;T\u0026gt;::~Deque() { for(int i = 0; i \u0026lt; num_blocks; i++) { delete[] blocks[i]; } delete[] blocks; } /** * case breakdown: * 0. deque is empty * * just add the element * 1. current block is not full * * just add the element * 2. current block is full --\u0026gt; further breakdown: * 2.1. there is space in the next block * * store the element in next block * * update the indexes * 2.2. there is no space in the next block * * create new blocks * * copy the elements to the new blocks * * delete the old blocks * * update the indexes */ template \u0026lt;typename T\u0026gt; void Deque\u0026lt;T\u0026gt;::push_back(T elem) { //if the deque is empty if(size_ == 0) { blocks_used = 1; blocks[block_back][++index_back] = elem; size_++; return; } // if the block is not full if(index_back \u0026lt; block_size - 1) { blocks[block_back][++index_back] = elem; size_++; return; } // ********************* // if current block is full //*********************** // if the next block is not full, move to it if(block_back \u0026lt; num_blocks - 1) { index_back = 0; blocks[++block_back][index_back] = elem; size_++; blocks_used++; return; } // if the next block is full, allocate new blocks // 0 1 1 1 -\u0026gt; 0 0 1 1 1 1 0 0 (8-3)/2 = 2 size_t old_block_front = block_front; num_blocks *= 2; index_back = 0; blocks_used++; size_++; block_front = (num_blocks - blocks_used) / 2; block_back = block_front + blocks_used - 1; T** new_blocks = new T*[num_blocks]; for(size_t i = 0; i \u0026lt; block_front; i++) { new_blocks[i] = new T[block_size]; // allocate new blocks } for(size_t i = block_front; i \u0026lt; block_back; i++) { new_blocks[i] = blocks[old_block_front++]; // reuse old blocks } for(size_t i = block_back; i \u0026lt; num_blocks; i++) { new_blocks[i] = new T[block_size]; // allocate new blocks } new_blocks[block_back][0] = elem; // only delete the ptrptr** T** temp = blocks; blocks = new_blocks; delete[] temp; // std::cout \u0026lt;\u0026lt; \u0026#34;Blocks: \u0026#34; \u0026lt;\u0026lt; num_blocks \u0026lt;\u0026lt; std::endl; // std::cout \u0026lt;\u0026lt; \u0026#34;Blocks used: \u0026#34; \u0026lt;\u0026lt; blocks_used \u0026lt;\u0026lt; std::endl; // std::cout \u0026lt;\u0026lt; \u0026#34;Block front: \u0026#34; \u0026lt;\u0026lt; block_front \u0026lt;\u0026lt; std::endl; // std::cout \u0026lt;\u0026lt; \u0026#34;Block back: \u0026#34; \u0026lt;\u0026lt; block_back \u0026lt;\u0026lt; std::endl; // std::cout \u0026lt;\u0026lt; \u0026#34;Index front: \u0026#34; \u0026lt;\u0026lt; index_front \u0026lt;\u0026lt; std::endl; // std::cout \u0026lt;\u0026lt; \u0026#34;Index back: \u0026#34; \u0026lt;\u0026lt; index_back \u0026lt;\u0026lt; std::endl; // std::cout \u0026lt;\u0026lt; std::endl \u0026lt;\u0026lt; std::endl; } // reverse the push_back template \u0026lt;typename T\u0026gt; void Deque\u0026lt;T\u0026gt;::push_front(T elem) { //if the deque is empty if(size_ == 0) { blocks_used = 1; blocks[block_front][--index_front] = elem; size_++; return; } // if the block is not full if(index_front \u0026gt; 0) { blocks[block_front][--index_front] = elem; size_++; return; } // ********************* // if the block is full //*********************** // if the prior block is not full, move to it if(block_front \u0026gt; 0) { index_front = block_size - 1; blocks[--block_front][index_front] = elem; size_++; blocks_used++; return; } // if the next block is full, allocate new blocks // 1 1 1 0 -\u0026gt; 0 0 1 1 1 1 0 0 (8-3)/2 = 2 size_t old_block_front = block_front; num_blocks *= 2; index_front = block_size - 1; blocks_used++; size_++; block_front = (num_blocks - blocks_used) / 2; block_back = block_front + blocks_used - 1; T** new_blocks = new T*[num_blocks]; for(size_t i = 0; i \u0026lt; block_front; i++) { new_blocks[i] = new T[block_size]; // allocate new blocks } for(size_t i = block_front; i \u0026lt; block_back; i++) { new_blocks[i] = blocks[old_block_front++]; // reuse old blocks } for(size_t i = block_back; i \u0026lt; num_blocks; i++) { new_blocks[i] = new T[block_size]; // allocate new blocks } new_blocks[block_front][index_front] = elem; // only delete the ptrptr** T** temp = blocks; blocks = new_blocks; delete[] temp; // std::cout \u0026lt;\u0026lt; \u0026#34;Blocks: \u0026#34; \u0026lt;\u0026lt; num_blocks \u0026lt;\u0026lt; std::endl; // std::cout \u0026lt;\u0026lt; \u0026#34;Blocks used: \u0026#34; \u0026lt;\u0026lt; blocks_used \u0026lt;\u0026lt; std::endl; // std::cout \u0026lt;\u0026lt; \u0026#34;Block front: \u0026#34; \u0026lt;\u0026lt; block_front \u0026lt;\u0026lt; std::endl; // std::cout \u0026lt;\u0026lt; \u0026#34;Block back: \u0026#34; \u0026lt;\u0026lt; block_back \u0026lt;\u0026lt; std::endl; // std::cout \u0026lt;\u0026lt; \u0026#34;Index front: \u0026#34; \u0026lt;\u0026lt; index_front \u0026lt;\u0026lt; std::endl; // std::cout \u0026lt;\u0026lt; \u0026#34;Index back: \u0026#34; \u0026lt;\u0026lt; index_back \u0026lt;\u0026lt; std::endl; // std::cout \u0026lt;\u0026lt; std::endl \u0026lt;\u0026lt; std::endl; } template \u0026lt;typename T\u0026gt; void Deque\u0026lt;T\u0026gt;::pop_back() { if(size_ == 0) return; // if the back index is not 0 size_--; blocks[block_back][index_back] = std::numeric_limits\u0026lt;T\u0026gt;::max(); // register an error signal index_back--; // if the back index is 0, move to prior block if(index_back == std::numeric_limits\u0026lt;size_t\u0026gt;::max()) // if(int(index_back) == -1) { index_back = block_size - 1; block_back--; } } template \u0026lt;typename T\u0026gt; void Deque\u0026lt;T\u0026gt;::pop_front() { if(size_ == 0) { return; } // if the back index is not 0 size_--; blocks[block_front][index_front] = std::numeric_limits\u0026lt;T\u0026gt;::max(); // register an error signal index_front++; // if the back index is 0 if(index_front == block_size) { block_front++; index_front = 0; } } template \u0026lt;typename T\u0026gt; T Deque\u0026lt;T\u0026gt;::back() { return blocks[block_back][index_back]; } template \u0026lt;typename T\u0026gt; T Deque\u0026lt;T\u0026gt;::front() { return blocks[block_front][index_front]; } template \u0026lt;typename T\u0026gt; bool Deque\u0026lt;T\u0026gt;::empty() { return size_ == 0; } template \u0026lt;typename T\u0026gt; T\u0026amp; Deque\u0026lt;T\u0026gt;::operator[](size_t index) // do not perform boundary checking { const size_t remain = block_size - index_front; if(index \u0026lt; remain) { return blocks[block_front][index_front + index]; } size_t curr_block = block_front + (index + remain) / block_size; size_t curr_index = (index + remain) % block_size; return blocks[curr_block][curr_index]; } template \u0026lt;typename T\u0026gt; T\u0026amp; Deque\u0026lt;T\u0026gt;::at(size_t index) // perform boundary checking { if(index \u0026gt;= size_) { throw std::out_of_range(\u0026#34;Deque::at() index out of range\u0026#34;); } const size_t remain = block_size - index_front; if(index \u0026lt; remain) { return blocks[block_front][index_front + index]; } const size_t curr_block = block_front + (index + remain) / block_size; const size_t curr_index = (index + remain) % block_size; return blocks[curr_block][curr_index]; } template \u0026lt;typename T\u0026gt; size_t Deque\u0026lt;T\u0026gt;::size() const { return size_; } int main() { Deque\u0026lt;int\u0026gt; deq; std::cout \u0026lt;\u0026lt; \u0026#34;-----push_back test-----\u0026#34; \u0026lt;\u0026lt; std::endl; for(int i = 0; i \u0026lt; 21; i++) { deq.push_back(i); std::cout \u0026lt;\u0026lt; deq.back() \u0026lt;\u0026lt; \u0026#39; \u0026#39;; } std::cout \u0026lt;\u0026lt; \u0026#34;\\n-----pop_back test-----\u0026#34; \u0026lt;\u0026lt; std::endl; for(int i = 0; i \u0026lt; 21; i++) { std::cout \u0026lt;\u0026lt; deq.back() \u0026lt;\u0026lt; \u0026#39; \u0026#39;; deq.pop_back(); } std::cout \u0026lt;\u0026lt; \u0026#34;\\n-----push_front test-----\u0026#34; \u0026lt;\u0026lt; std::endl; for(int i = 0; i \u0026lt; 21; i++) { deq.push_front(i); std::cout \u0026lt;\u0026lt; deq.front() \u0026lt;\u0026lt; \u0026#39; \u0026#39;; } std::cout \u0026lt;\u0026lt; \u0026#34;\\n-----pop_front test-----\u0026#34; \u0026lt;\u0026lt; std::endl; for(int i = 0; i \u0026lt; 21; i++) { std::cout \u0026lt;\u0026lt; deq.front() \u0026lt;\u0026lt; \u0026#39; \u0026#39;; deq.pop_front(); } std::cout \u0026lt;\u0026lt; \u0026#34;\\n-----operator[] test-----\u0026#34; \u0026lt;\u0026lt; std::endl; for(int i = 0; i \u0026lt; 11; i++) deq.push_back(i); for(int i = 0; i \u0026lt; 12; i++) { std::cout \u0026lt;\u0026lt; deq[i] \u0026lt;\u0026lt; \u0026#39; \u0026#39;; } std::cout \u0026lt;\u0026lt; \u0026#34;\\n-----at() test-----\u0026#34; \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; deq.size() \u0026lt;\u0026lt; std::endl; for(int i = 0; i \u0026lt; 12; i++) { std::cout \u0026lt;\u0026lt; deq.at(i) \u0026lt;\u0026lt; \u0026#39; \u0026#39;; } } Reference https://stackoverflow.com/questions/6292332/what-really-is-a-deque-in-stl https://github.com/rdmc10/Deque/blob/main/deque.cpp https://en.cppreference.com/w/cpp/container/deque\n","permalink":"https://yuang-chen.github.io/posts/2023-09-04-deque/","summary":"Description std::deque extends the interfaces of std::vector with push_front, pop_front, etc., such that elements can be inserted or removed at the end or beginning at constant time.\nI\u0026rsquo;ve hardly ever incorporated std::deque in my own coding projects, and it\u0026rsquo;s a rarity in other people\u0026rsquo;s work as well.\nCode std::deque is essentially a sequence of individually allocated fixed-size arrays. The real challenge lies in the bookkeeping. Four variables are relied on to keep track of data:","title":"Deque"},{"content":" Array is allocated in stack memory Vector is allocated in heap memory. Its capacity is “pre-allocated”. #include \u0026lt;iostream\u0026gt; template\u0026lt;typename T\u0026gt; class Vector { private: T* data_; size_t size_; size_t capacity_; public: Vector(): data_(nullptr), size_(0), capacity_(0) {} Vector(size_t n_): size_(n_), capacity_(n_) { data_ = new T[n_]; } ~Vector() { delete [] data_; }; T\u0026amp; operator[] (size_t index) { return data_[index]; } const T\u0026amp; operator[] (size_t index) const { return data_[index]; } size_t size() const { return size_; } void push_back(const T\u0026amp; value) { if(size_ == capacity_) { capacity_ = size_ == 0? 1: 2 * capacity_; T* new_data_ = new T[capacity_]; for(size_t i = 0; i \u0026lt; size_; i++) { new_data_[i] = data_[i]; } delete[] data_; data_ = new_data_; } data_[size_] = value; size_++; } }; template\u0026lt;typename T, size_t size_\u0026gt; class Array { private: T data_[size_]; public: T\u0026amp; operator[] (size_t index) { return data_[index]; } const T\u0026amp; operator[] (size_t index) const { return data_[index]; } size_t size() const { return size_; } }; int main() { Vector\u0026lt;int\u0026gt; vec; vec.push_back(10); vec.push_back(2); for(int i =0; i \u0026lt; vec.size(); i++) std::cout \u0026lt;\u0026lt; vec[i] \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; Array\u0026lt;int, 3\u0026gt; arr{}; arr[1] = 9; for(int i =0; i \u0026lt; arr.size(); i++) std::cout \u0026lt;\u0026lt; arr[i] \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; return 0; } ","permalink":"https://yuang-chen.github.io/posts/2023-09-02-vector-array/","summary":"Array is allocated in stack memory Vector is allocated in heap memory. Its capacity is “pre-allocated”. #include \u0026lt;iostream\u0026gt; template\u0026lt;typename T\u0026gt; class Vector { private: T* data_; size_t size_; size_t capacity_; public: Vector(): data_(nullptr), size_(0), capacity_(0) {} Vector(size_t n_): size_(n_), capacity_(n_) { data_ = new T[n_]; } ~Vector() { delete [] data_; }; T\u0026amp; operator[] (size_t index) { return data_[index]; } const T\u0026amp; operator[] (size_t index) const { return data_[index]; } size_t size() const { return size_; } void push_back(const T\u0026amp; value) { if(size_ == capacity_) { capacity_ = size_ == 0?","title":"Vector \u0026 Array"},{"content":"Iterative BFS Despite its apparent simplicity, this approach relies heavily on the utilization of various STL containers. std::unordered_map records the parent of each node std::unordered_set checks if a node has been visited std::queue allows the nodes be accessed in the width-first flow; using std::stack for depth-first flow std::stack reverses the parents, so the path can be printed in root-to-target order. #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;unordered_map\u0026gt; #include \u0026lt;unordered_set\u0026gt; #include \u0026lt;queue\u0026gt; #include \u0026lt;stack\u0026gt; std::stack\u0026lt;int\u0026gt; BFS(const int root, const int target, const std::vector\u0026lt;int\u0026gt;\u0026amp; rowPtr, const std::vector\u0026lt;int\u0026gt;\u0026amp; colIdx) { std::unordered_map\u0026lt;int, int\u0026gt; parent; std::unordered_set\u0026lt;int\u0026gt; visited; std::queue\u0026lt;int\u0026gt; nodeQue; // std::stack\u0026lt;int\u0026gt; nodeStk for DFS std::stack\u0026lt;int\u0026gt; path; bool hasFound = false; nodeQue.push(root); visited.insert(root); while(!nodeQue.empty()) { auto curr = nodeQue.front(); // nodeStk.top() for DFS nodeQue.pop(); if(curr == target) { hasFound = true; break; } for(int i = rowPtr[curr]; i \u0026lt; rowPtr[curr+1]; i++) { auto next = colIdx[i]; if(visited.count(next) == 0) { nodeQue.push(next); visited.insert(next); parent[next] = curr; } } } if(hasFound) { auto curr = target; path.push(curr); if(curr != root) { curr = parent[curr]; path.push(curr); } path.push(root); } return path; } int main() { std::vector\u0026lt;int\u0026gt; rowPointer = {0, 2, 4, 6, 8, 10, 12}; std::vector\u0026lt;int\u0026gt; colIndices = {1, 2, 0, 3, 0, 1, 4, 1, 2, 5, 3, 5}; auto path = BFS(0, 5, rowPointer, colIndices); if(path.empty()) { std::cout \u0026lt;\u0026lt; \u0026#34;Path Not Found\\n\u0026#34;; } else { std::cout \u0026lt;\u0026lt; \u0026#34;Path from Root to Target: \u0026#34;; while(!path.empty()) { std::cout \u0026lt;\u0026lt; path.top() \u0026lt;\u0026lt; \u0026#39; \u0026#39;; path.pop(); } } return 0; } Recursive DFS DFS naturally aligns with recursion. In the earlier example, the iterative DFS employs the std::stack to facilitate the depth-first progression. On the other hand, recursion, which stores function calls in the stack memory, permits the execution of DFS functions in a last-in-first-out manner, eliminating the need for using std::stack. Here, std::vector\u0026lt;bool\u0026gt; is utilized instead of std::unordered_set\u0026lt;int\u0026gt; for memory efficiency. #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; void DFS(const int root, const int target, std::vector\u0026lt;int\u0026gt;\u0026amp; path, std::vector\u0026lt;bool\u0026gt;\u0026amp; visited, const std::vector\u0026lt;int\u0026gt;\u0026amp; rowPtr, const std::vector\u0026lt;int\u0026gt;\u0026amp; colIdx) { visited[root] = true; path.push_back(root); if(root == target) { std::cout \u0026lt;\u0026lt; \u0026#34;DFS path from root to target: \u0026#34;; for(int i = 0; i \u0026lt; path.size(); i++) { std::cout \u0026lt;\u0026lt; path[i] \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } std::cout \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; } for(int i = rowPtr[root]; i \u0026lt; rowPtr[root+1]; i++) { int next = colIdx[i]; if( visited[next] == false ) { DFS(next, target, path, visited, rowPtr, colIdx); } } } int main() { std::vector\u0026lt;int\u0026gt; rowPointer = {0, 2, 4, 6, 8, 10, 12}; std::vector\u0026lt;int\u0026gt; colIndices = {1, 2, 0, 3, 0, 1, 4, 1, 2, 5, 3, 5}; std::vector\u0026lt;int\u0026gt; path; std::vector\u0026lt;bool\u0026gt; visited( rowPointer.size()-1, false ); //used as unordered_set DFS(0, 5, path, visited, rowPointer, colIndices); return 0; } ","permalink":"https://yuang-chen.github.io/posts/2023-09-01-bfs/","summary":"Iterative BFS Despite its apparent simplicity, this approach relies heavily on the utilization of various STL containers. std::unordered_map records the parent of each node std::unordered_set checks if a node has been visited std::queue allows the nodes be accessed in the width-first flow; using std::stack for depth-first flow std::stack reverses the parents, so the path can be printed in root-to-target order. #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;unordered_map\u0026gt; #include \u0026lt;unordered_set\u0026gt; #include \u0026lt;queue\u0026gt; #include \u0026lt;stack\u0026gt; std::stack\u0026lt;int\u0026gt; BFS(const int root, const int target, const std::vector\u0026lt;int\u0026gt;\u0026amp; rowPtr, const std::vector\u0026lt;int\u0026gt;\u0026amp; colIdx) { std::unordered_map\u0026lt;int, int\u0026gt; parent; std::unordered_set\u0026lt;int\u0026gt; visited; std::queue\u0026lt;int\u0026gt; nodeQue; // std::stack\u0026lt;int\u0026gt; nodeStk for DFS std::stack\u0026lt;int\u0026gt; path; bool hasFound = false; nodeQue.","title":"BFS \u0026 DFS"},{"content":"Considering myself a researcher in graph algorithms, I\u0026rsquo;ve come to the surprising realization that my grasp of these algorithms is not as solid as I thought. Hence, this blog series aims to document my exploration of various graph algorithms I\u0026rsquo;ve encountered thus far, regardless of their complexity.\nThe algorithms are selected from the parallel graph frameworks GAP and GBBS, focusing on their single-threaded versions to assess their complexity.\nBreadth-First Search (BFS) Single-Source Shortest Paths (SSSP) Connected Components (CC) Betweenness Centrality (BC) Triangle Counting (TC) Minimum Spanning Tree (MST) Strongly Connected Components (SCC) SCAN Clustering (SCAN) Low Diameter Decomposition (LDD) Biconnected-Components (BC) Graph Coloring (COLOR) Maximal Matching (MM) Maximal Independent Set (MIS) ","permalink":"https://yuang-chen.github.io/posts/2023-08-31-graph-algorithms/","summary":"Considering myself a researcher in graph algorithms, I\u0026rsquo;ve come to the surprising realization that my grasp of these algorithms is not as solid as I thought. Hence, this blog series aims to document my exploration of various graph algorithms I\u0026rsquo;ve encountered thus far, regardless of their complexity.\nThe algorithms are selected from the parallel graph frameworks GAP and GBBS, focusing on their single-threaded versions to assess their complexity.\nBreadth-First Search (BFS) Single-Source Shortest Paths (SSSP) Connected Components (CC) Betweenness Centrality (BC) Triangle Counting (TC) Minimum Spanning Tree (MST) Strongly Connected Components (SCC) SCAN Clustering (SCAN) Low Diameter Decomposition (LDD) Biconnected-Components (BC) Graph Coloring (COLOR) Maximal Matching (MM) Maximal Independent Set (MIS) ","title":"Graph Algorithms"},{"content":"In my HPC-oriented programming, my go-to choices are typically limited to arrays and vectors because of their memory efficiency. Linked lists and hash maps, being non-contiguous in memory space, rarely find their way into my toolkit. These containers draw upon many classic algorithmic designs. Lately, as I\u0026rsquo;ve been revisiting fundamental graph algorithms, I\u0026rsquo;ve also decided to take on the tasks of re-implementing these containers in a simplified illustration.\nThey are:\nC++98: std::map, std::set, std::multimap, and std::multiset C++11: std::unordered_map, std::unordered_set, std::unordered_multimap, and std::unordered_multiset C++23: std::flat_map, std::flat_set, std::flat_multimap, and std::flat_multiset Sequence Containers Data structures which can be accessed sequentially.\nstd::array\u0026lt;T,size\u0026gt; std::vector\u0026lt;T\u0026gt; std::deque\u0026lt;T\u0026gt; std::list\u0026lt;T\u0026gt; std::forward_list\u0026lt;T\u0026gt; Associative Containers Sorted data structures (i.e., balanced binary search tree) that can be quickly searched (O(log n) complexity).\nstd::set\u0026lt;T\u0026gt; std::multiset\u0026lt;T\u0026gt; std::map\u0026lt;Key, Value\u0026gt; std::multimap\u0026lt;Key, Value\u0026gt; Typically, an associate container consists a data type(s), a comparison function, and an allocator\ntemplate\u0026lt;typename Key, typename Value, typename Compare = std::less\u0026lt;key\u0026gt;, typename Allocator = std::allocator\u0026lt;std::pair\u0026lt;const Key, Value\u0026gt;\u0026gt;\u0026gt; Unordered Associative Containers Unsorted data structures (i.e., hashing bucket) that can be quickly searched (O(1) average, O(n) worst-case complexity).\nstd::unordered_set\u0026lt;T\u0026gt; std::unordered_map\u0026lt;Key, Value\u0026gt; std::unordered_multiset\u0026lt;T\u0026gt; std::unordered_multimap\u0026lt;Key, Value\u0026gt; Typically, an unordered associate container consists a data type(s), a hash function, an equal function and an allocator. The equal function indicates this type of containers does not provide support for comparison.\ntemplate\u0026lt; typename Key, typename T, typename Hash = std::hash\u0026lt;Key\u0026gt;, typename KeyEqual = std::equal_to\u0026lt;Key\u0026gt;, typename Allocator = std::allocator\u0026lt;std::pair\u0026lt;const Key, T\u0026gt;\u0026gt; \u0026gt; Container adaptors A different interface for sequential containers.\nstd::stack\u0026lt;T\u0026gt; std::queue\u0026lt;T\u0026gt; std::priority_queue\u0026lt;T\u0026gt; std::flat_set (c++23) std::flat_map (c++23) std::flat_multiset (c++23) std::flat_multimap (c++23) The flat-ordered associative containers in C++23 have the same interface as their C++98 pendants. They adopt from sequence containers, e.g., std::vector by default.\nReference [1] Back to Basics: Standard Library Containers in Cpp - Rainer Grimm - CppCon 2022\n[2] C++23: Four new Associative Containers\n[3] Refresher on Containers, Algorithms and Performance in C++ - Vladimir Vishnevskii - CppCon 2022\n","permalink":"https://yuang-chen.github.io/posts/2023-08-30-stl-containers/","summary":"In my HPC-oriented programming, my go-to choices are typically limited to arrays and vectors because of their memory efficiency. Linked lists and hash maps, being non-contiguous in memory space, rarely find their way into my toolkit. These containers draw upon many classic algorithmic designs. Lately, as I\u0026rsquo;ve been revisiting fundamental graph algorithms, I\u0026rsquo;ve also decided to take on the tasks of re-implementing these containers in a simplified illustration.\nThey are:","title":"STL Containers"},{"content":"Background Scope Guard is a concept reminiscent of the RAII (Resource Acquisition Is Initialization) principle in C++. The idea is to manage resources (like memory, files, network sockets, etc.) using object lifetime. When the object goes out of scope, its destructor ensures that the resource is cleaned up properly. The scope guard is intended to run a given callable (like a function or lambda) when it is destroyed.\nRAII (Resource Acquisition Is Initialization) is a programming idiom used in C++ where the lifetime of an object is bound to the lifetime of its scope (typically represented by a block of code wrapped in curly braces {}).\nHere\u0026rsquo;s a breakdown of RAII:\nResource Acquisition: When an object is created, it acquires a specific resource. Initialization: The resource acquisition is done during the object\u0026rsquo;s construction (i.e., when it\u0026rsquo;s initialized). RAII ensures the following:\nResources are acquired in a deterministic manner (during object creation). Resources are released in a deterministic manner (during object destruction). Exception safety, as resources are automatically cleaned up even if an exception is thrown. Example A simple example of RAII is the use of std::unique_ptr to manage dynamically allocated memory:\nvoid exampleFunction() { std::unique_ptr\u0026lt;int\u0026gt; p(new int(5)); // Resource (memory) is acquired and \u0026#34;owned\u0026#34; by p. // Do some operations with p... } // p goes out of scope and its destructor is called, which deletes the memory. No memory leak! This RAII behavior is contrasted with manual memory management where you\u0026rsquo;d have to remember to call delete:\nvoid nonRAIIExample() { int* p = new int(5); // Memory is acquired. // Do some operations... delete p; // You have to manually release the memory. Risky! } Implementation of a Scope Guard Requirements Three requirements are listed in the following code block for implementing the scope guard.\n#include \u0026lt;cstdio\u0026gt; #include \u0026lt;cassert\u0026gt; #include \u0026lt;stdexcept\u0026gt; #include \u0026lt;iostream\u0026gt; #include \u0026lt;functional\u0026gt; int main() { { // Requirement 0: Support lambda FILE * fp = nullptr; try{ fp = fopen(\u0026#34;test.txt\u0026#34;,\u0026#34;a\u0026#34;); auto guard = scope_guard([\u0026amp;] { fclose(fp); fp = nullptr; }); throw std::runtime_error{\u0026#34;Test\u0026#34;}; } catch(std::exception \u0026amp; e){ puts(e.what()); } assert(fp == nullptr); } puts(\u0026#34;----------\u0026#34;); { // Requirement 1: Support function object invocation // \u0026amp; binding arguments to the callable. struct Test { void operator()(X* x) { delete x; } } t; auto x = new X{}; auto guard = scope_guard(t, x); } puts(\u0026#34;----------\u0026#34;); { // Requirement 2: Support member functions and std::ref. auto x = new X{}; { struct Test { void f(X*\u0026amp; px) { delete px; px = nullptr; } } t; auto guard = scope_guard{\u0026amp;Test::f, \u0026amp;t, std::ref(x)}; } assert(x == nullptr); } Solutions Naive To meet the basic requirement, all you need to do is keep the lambda stored within a std::function:\n// naive solution class scope_guard { public: explicit scope_guard(std::function\u0026lt;void()\u0026gt; onExitScope) : onExitScope_(onExitScope) {} ~scope_guard() { on_exit_scope(); } private: std::function\u0026lt;void()\u0026gt; on_exit_scope; }; Conventional However, for requirements 2 and 3, things get trickier. We need to deal with more complex situations like binding arguments and passing by reference. As a result, we\u0026rsquo;re stepping up our game with an upgraded solution:\n// conventional solution class scope_guard { public: template \u0026lt;typename Callable, typename... Args\u0026gt; scope_guard(Callable\u0026amp;\u0026amp; func, Args\u0026amp;\u0026amp;... args) { on_exit_scope = std::bind(std::forward\u0026lt;Callable\u0026gt;(func), std::forward\u0026lt;Args\u0026gt;(args)...); } ~scope_guard() { on_exit_scope(); } private: std::function\u0026lt;void()\u0026gt; on_exit_scope; }; Fancy However, this simple solution isn\u0026rsquo;t cool anymore. The use of std::bind dates back to the old c++11 days, but we\u0026rsquo;re now in the modern world of c++23. Let\u0026rsquo;s modernize (and over-complicate) the code:\n// fancy solution class scope_guard { public: template\u0026lt;typename Callable, typename... Args\u0026gt; requires std::invocable\u0026lt;Callable, std::unwrap_reference_t\u0026lt;Args\u0026gt;...\u0026gt; scope_guard(Func\u0026amp;\u0026amp; func, Args\u0026amp;\u0026amp;...args) :f{ [func = std::forward\u0026lt;Func\u0026gt;(func), ...args = std::forward\u0026lt;Args\u0026gt;(args)]() mutable { std::invoke(std::forward\u0026lt;std::decay_t\u0026lt;Func\u0026gt;\u0026gt;(func), std::unwrap_reference_t\u0026lt;Args\u0026gt;(std::forward\u0026lt;Args\u0026gt;(args))...); } }{} ~scope_guard() { on_exit_scope(); } // Prevent copying, but allow moves. scope_guard(const scope_guard\u0026amp;) = delete; scope_guard\u0026amp; operator=(const scope_guard\u0026amp;) = delete; scope_guard(scope_guard\u0026amp;\u0026amp;) = default; scope_guard\u0026amp; operator=(scope_guard\u0026amp;\u0026amp;) = default; private: std::function\u0026lt;void()\u0026gt; on_exit_scope; }; ","permalink":"https://yuang-chen.github.io/posts/2023-08-29-scope-guard/","summary":"Background Scope Guard is a concept reminiscent of the RAII (Resource Acquisition Is Initialization) principle in C++. The idea is to manage resources (like memory, files, network sockets, etc.) using object lifetime. When the object goes out of scope, its destructor ensures that the resource is cleaned up properly. The scope guard is intended to run a given callable (like a function or lambda) when it is destroyed.\nRAII (Resource Acquisition Is Initialization) is a programming idiom used in C++ where the lifetime of an object is bound to the lifetime of its scope (typically represented by a block of code wrapped in curly braces {}).","title":"Scope Guard"},{"content":"C++ templates are blueprints and don\u0026rsquo;t represent specific types until they are instantiated with actual types. Once instantiated, the compiler creates a specific version of that template for the provided type. For template classes, each instantiation has its own unique version of the static members, making them distinct for each type the template is instantiated with.\n///////////////////// // Code Block 1 ///////////////////// #include\u0026lt;iostream\u0026gt; class ComponentBase{ protected: // component_type_count is a static variable shared by derived classes static inline size_t component_type_count = 0; }; template\u0026lt;typename T\u0026gt; class Component : public ComponentBase{ public: static size_t component_type_id(){ // ID is the static local variable for a particular type T static size_t ID = component_type_count++; return ID; } }; class A : public Component\u0026lt;A\u0026gt; {}; class B : public Component\u0026lt;B\u0026gt; {}; class C : public Component\u0026lt;C\u0026gt; {}; int main() { std::cout \u0026lt;\u0026lt; A::component_type_id() \u0026lt;\u0026lt; std::endl; // 0 std::cout \u0026lt;\u0026lt; B::component_type_id() \u0026lt;\u0026lt; std::endl; // 1 std::cout \u0026lt;\u0026lt; B::component_type_id() \u0026lt;\u0026lt; std::endl; // 1 std::cout \u0026lt;\u0026lt; A::component_type_id() \u0026lt;\u0026lt; std::endl; // 0 std::cout \u0026lt;\u0026lt; A::component_type_id() \u0026lt;\u0026lt; std::endl; // 0 std::cout \u0026lt;\u0026lt; C::component_type_id() \u0026lt;\u0026lt; std::endl; // 2 } Key Points:\ncomponent_type_count belongs to the base class ComponentBase but shared by all derived classes. A unique ID belongs to every instantiated class (e.g., A, B, C). In code block 1, the component_type_id() function has a static local variable ID. When this function is called for the first time for a particular type T, the ID variable is initialized with the current value of component_type_count and then component_type_count is incremented. For all subsequent calls to this function for the same type T, the ID variable retains its value from the first call and just returns that value. Essentially, each type T gets a unique ID the first time this function is called, and the same ID is returned for all subsequent calls.\n///////////////////// // Code Block 2 ///////////////////// template\u0026lt;typename T\u0026gt; class Component : public ComponentBase{ public: static size_t component_type_id(){ return component_type_count++; } }; // print: 0 1 2 3 4 5 In code block 2, the component_type_id() function increments and returns the value of component_type_count every time it\u0026rsquo;s called, regardless of the type T. So, unlike the first block, every call to component_type_id() for a given type T will return a new, incremented value.\n","permalink":"https://yuang-chen.github.io/posts/2023-08-27-static-local-member/","summary":"C++ templates are blueprints and don\u0026rsquo;t represent specific types until they are instantiated with actual types. Once instantiated, the compiler creates a specific version of that template for the provided type. For template classes, each instantiation has its own unique version of the static members, making them distinct for each type the template is instantiated with.\n///////////////////// // Code Block 1 ///////////////////// #include\u0026lt;iostream\u0026gt; class ComponentBase{ protected: // component_type_count is a static variable shared by derived classes static inline size_t component_type_count = 0; }; template\u0026lt;typename T\u0026gt; class Component : public ComponentBase{ public: static size_t component_type_id(){ // ID is the static local variable for a particular type T static size_t ID = component_type_count++; return ID; } }; class A : public Component\u0026lt;A\u0026gt; {}; class B : public Component\u0026lt;B\u0026gt; {}; class C : public Component\u0026lt;C\u0026gt; {}; int main() { std::cout \u0026lt;\u0026lt; A::component_type_id() \u0026lt;\u0026lt; std::endl; // 0 std::cout \u0026lt;\u0026lt; B::component_type_id() \u0026lt;\u0026lt; std::endl; // 1 std::cout \u0026lt;\u0026lt; B::component_type_id() \u0026lt;\u0026lt; std::endl; // 1 std::cout \u0026lt;\u0026lt; A::component_type_id() \u0026lt;\u0026lt; std::endl; // 0 std::cout \u0026lt;\u0026lt; A::component_type_id() \u0026lt;\u0026lt; std::endl; // 0 std::cout \u0026lt;\u0026lt; C::component_type_id() \u0026lt;\u0026lt; std::endl; // 2 } Key Points:","title":"Static Local Member"},{"content":"We can customize the (printing) format of a given class by using the specialization of formatter.\n#include \u0026lt;format\u0026gt; #include \u0026lt;iostream\u0026gt; struct Frac { int a, b; }; template \u0026lt;\u0026gt; struct std::formatter\u0026lt;Frac\u0026gt; : std::formatter\u0026lt;string_view\u0026gt; { // parse() is inherited from the base class std::formatter\u0026lt;string_view\u0026gt; // * an efficient solution: auto format(const Frac\u0026amp; frac, std::format_context\u0026amp; ctx) const { return std::format_to(ctx.out(), \u0026#34;{}/{}\u0026#34;, frac.a, frac.b); } // the same functionality as above, but inefficient due to the temporary string // auto format(const Frac\u0026amp; frac, std::format_context\u0026amp; ctx) const { // std::string temp; // std::format_to(std::back_inserter(temp), \u0026#34;{}/{}\u0026#34;, // frac.a, frac.b); // return std::formatter\u0026lt;string_view\u0026gt;::format(temp, ctx); // } }; void print(std::string_view fmt,auto\u0026amp;\u0026amp;...args){ std::cout \u0026lt;\u0026lt; std::vformat(fmt, std::make_format_args(std::forward\u0026lt;decltype(args)\u0026gt;(args)...)); } int main() { Frac f{ 1,10 }; print(\u0026#34;{}\u0026#34;, f); // prints \u0026#34;1/10\u0026#34; } ctx: provides access to formatting state consisting of the formatting arguments and the output iterator. ctx.out(): the iterator to the output buffer std::format_to(): append parts of the formatted output to the target destination. ","permalink":"https://yuang-chen.github.io/posts/2023-08-25-formatter-specialization/","summary":"We can customize the (printing) format of a given class by using the specialization of formatter.\n#include \u0026lt;format\u0026gt; #include \u0026lt;iostream\u0026gt; struct Frac { int a, b; }; template \u0026lt;\u0026gt; struct std::formatter\u0026lt;Frac\u0026gt; : std::formatter\u0026lt;string_view\u0026gt; { // parse() is inherited from the base class std::formatter\u0026lt;string_view\u0026gt; // * an efficient solution: auto format(const Frac\u0026amp; frac, std::format_context\u0026amp; ctx) const { return std::format_to(ctx.out(), \u0026#34;{}/{}\u0026#34;, frac.a, frac.b); } // the same functionality as above, but inefficient due to the temporary string // auto format(const Frac\u0026amp; frac, std::format_context\u0026amp; ctx) const { // std::string temp; // std::format_to(std::back_inserter(temp), \u0026#34;{}/{}\u0026#34;, // frac.","title":"Formatter Specialization"},{"content":"User Defined Literals (UDL) produces an object in an interesting way:\nconstexpr auto operator\u0026#34;\u0026#34;_f(const char* fmt, size_t) { return[=]\u0026lt;typename... T\u0026gt;(T\u0026amp;\u0026amp;... Args) { return std::vformat(fmt, std::make_format_args(std::forward\u0026lt;T\u0026gt;(Args)...)); }; } auto s = \u0026#34;example {} see {}\u0026#34;_f(\u0026#34;yep\u0026#34;, 1.1); // s = \u0026#34;example yep 1.1\u0026#34; The UDL _f has the same effect of std::format(\u0026quot;example {} see {}\u0026quot;, \u0026quot;yep\u0026quot;, 1.1). Pretty familiar (as libfmt), right?\nNow, let\u0026rsquo;s break the definition of _f down:\nint x = 10; double y = 3.14; // 0. std::string format( std::format_string\u0026lt;Args...\u0026gt; fmt, Args\u0026amp;\u0026amp;... args ); // basic format string. // it is an error if the format string is not a constant expression, i.e., known at compile-time, // std::vformat can be used in this case. std::string s = std::format(\u0026#34;x is {} and y is {}\u0026#34;, x, y); // 1. std::string vformat( std::string_view fmt, std::format_args args ); // It\u0026#39;s useful in scenarios where you need to pass around the arguments // separately from the format string auto args = std::make_format_args(x, y); std::string s = std::vformat(\u0026#34;x is {} and y is {}\u0026#34;, args); // 2. a variadic lambda template return [=]\u0026lt;typename... T\u0026gt;(T\u0026amp;\u0026amp;... Args) {...} // 3. user-defined literals // this UDL returns variadic lambda template // e.g., auto str = \u0026#34;haha\u0026#34;_f(x, y) // -\u0026gt; auto f = \u0026#34;haha\u0026#34;_f; auto str = f(x,y); constexpr auto operator\u0026#34;\u0026#34;_f(const char* fmt, size_t) { return [=]\u0026lt;typename... T\u0026gt;(T\u0026amp;\u0026amp;... Args) {...} } // 4. BUG! While the lambda itself is constexpr and the captured fmt // is a pointer to a string literal (which is also a compile-time constant), // the usage of fmt inside the lambda isn\u0026#39;t considered a constant expression // in the strictest sense when passed to std::format. for constexpr auto operator\u0026#34;\u0026#34;_f(const char* fmt, size_t) { return [=]\u0026lt;typename... T\u0026gt;(T\u0026amp;\u0026amp;... Args) { return std::format(fmt, std::forward\u0026lt;T\u0026gt;(Args)...); }; } //***************************// //Combining all above, we get: //***************************// constexpr auto operator\u0026#34;\u0026#34;_f(const char* fmt, size_t) { return[=]\u0026lt;typename... T\u0026gt;(T\u0026amp;\u0026amp;... Args) { return std::vformat(fmt, std::make_format_args(std::forward\u0026lt;T\u0026gt;(Args)...)); }; } //e.g., put the \u0026#34;yep\u0026#34; string into the {} of the example string. auto s = \u0026#34;example {}\u0026#34;_f(\u0026#34;yep\u0026#34;); ","permalink":"https://yuang-chen.github.io/posts/2023-08-22-user-defined-literals/","summary":"User Defined Literals (UDL) produces an object in an interesting way:\nconstexpr auto operator\u0026#34;\u0026#34;_f(const char* fmt, size_t) { return[=]\u0026lt;typename... T\u0026gt;(T\u0026amp;\u0026amp;... Args) { return std::vformat(fmt, std::make_format_args(std::forward\u0026lt;T\u0026gt;(Args)...)); }; } auto s = \u0026#34;example {} see {}\u0026#34;_f(\u0026#34;yep\u0026#34;, 1.1); // s = \u0026#34;example yep 1.1\u0026#34; The UDL _f has the same effect of std::format(\u0026quot;example {} see {}\u0026quot;, \u0026quot;yep\u0026quot;, 1.1). Pretty familiar (as libfmt), right?\nNow, let\u0026rsquo;s break the definition of _f down:\nint x = 10; double y = 3.","title":"User Defined Literals"},{"content":"Reference: here.\nThe return of overloaded operator should be a reference, otherwise return-by-code will create a (temporary) rvalue that cannot be passed to the next operation f2 by non-const reference. i.e., rvalue cannot be non-const referenced.\n#include \u0026lt;vector\u0026gt; #include \u0026lt;iostream\u0026gt; #include \u0026lt;functional\u0026gt; template\u0026lt;typename T, typename FN\u0026gt; requires std::invocable\u0026lt;FN, T\u0026amp;\u0026gt; // diff std::invocable? std::vector\u0026lt;T\u0026gt;\u0026amp; operator| (std::vector\u0026lt;T\u0026gt;\u0026amp; vec, FN fn) noexcept { for(auto\u0026amp; e: vec) { fn(e); } return vec; } int main(){ std::vector v{1, 2, 3}; auto f1 = [](int\u0026amp; i) {i *= i; }; std::function f2 {[](const int\u0026amp; i) {std::cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#39; \u0026#39;; } }; v | f1 | f2; }``` ","permalink":"https://yuang-chen.github.io/posts/2023-08-17-operator-overload/","summary":"Reference: here.\nThe return of overloaded operator should be a reference, otherwise return-by-code will create a (temporary) rvalue that cannot be passed to the next operation f2 by non-const reference. i.e., rvalue cannot be non-const referenced.\n#include \u0026lt;vector\u0026gt; #include \u0026lt;iostream\u0026gt; #include \u0026lt;functional\u0026gt; template\u0026lt;typename T, typename FN\u0026gt; requires std::invocable\u0026lt;FN, T\u0026amp;\u0026gt; // diff std::invocable? std::vector\u0026lt;T\u0026gt;\u0026amp; operator| (std::vector\u0026lt;T\u0026gt;\u0026amp; vec, FN fn) noexcept { for(auto\u0026amp; e: vec) { fn(e); } return vec; } int main(){ std::vector v{1, 2, 3}; auto f1 = [](int\u0026amp; i) {i *= i; }; std::function f2 {[](const int\u0026amp; i) {std::cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#39; \u0026#39;; } }; v | f1 | f2; }``` ","title":"Operator Overload"},{"content":"Finally, C++23 allows overload for the subscript operator [] to be multi-dimensional.\nBefore that, we normally either use:\nvector of vector to form a matrix, and access it as mat[i][j] a class containing a big 1-d vector, but behaves as 2-d by overloading the operator (), e.g., mat(i,j) Now, with C++23, we advance the second option (which offers efficient memory access) with better indexing approaching as follow:\ntemplate \u0026lt;typename T, size_t R, size_t C\u0026gt; struct matrix { T\u0026amp; operator[](size_t const r, size_t const c) noexcept { return data_[r * C + c]; } T const\u0026amp; operator[](size_t const r, size_t const c) const noexcept { return data_[r * C + c]; } static constexpr size_t Rows = R; static constexpr size_t Columns = C; private: std::array\u0026lt;T, R * C\u0026gt; data_; }; int main() { matrix\u0026lt;int, 3, 2\u0026gt; m; for(size_t i = 0; i \u0026lt; m.Rows; ++i) { for(size_t j = 0; j \u0026lt; m.Columns; ++j) { m[i, j] = i * m.Columns + (j + 1); } } for(size_t i = 0; i \u0026lt; m.Rows; ++i) { for(size_t j = 0; j \u0026lt; m.Columns; ++j) { std::cout \u0026lt;\u0026lt; m[i, j] \u0026lt;\u0026lt; \u0026#39; \u0026#39;; } std::cout \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; } } ","permalink":"https://yuang-chen.github.io/posts/2023-05-13-multidim-subscript-operator/","summary":"Finally, C++23 allows overload for the subscript operator [] to be multi-dimensional.\nBefore that, we normally either use:\nvector of vector to form a matrix, and access it as mat[i][j] a class containing a big 1-d vector, but behaves as 2-d by overloading the operator (), e.g., mat(i,j) Now, with C++23, we advance the second option (which offers efficient memory access) with better indexing approaching as follow:\ntemplate \u0026lt;typename T, size_t R, size_t C\u0026gt; struct matrix { T\u0026amp; operator[](size_t const r, size_t const c) noexcept { return data_[r * C + c]; } T const\u0026amp; operator[](size_t const r, size_t const c) const noexcept { return data_[r * C + c]; } static constexpr size_t Rows = R; static constexpr size_t Columns = C; private: std::array\u0026lt;T, R * C\u0026gt; data_; }; int main() { matrix\u0026lt;int, 3, 2\u0026gt; m; for(size_t i = 0; i \u0026lt; m.","title":"Multidimensional Subscript Operator []"},{"content":"🦥 An old note.\nBitwise vs Arithmetic running on a vector of size 2^31, bitwise operations are significantly faster than arithmetic counterparts:\nseg = 64; volume = (vec_size - 1)/ seg + 1; unsigned bs = log2(seg); unsigned bv= log2(volume); unsigned bbv = volume - 1; Arithmetic: out[i] = i % volume * seg + i / volume\nBitwise: out[i] = ((i \u0026amp; bbv) \u0026lt;\u0026lt; bs) + (i \u0026gt;\u0026gt; bv)\nThe time difference:\nmethods time (10^-6s) arithmetic 18.92 bitwise 5.58 ","permalink":"https://yuang-chen.github.io/posts/2023-05-07-bitwise-op/","summary":"🦥 An old note.\nBitwise vs Arithmetic running on a vector of size 2^31, bitwise operations are significantly faster than arithmetic counterparts:\nseg = 64; volume = (vec_size - 1)/ seg + 1; unsigned bs = log2(seg); unsigned bv= log2(volume); unsigned bbv = volume - 1; Arithmetic: out[i] = i % volume * seg + i / volume\nBitwise: out[i] = ((i \u0026amp; bbv) \u0026lt;\u0026lt; bs) + (i \u0026gt;\u0026gt; bv)","title":"Bitwise Op"},{"content":"The results look suspicious to me\u0026hellip; But I wrote down this note many days ago 🦥. Maybe I need to evaluate it again.\nMultiple Parallel Regions The cost of constructing parallel region is expensive in OpenMP. Let\u0026rsquo;s use two example for illustration:\nThree loops operating on a vector of size 2^31, e.g.,\nfor(size_t i = 0; i \u0026lt; vec.size(); i++) vec[i] += 1, vec[i] *= 0.9, vec[i] /= 7, Case 1: a large parallel region including the three loops by omp parallel { omp for }\nCase 2: three separate parallel region are built for each loop via omp parallel for.\nThe time difference:\n#parallel region time (ms) one 2.59 three 0.57 The result is contradictory to our intuition, as we expect a big parallel region (case 1) to run faster than three regions (case 2). The contradition results from the expensive overhead of building the big parallel region. By breaking down the performance and measuring the three loops respetively, we obtain:\nloop one three init 2.298 / 1st 0.017 0.057 2nd 0.011 0.032 3rd 0.020 0.030 The initialization of a parallel region is extremely expensive (i.e., 2.298ms), which consumes even more time than the computational tasks in our case. Within the parallel region of case 1, each loop costs shorter than their counterparts in case 2. Thus, together with the initialization phase, the computing tasks in case 1 deliver suboptimal performance than the sum of individual regions.\n","permalink":"https://yuang-chen.github.io/posts/2023-05-02-omp-parallel-region/","summary":"The results look suspicious to me\u0026hellip; But I wrote down this note many days ago 🦥. Maybe I need to evaluate it again.\nMultiple Parallel Regions The cost of constructing parallel region is expensive in OpenMP. Let\u0026rsquo;s use two example for illustration:\nThree loops operating on a vector of size 2^31, e.g.,\nfor(size_t i = 0; i \u0026lt; vec.size(); i++) vec[i] += 1, vec[i] *= 0.9, vec[i] /= 7, Case 1: a large parallel region including the three loops by omp parallel { omp for }","title":"Omp Parallel Region"},{"content":"One of my old-day notes 🦥.\nCollapse of Nested Loops The collapse clause converts a prefect nested loop into a single loop then parallelize it. The condition of a perfect nested loop is that, the inner loop is tightly included by the outer loop, and no other codes lying between:\nfor(int i = 0 ... ) { for(int j = 0 ...) { task[i][j]; } } Such condition is hard to meet. Moreover, it best suits with (1) the static scheduler instead of the dynamic one, and (2) when the parallelism of the outer loop is smaller than the number of threads, i.e., i \u0026lt; num_threads.\nIn the situation where the workload is imbalanced and the parallelism of outer loop is not an issue, the dynamic scheduler still performs better as in our test cases.\nExperimental setting: only change the Scatter phase with the following scheduling policy,\nscheduler time (s) dynamic collapse 2.004 static collapse 1.329 dynamic 1.276 ","permalink":"https://yuang-chen.github.io/posts/2023-05-02-omp-collapse/","summary":"One of my old-day notes 🦥.\nCollapse of Nested Loops The collapse clause converts a prefect nested loop into a single loop then parallelize it. The condition of a perfect nested loop is that, the inner loop is tightly included by the outer loop, and no other codes lying between:\nfor(int i = 0 ... ) { for(int j = 0 ...) { task[i][j]; } } Such condition is hard to meet.","title":"Omp Collapse"},{"content":"Another post recycled from my earlier notes. I really don\u0026rsquo;t have motivation to improve it further 🦥.\nVector vs Array Initilization The Vector is the preferred choice for data storage in mordern C++. It is internally implemented based on the Array. However, the performance gap between the two is indeed obvious.\nThe Vector can be initialized via std::vector\u0026lt;T\u0026gt; vec(size). Meanwhile, an Array is initialized by T* arr = new T[size]\nallocate data field of size 2^63 (initializing to default value Zero), the time difference:\nData Field time (1^-6s) Vector 14.18 Array 3.67 !! smart pointers (e.g., unique_ptr) are as slow as vector.\ncreating multi-dimensional arrays/vectors, the array of arrays still performs as the fastest one:\nArray of Array: T** AoA = new T [size] Array of Vector: vector\u0026lt;T\u0026gt;* AoV = new vector\u0026lt;T\u0026gt; [size] Vector of Vector: vector\u0026lt;vector\u0026lt;T\u0026gt;\u0026gt; = VoV(size) Additionally, Static Array of Vector: vector\u0026lt;T\u0026gt; SAoV [size]. We ignore the static array of (static) array because they are not often used for large volume of data. Given the 1st dimension to be 2^10 and the 2nd dimension to be 2^31, the time difference between those structures are:\nStructure time (1^-6s) AoA 0.005417 SAoA 0.008417 AoV 0.010792 VoV 2.92248 Filling C++ also provides function to fill the array/vector with the same elements, by the function std::fill. Additionally, vector can also be initialized with the same value. Below is the performance variation over different approaches.\nmethod time (1^-6s) fill arr 0.003667 fill vec 0.003667 init vec 0.007917 fill_n arr 0.01225 omp parallel for 8.55099 omp parallel for simd 2.32663 using std::fill achieves the best performance for both array and vector. The initialization of vector with a specific value (i.e., init vec) performs as the second fastest method, which is mainly decelerated by vector\u0026rsquo;s inherent overhead. std::fill_n is (somehow?) substantially slower than the popular std::fill. Do NOT rely on the omp for simple data assignment, as its performance is TOO poor!!!\nUsage of C-style Array employ delete/delete[] when new/new[] are used. The number of delete and new shall be matched.\nAn object is created by new. When using delete, the destructor of the pointed-to object is called before de-allocation.\nSimilarly, an array of objects are allocated using new[]. The destructors of all created objects are called when calling the array version delete[].\nThus, if an array of vector is created using new[], e.g., AoV in the prior section, one just need one delete[] to release all resource. There is NO need to delete/erase/clear any vector objects before deleting the array.\nAs for the vector, which requires no new nor delete, the destructor of a std::vector automatically calls the destructors of the contained objects. Much more convenient (at the cost of speed), as programmers carry no burden of counting the new and delete pairs.\n","permalink":"https://yuang-chen.github.io/posts/2023-05-01-vector-vs-array/","summary":"Another post recycled from my earlier notes. I really don\u0026rsquo;t have motivation to improve it further 🦥.\nVector vs Array Initilization The Vector is the preferred choice for data storage in mordern C++. It is internally implemented based on the Array. However, the performance gap between the two is indeed obvious.\nThe Vector can be initialized via std::vector\u0026lt;T\u0026gt; vec(size). Meanwhile, an Array is initialized by T* arr = new T[size]","title":"Vector vs Array"},{"content":"Writing SIMD code that works across different platforms can be a challenging task. The following log illustrates how a seemingly simple operation in C++ can quickly escalate into a significant problem.\nLet\u0026rsquo;s look into the code below, where the elements of x is accessed through indices specified by idx.\nnormal code std::vector\u0026lt;float\u0026gt; x = /*some data*/ std::vector\u0026lt;int\u0026gt; idx = /* index */ for(auto i: idx) { auto data = x[i]; } Gather with Intel In AVX512, Gather is a specific intrinsic function to transfer data from a data array to a target vec, according to an index vec. This intrinsic vectorizes the example of normal code.\nSIMD code int simd_width = 16; for(size_t i = 0; i \u0026lt; x.size(); i+= simd_width) { __m512i idx_vec = _mm512_loadu_epi32(\u0026amp;idx[i]); __m512 x_vec = _mm512_i32gather_ps(idx_vec, \u0026amp;x[0], sizeof(float)); } With Intel\u0026rsquo;s SIMD, the code snippet gets the data from the vector x based on the index register idx_vec and store the resultant data into the result register x_vec.\nPersonally, after a few days of SIMD coding, I do appreciate such code, and consider this SIMD solution is simple and elegant: two instructions are used, which is nicely aligned with what happens in the normal code:\nloading the data from the idx vector; loading the data from x vector according to the result of the 1st step. A big BUT, the gather (and scatter) operation is not supported by most of other sets \u0026ndash; they simply just do NOT offer these instructions 😮‍💨. To achieve the same data loading task, more efforts are needed.\nCustomized Gather with ARM Using ARM intrinsics, we have to implement our own gather. I found three solutions do so and benchmarked their performances.\nTmp array /* tmp array */ int simd_width = 4; aligns(16) std::array\u0026lt;float,simd_width\u0026gt; tmp; for(size_t i = 0; i \u0026lt; x.size(); i += simd_width) { tmp[0] = x[idx[i]]; tmp[1] = x[idx[i + 1]]; tmp[2] = x[idx[i + 2]]; tmp[3] = x[idx[i + 3]]; float32x4_t tmp_vec = vld1q_f32(tmp.data()); // loading to register vst1q_f32(\u0026amp;buf[i], tmp_vec); } A naive solution (suggested by ChatGPT-4 and many GitHub repos) is to load the idx and x[idx] directly without the help of intrinsics, store the data in a temporary array, and then load to the target register. This solution mixes SIMD and non-SIMD. The indexing accesses (e.g., [i]) to the arrays lets the compilers/CPU do whatever they want, which loses the register-level control.\nUnion union alignas(64) f32x4_union { float32x4_t reg128; std::array\u0026lt;float, 4\u0026gt; f32x4; }; f32x4_union res_vec; for(size_t i = 0; i \u0026lt; size; i += 4) { res_vec.f32x4[0] = x[idx[i]]; res_vec.f32x4[1] = x[idx[i + 1]]; res_vec.f32x4[2] = x[idx[i + 2]]; res_vec.f32x4[3] = x[idx[i + 3]]; vst1q_f32(\u0026amp;buf[i], res_vec.reg128); } By put the array and register into a union, we now have the access to the elements of the register by indexing. Compared to the tmp array solution, the union solution avoids the code of loading data to the register (i.e., vld1q_f32), thus improving the efficiency. However, the indexing access is still under the control of the compiler/CPU.\nget \u0026amp; set uint32x4_t idx_vec; float32x4_t x_vec; for(size_t i = 0; i \u0026lt; size; i += 4) { idx_vec = vld1q_u32(\u0026amp;idx[i]); x_vec = vsetq_lane_f32(x[vgetq_lane_u32(idx_vec, 0)], x_vec, 0); x_vec = vsetq_lane_f32(x[vgetq_lane_u32(idx_vec, 1)], x_vec, 1); x_vec = vsetq_lane_f32(x[vgetq_lane_u32(idx_vec, 2)], x_vec, 2); x_vec = vsetq_lane_f32(x[vgetq_lane_u32(idx_vec, 3)], x_vec, 3); vst1q_f32(\u0026amp;buf[i], x_vec); } This solution combines the get and set intrinsics to mimic the advanced gather operation. The code is \u0026hellip; ugly, but efficient. It makes sure that idx_vec and x_vec are carefully reused, allowing the finest control in the registers.\nget \u0026amp; tmp array alignas(16) std::array\u0026lt;float, 4\u0026gt; values; for(size_t i = 0; i \u0026lt; size; i += 4) { uint32x4_t idx_vec = vld1q_u32(\u0026amp;idx[i]); values[0] = x[vgetq_lane_u32(idx_vec, 0)]; values[1] = x[vgetq_lane_u32(idx_vec, 1)]; values[2] = x[vgetq_lane_u32(idx_vec, 2)]; values[3] = x[vgetq_lane_u32(idx_vec, 3)]; float32x4_t x_vec = vld1q_f32(values.data()); vst1q_f32(\u0026amp;buf[i], x_vec); } The last solution mixes get with tmp array. It is an intermediate between the 1nd and 3rd solution in terms of the use of registers.\nBenchmarking With data size of 1\u0026lt;\u0026lt;27, the performance of the four solutions are:\nperformance union tmp get\u0026amp;set get\u0026amp;tmp time (ms) 703 639 583 648 assembly code lines 16 16 18 18 The union solution yields the shortest assembly code but the longest execution time. This short code piece is reasonable, as it eliminates one line code compared with the tmp method. But why so long time? The key reason is that, the writing to the union elements by indexing (res_vec.f32x4[i] = ...) is inefficient, compared to the use of intrinsic vld1q_f32(*ptr). Explicit control on the registers promises better performance! The get\u0026amp;set facilitates the finest control of registers, and thus gives the shortest time.\nA weird result is acquired by get\u0026amp;tmp. It actually slightly slower than tmp and I do not understand why. I feed the assembly code of the last three solutions to ChatGPT-4, and this is its analysis:\ntmp \u0026ndash; L10 (first loop): This loop involves more register manipulation (bfi, fmov, ins) compared to the other loops. It may contribute to higher register pressure and could potentially limit instruction-level parallelism, affecting performance.\nget\u0026amp;set \u0026ndash; L11 (second loop): This loop uses ld1 and ld1q instructions to load the required values into SIMD registers directly from memory. This reduces the amount of register manipulation required compared to L10, which could lead to better performance.\nget\u0026amp;tmp \u0026ndash; L12 (third loop): This loop uses a similar approach to L10, using a mix of bfi, fmov, and ins instructions to manipulate registers. However, it also involves an additional ldr instruction for each iteration, which increases the amount of memory operations per iteration compared to L10. This could potentially explain why L12 is slower than L10.\nIn conclusion, the second loop (L11) has the least amount of register manipulation and memory operations per iteration, which may contribute to its better performance. The third loop (L12) has more memory operations per iteration compared to the first loop (L10), which could lead to a slower execution time.\nTo wrap up, the get\u0026amp;set invokes the least number of instructions, thus yielding the best performance. This reason seems to make sense.\nThe complete code is here\n","permalink":"https://yuang-chen.github.io/posts/2023-04-27-gather-simd/","summary":"Writing SIMD code that works across different platforms can be a challenging task. The following log illustrates how a seemingly simple operation in C++ can quickly escalate into a significant problem.\nLet\u0026rsquo;s look into the code below, where the elements of x is accessed through indices specified by idx.\nnormal code std::vector\u0026lt;float\u0026gt; x = /*some data*/ std::vector\u0026lt;int\u0026gt; idx = /* index */ for(auto i: idx) { auto data = x[i]; } Gather with Intel In AVX512, Gather is a specific intrinsic function to transfer data from a data array to a target vec, according to an index vec.","title":" Gather with SIMD"},{"content":"Writing code with SIMD for vectorization is painful. It deserves a blog series to record all sorts of pains I have encountered and (partially) overcome.\nIndeed, once the pain of coding and debugging is finished, the program is lightning-faster. Nonetheless, I am here to complain instead of praising. Let me state why writing SIMD code is causing me emotional damage:\na single line of normal c++ code could be easily inflated to a dozen lines of code. when the code comes with data dependency across loop iterations, the SIMD would hit right at my front head and give me massive headache (for debugging). the usage of SIMD require low-level C coding SIMD intrinsics are often not compatible across different platforms, and even different CPU models. SIMD intrinsics are available in ARM, Intel, AMD and Nvidia chips, but GPU/CUDA opens another genre of SIMD programming paradigm so I will not discuss here. AMD, for the x86 arch, offers the same intrinsic set as Intel does. Thus, only the intrinsics of ARM and Intel are really concerned.\nNotation Before going any further, I would like firstly clarify the terms of \u0026ldquo;vector\u0026rdquo; used in this blog, which unfortunately can be used to name two distinct matters.\nvector in C++, is a container with variable size holding dynamically allocated data in heap. vector in SIMD, is a type specifying the data stored in registers, with fixed sizes such as 128, 256, 512 bits. In following context, vector refers to as the container, and vec denotes the data in register.\nIntel Intrinsic Let\u0026rsquo;s talk about the Intel firstly, the (aging) boss of CPU.\nIntel provides a number of intrinsic sets to us, such as SSE, AVX2, \u0026hellip;, AVX512. I only use AVX512 because it is the newest and widest set.\nnew means that AVX512 has something other sets do not have, for instance, the scatter and gather operations. The two counterparts are very useful, which is further discussed in another log.\nwide means AVX512 has 512-bit width vec. It is not obvious to see from the name at all.\nARM Intrinsics The computing capacity of ARM chip is weaker than that of Intel\u0026rsquo;s chip \u0026ndash; I derive this personal and irresponsible conclusion based on the fact that the SIMD width of ARM is merely 128 bits and sometimes is even 64 bits. Why so short? I guess ARM prefers to 8-bit or 16-bit data type, sacrificing a little precision for efficiency, which makes the shorter vec more reasonable.\nAnother shortcoming of ARM intrinsics is the lack of masked operations. It happens all the time when the input data cannot be exactly fitted in the SIMD vec, or I just need a portion of data. The mask in Intel intrinsics allows us to easily extract/fill the imperfectly aligned vec. For ARM, sorry, we have to find alternative solutions, as described in this [log]\n","permalink":"https://yuang-chen.github.io/posts/2023-04-25-simd-pain-intro/","summary":"Writing code with SIMD for vectorization is painful. It deserves a blog series to record all sorts of pains I have encountered and (partially) overcome.\nIndeed, once the pain of coding and debugging is finished, the program is lightning-faster. Nonetheless, I am here to complain instead of praising. Let me state why writing SIMD code is causing me emotional damage:\na single line of normal c++ code could be easily inflated to a dozen lines of code.","title":"SIMD is Pain"},{"content":"The content of this post is extracted from my previous random notes. I am too lazy to update and organize it 🦥.\nC++17 new feature \u0026ndash; parallel algorithms The parallel algorithms and execution policies are introduced in C++17. Unfortuantely, according to CppReference, only GCC and Intel support these features. Clang still leaves them unimplemented.\nA blog about it.\nThe parallel library brough by C++17 requires the usage of Intel\u0026rsquo;s oneTBB for multithreading.\nHowever, there are version conflicts between gcc and oneTBB, as mentioned in issue1 and issue2. Thus, we need to match the gcc with oneTBB for correct version:\ng++11 with oneTBB.2021 (tested). g++9/10 with oneTBB.2019 (untested). Moreoever, the TBB-backboned parallel algorithms does not promise superior performance (perphase due to the implementation overheads), according to the discussion here. Programmers may need to implement their own parallel algorithms to achieve optimal speed.\nFast Parallel Algorithms For implementation, we test std::, tbb-based std::parallel with par and par_unseq, and gnu_parallel for performance evaluation. gnu_parallel performs as the fastest toolkits.\nTODO: I should implement all those algorithms by myself in the near future.\nSorting gnu_parallel is favored by someones\nWhen operating on a vector of size 2^31, the performance of various implementations are:\nmethods time (10^-6s) std:: 15.87 par 2373.95 par_unseq 11.50 gnu_parallel 6.54 Prefix Sum It is also a well-studied algorithm, as descripted by link.\nWhen operating on a vector of size 2^31, the performance of various implementations are:\nmethods time (10^-6s) std:: 5.08 par 5.58 par_unseq 5.42 gnu_parallel 4.25 Conclusion libstdc++ offers built-in parallel implementations for a variety of algorithms, including sort and partial_sum. The parallel mode is implicitly enabled during the compilation with -fopenmp and _GLIBCXX_PARALLEL.\nMoreover, the parallel components called by e.g., std::sort are in fact the gnu_parallel codes. We can also explicitly call gnu_parallel by including the header, e.g.,\u0026lt;parallel/algorithm\u0026gt;. Compared with the parallel std::, gnu_parallel incurs smaller overhead and thus delivers (slightly) better performance.\nThe tbb-based parallel methods, which is the new feature of C++17, are unsatisfactory. The par policy behaves extremely poorly in Sorting and a bit bad in Prefix Sum. Suprisingly, the par_unseq policy (parallelism + vectorization) is rather good in Sorting, only second to gnu_parallel. The TBB and its optimization strategies remain to be explored in the future.\nMore details regarding gnu_parallel can be found on this page.\n","permalink":"https://yuang-chen.github.io/posts/2023-04-25-par-algo/","summary":"The content of this post is extracted from my previous random notes. I am too lazy to update and organize it 🦥.\nC++17 new feature \u0026ndash; parallel algorithms The parallel algorithms and execution policies are introduced in C++17. Unfortuantely, according to CppReference, only GCC and Intel support these features. Clang still leaves them unimplemented.\nA blog about it.\nThe parallel library brough by C++17 requires the usage of Intel\u0026rsquo;s oneTBB for multithreading.","title":"Parallel Algorithms from Libraries"},{"content":"I am now a Postdoc at CUHK, and graduated from CUHK on Shenzhen campus for my PhD. My CV is da.\nMy research explores sparse workloads on modern computing hardware. I have a keen interest in tackling graph-related problems across a range of computing platforms, including CPUs with OpenMP, GPUs utilizing CUDA, and clusters utilizing MPI and RMDA.\nAt the heart of my research lies in the interplay between sparse matrices and dense computing. By delving into their conflicts, I aim to uncover strategies and techniques that can effectively harness the power of powerful computing units such as SIMD and Tensor Cores.\nSpecifically, I am looking into three types of research problems:\nParallel Graph algorithms, e,g, PageRank, BFS, Triangle Counting, etc. Sparse matrix multiplication (SpMV, SpMM, SDDMM, SpGEMM) on CPUs and GPUs Graph Neural Network \u0026amp; Sparse Deep Neural Network. Also, I am a fan of modern C++ programming, particularly in the context of C++23.\nIn 2018, I received my dual master degrees in Multicore Systems from TU Eindhoven, Netherlands and TU Berlin, Germany with full scholarship granted by European Union. In 2017-2018, I worked as Masterarbeitor (i.e., intern) in Frauhofer FOKUS to build Germany\u0026rsquo;s e-health infrastructure, based on which I developed my master thesis \u0026ldquo;Providing the Infrastructure for SICCT Protocol Tests with Focus on Service Discovery and Pairing\u0026rdquo;.\nIn 2015, I obtained my bachelor degree at Huazhong University of Science and Technology (HUST). Also, in 2014-2015, being sponsored by CSC scholarship, I studied as an exchange student in RWTH Aachen, Germany.\nPublications YuAng Chen and Yeh-Ching Chung, “Workload Balancing via Graph Reordering on Multicore Systems,” IEEE Transactions on Parallel and Distributed Systems (TPDS), Vol. 33, No. 5, 2022, pp. 1231-1245.\nYuAng Chen and Yeh-Ching Chung, “HiPa: Hierarchical Partitioning for Fast Page Rank on Multicore Systems,” Proceedings of IEEE International conference on Parallel Processing (ICPP), Article No. 24, 2021, pp. 1-10.\nYuAng Chen and Yeh-Ching Chung, “Corder: Cache-Aware Reordering For Optimizing Graph Analytics,” Proceedings of ACM International conference on Principles and Practice of Parallel Programming (PPoPP), 2021, pp.472-473.\nYuAng Chen and Yeh-Ching Chung, \u0026ldquo;A Unequal Caching Strategy for Shared-Memory Graph Analytics\u0026rdquo;, to apear in IEEE Transactions on Parallel and Distributed Systems (TPDS).\nYuAng Chen and Yeh-Ching Chung, \u0026ldquo;Connectivity-Aware Link Analysis for Skewed Graphs,\u0026rdquo; Proceedings of International Conference on Parallel Processing (ICPP), pp. 482-491. 2023.\nOngoing Work\nYuAng Chen and Yeh-Ching Chung, \u0026ldquo;Locality Extraction \u0026amp; Blocking for Graph Adjacency Matrix Multiplications\u0026rdquo;, in preparation\nYuAng Chen and Jeffery Xu Yu, \u0026ldquo;Accelerating SpMV for Scale-Free Graphs with Optimized Bins\u0026rdquo;, ICDE'24\nYuAng Chen and Jeffery Xu Yu, \u0026ldquo;Vectorized Sparse Blocks of Graph Matrices\u0026rdquo;, DASFAA'24\nYuAng Chen and Jeffery Xu Yu, \u0026ldquo;Triangle Counting on Tensor Cores\u0026rdquo;, ATC'24\nTeaching 2020 Spring : Introduction to Programming Methodology\n2020 Fall : Operating System\n2021 Spring : Compiler Design\n2021 Fall : Operating System\n2022 Spring : Computer Architecture\n","permalink":"https://yuang-chen.github.io/about/aboutme/","summary":"I am now a Postdoc at CUHK, and graduated from CUHK on Shenzhen campus for my PhD. My CV is da.\nMy research explores sparse workloads on modern computing hardware. I have a keen interest in tackling graph-related problems across a range of computing platforms, including CPUs with OpenMP, GPUs utilizing CUDA, and clusters utilizing MPI and RMDA.\nAt the heart of my research lies in the interplay between sparse matrices and dense computing.","title":"About Me"}]